[
  {
    "objectID": "publication/2023-07-01-tractable-reinforcement-learning/index.html#abstract",
    "href": "publication/2023-07-01-tractable-reinforcement-learning/index.html#abstract",
    "title": "Tractable large-scale deep reinforcement learning",
    "section": "Abstract",
    "text": "Abstract\nReinforcement learning (RL) has emerged as one of the most promising and powerful techniques in deep learning. The training of intelligent agents requires a myriad of training examples which imposes a substantial computational cost. Consequently, RL is seldom applied to real-world problems and historically has been limited to computer vision tasks, similar to supervised learning. This work proposes an RL framework for complex, partially observable, large-scale environments. We introduce novel techniques for tractable training on commodity GPUs, and significantly reduce computational costs. Furthermore, we present a self-supervised loss that improves the learning stability in applications with a long-time horizon, shortening the training time. We demonstrate the effectiveness of the proposed solution on the application of road extraction from high-resolution satellite images. We present experiments on satellite images of fifteen cities that demonstrate comparable performance to state-of-the-art methods. To the best of our knowledge, this is the first time RL has been applied for extracting road networks. The code is publicly available at https://github.com/nsarang/road-extraction-rl."
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVoice Cloning in Browser with F5-TTS\n\n\n\nTTS\n\nVoice Cloning\n\nMachine Learning\n\nWebML\n\nEdge Computing\n\nWebGPU\n\nONNX\n\nJavaScript\n\n\n\nVoice cloning using the F5-TTS model in your browser. No servers, no uploads, just pure client-side on-device execution.\n\n\n\nSep, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrameDisplay: Enhanced DataFrame Display\n\n\n\nPython\n\nPandas\n\nJupyter\n\nData Visualization\n\nHTML\n\nJavaScript\n\n\n\nInteractive DataFrame display in Jupyter with resizable columns, sorting, and sticky headers\n\n\n\nJul, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPymortem\n\n\n\nPython\n\nDebugging\n\n\n\nA post-mortem debugging tool for inspection and manipulation of execution contexts after exceptions occur.\n\n\n\nMay, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWater Supply Forecast Rodeo\n\n\nA report on my 5th place finish in the Water Supply Forecast Rodeo, an ML competition on predicting seasonal water supply in the western US.\n\n\n\nOct, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustodium\n\n\n\nPython\n\nFinance\n\nPortfolio Management\n\nTax\n\n\n\nA Python package for tracking investment portfolios and calculating adjusted cost basis (ACB) for Canadian capital gain/loss calculations.\n\n\n\nApr, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nDCGAN on MNIST\n\n\n\nGAN\n\nCNN\n\nKeras\n\n\n\nA Keras implementation of Deep Convolutional Generative Adversarial Networks (DCGAN) trained on MNIST dataset.\n\n\n\nJun, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nD3 Force Layout - Biological Networks\n\n\n\nd3\n\ngraph\n\nprotein-networks\n\nvisualization\n\n\n\nAn interactive graph visualization of protein-protein interaction networks.\n\n\n\nSep, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/2025-07-19-interactive-pandas/index.html",
    "href": "project/2025-07-19-interactive-pandas/index.html",
    "title": "FrameDisplay: Enhanced DataFrame Display",
    "section": "",
    "text": "FrameDisplay is a lightweight Python package for rendering Pandas DataFrames as interactive HTML tables within Jupyter Notebooks and JupyterLab. It improves the default DataFrame display by adding features such as resizable columns, client-side sorting, sticky headers and index for improved navigation, data type indicators in column headers, distinct styling for null values, and tooltips for viewing complete cell content.\nI work extensively with Pandas in my personal projects and have always wanted something similar to Databricks’ display function, but for Jupyter. The existing open-source alternatives were either too heavyweight, lacked the visual appeal or didn’t check all the boxes I needed. So I built this package to bridge that gap. It’s not perfect yet, but I like it more than the alternatives :)"
  },
  {
    "objectID": "project/2025-07-19-interactive-pandas/index.html#features",
    "href": "project/2025-07-19-interactive-pandas/index.html#features",
    "title": "FrameDisplay: Enhanced DataFrame Display",
    "section": "1 Features",
    "text": "1 Features\n\nResizable Columns: Drag column dividers to resize them.\nSortable Columns: Click on column headers to sort the data.\nSticky Header & Index: The header and index rows remain visible during vertical and horizontal scrolling.\nColumn Type Icons: Icons in headers indicate data types (numeric, string, etc.).\nNull Value Styling: null values are visually distinct.\nTooltips: Hover over cell content to see the full value.\nNo Size Limit: Display DataFrames of any size (be mindful of browser performance with very large tables).\n\nRoadmap\n\nVirtual scrolling for improved performance with very large DataFrames.\nAdditional customization options (e.g., theming)."
  },
  {
    "objectID": "project/2025-07-19-interactive-pandas/index.html#live-demo",
    "href": "project/2025-07-19-interactive-pandas/index.html#live-demo",
    "title": "FrameDisplay: Enhanced DataFrame Display",
    "section": "2 Live Demo",
    "text": "2 Live Demo\n\n\n\n        \n            \n            \n        \n            \n                \n                     \n                    IDNameDatePriceQuantityRatingDescriptionCategoryColorIsAvailableCreatedByPercentageTemperatureWeightHeightAgeCodeURLEmailPhoneNumberUpdatedDateCreatedDateTimeFormattedPriceLongDescriptionTotalValueDiscountPriceMetadata\n                \n            \n            \n                01000Alpha-02021-08-26 00:00:0073.5978.02.5This is a sample description for item 0. It contains details about the product.HomeBlueFalseadmin1null95.316.851.559.0CODE-5432https://example.com/product/0user0@example.com(483)-674-56312021-12-07 00:00:002021-08-14 00:00:00$73.59This is a sample description for item 0. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 5740.0266.23100000000001{\"id\": 1000, \"category\": \"Home\", \"rating\": 2.5}11001Delta-12020-02-22 00:00:00547.1980.04.2This is a sample description for item 1. It contains details about the product.SportsPinkTrueadmin244.6792.117.261.0626.0CODE-7552https://example.com/product/1null(210)-593-20452021-02-05 00:00:002020-02-14 00:00:00$547.19This is a sample description for item 1. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 43775.200000000004492.47100000000006{\"id\": 1001, \"category\": \"Sports\", \"rating\": 4.2}21002Beta-22023-07-15 00:00:00214.7767.01.6This is a sample description for item 2. It contains details about the product.ElectronicsPinknulladmin232.9690.58.672.7273.0CODE-9402https://example.com/product/2user2@example.com(383)-755-63272024-04-11 00:00:002023-06-21 00:00:00$214.77This is a sample description for item 2. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 14389.59193.293{\"id\": 1002, \"category\": \"Electronics\", \"rating\": 1.6}31003Iota-32020-11-21 00:00:00905.7957.01.5This is a sample description for item 3. It contains details about the product.HomenullTrueadmin157.2968.113.062.7348.0CODE-6471https://example.com/product/3user3@example.com(367)-771-58952021-03-08 00:00:002020-11-10 00:00:00$905.79This is a sample description for item 3. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 51630.03815.211{\"id\": 1003, \"category\": \"Home\", \"rating\": 1.5}41004Eta-42020-01-13 00:00:00698.5776.02.3This is a sample description for item 4. It contains details about the product.nullGreenTrueguest35.9661.86.082.6229.0CODE-6270https://example.com/product/4user4@example.com(629)-883-49372020-03-25 00:00:002019-12-25 00:00:00$698.57This is a sample description for item 4. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 53091.32000000001628.7130000000001null51005null2020-04-07 00:00:00767.9440.04.4This is a sample description for item 5. It contains details about the product.HomeGreenTrueadmin266.7889.713.942.4832.0CODE-2831nulluser5@example.com(924)-977-76202020-10-13 00:00:002020-03-21 00:00:00$767.94This is a sample description for item 5. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 30717.600000000002691.1460000000001{\"id\": 1005, \"category\": \"Home\", \"rating\": 4.4}61006Delta-62023-03-14 00:00:0015.848.04.6This is a sample description for item 6. It contains details about the product.BeautyGreenTrueadmin215.97null3.632.7225.0CODE-3066https://example.com/product/6null(797)-243-14172023-04-04 00:00:002023-03-06 00:00:00$15.84This is a sample description for item 6. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 126.7214.256{\"id\": 1006, \"category\": \"Beauty\", \"rating\": 4.6}71007Alpha-72023-04-28 00:00:00755.9495.0nullThis is a sample description for item 7. It contains details about the product.nullRedTrueguest92.3183.46.451.8640.0CODE-7307https://example.com/product/7user7@example.com(402)-365-78572023-08-09 00:00:002023-04-25 00:00:00$755.94This is a sample description for item 7. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 71814.3680.3460000000001null81008Iota-82023-09-04 00:00:00953.998.04.5This is a sample description for item 8. It contains details about the product.SportsYellowTrueadmin176.9871.812.692.5735.0CODE-0567https://example.com/product/8user8@example.com(854)-136-41112024-01-04 00:00:002023-09-01 00:00:00$953.90This is a sample description for item 8. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 93482.2858.51{\"id\": 1008, \"category\": \"Sports\", \"rating\": 4.5}91009Theta-92024-01-03 00:00:00750.0428.01.3This is a sample description for item 9. It contains details about the product.ElectronicsBlackTrueuser363.8769.215.662.6138.0CODE-4026https://example.com/product/9user9@example.com(830)-327-97592024-08-05 00:00:002024-01-02 00:00:00$750.04This is a sample description for item 9. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 21001.12675.036{\"id\": 1009, \"category\": \"Electronics\", \"rating\": 1.3}101010Alpha-102021-03-14 00:00:00607.2211.02.1This is a sample description for item 10. It contains details about the product.BooksOrangeFalseuser136.7569.85.682.0662.0CODE-8173https://example.com/product/10user10@example.com(587)-787-68732022-02-08 00:00:002021-02-15 00:00:00$607.22This is a sample description for item 10. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 6679.42546.498{\"id\": 1010, \"category\": \"Books\", \"rating\": 2.1}111011Gamma-112021-07-02 00:00:00227.2323.02.6This is a sample description for item 11. It contains details about the product.ToysYellowTrueadmin295.7497.77.361.3451.0CODE-9022https://example.com/product/11user11@example.com(965)-274-39672021-09-28 00:00:002021-06-27 00:00:00$227.23This is a sample description for item 11. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 5226.29204.507{\"id\": 1011, \"category\": \"Toys\", \"rating\": 2.6}121012Zeta-122020-07-09 00:00:0038.7321.02.3This is a sample description for item 12. It contains details about the product.BeautyOrangeTrueguest45.590.54.081.5934.0CODE-1437https://example.com/product/12user12@example.com(582)-231-52372020-10-17 00:00:002020-06-29 00:00:00$38.73This is a sample description for item 12. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 813.329999999999934.857{\"id\": 1012, \"category\": \"Beauty\", \"rating\": 2.3}131013Delta-132023-05-14 00:00:00682.35null3.1This is a sample description for item 13. It contains details about the product.BeautyYellowFalseguest37.3688.93.682.952.0CODE-2248https://example.com/product/13null(672)-881-39992024-05-08 00:00:002023-05-07 00:00:00$682.35This is a sample description for item 13. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. null614.115{\"id\": 1013, \"category\": \"Beauty\", \"rating\": 3.1}141014Zeta-142024-11-23 00:00:00173.3977.03.5This is a sample description for item 14. It contains details about the product.ClothingOrangeTruenull91.2878.40.642.833.0CODE-8614https://example.com/product/14user14@example.com(443)-770-24632025-04-24 00:00:002024-10-28 00:00:00$173.39This is a sample description for item 14. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 13351.029999999999156.051{\"id\": 1014, \"category\": \"Clothing\", \"rating\": 3.5}151015Eta-152021-05-07 00:00:00136.355.03.0This is a sample description for item 15. It contains details about the product.FoodBlackTrueguest80.274.118.871.2721.0CODE-8907nulluser15@example.com(328)-862-60312021-09-15 00:00:002021-04-28 00:00:00$136.35This is a sample description for item 15. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 681.75122.715{\"id\": 1015, \"category\": \"Food\", \"rating\": 3.0}161016Zeta-162024-12-18 00:00:0036.791.03.5This is a sample description for item 16. It contains details about the product.ElectronicsPinkFalseadmin276.1981.915.971.8272.0CODE-6266https://example.com/product/16user16@example.com(439)-480-52692025-05-17 00:00:002024-11-20 00:00:00$36.70This is a sample description for item 16. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 3339.700000000000333.03{\"id\": 1016, \"category\": \"Electronics\", \"rating\": 3.5}171017Alpha-172022-01-02 00:00:00568.7559.03.7This is a sample description for item 17. It contains details about the product.HomeGreenTrueguest59.5895.61.481.5526.0CODE-5883https://example.com/product/17user17@example.com(574)-971-29292022-11-07 00:00:002021-12-26 00:00:00$568.75This is a sample description for item 17. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 33556.25511.875{\"id\": 1017, \"category\": \"Home\", \"rating\": 3.7}181018Iota-182024-09-06 00:00:00null30.01.9This is a sample description for item 18. It contains details about the product.SportsGreenTrueadmin152.5598.315.922.427.0CODE-7940https://example.com/product/18user18@example.com(987)-749-21662025-05-22 00:00:002024-08-28 00:00:00nullThis is a sample description for item 18. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. nullnull{\"id\": 1018, \"category\": \"Sports\", \"rating\": 1.9}191019Eta-192024-02-28 00:00:00144.3481.04.9This is a sample description for item 19. It contains details about the product.ToysRedTrueguest36.1893.62.871.2983.0CODE-9267https://example.com/product/19user19@example.com(299)-612-29632025-02-06 00:00:002024-02-20 00:00:00$144.34This is a sample description for item 19. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 11691.54129.906{\"id\": 1019, \"category\": \"Toys\", \"rating\": 4.9}201020Epsilon-202024-02-22 00:00:0059.7826.03.8This is a sample description for item 20. It contains details about the product.BooksWhiteTrueadmin111.2292.75.131.581.0CODE-6363https://example.com/product/20user20@example.com(490)-642-58882024-12-12 00:00:002024-02-10 00:00:00$59.78This is a sample description for item 20. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 1554.2853.802{\"id\": 1020, \"category\": \"Books\", \"rating\": 3.8}211021Kappa-212023-02-19 00:00:00219.5470.01.7nullClothingBlueFalseguest90.29null8.521.7939.0CODE-0427https://example.com/product/21user21@example.com(991)-974-34702023-08-30 00:00:002023-02-17 00:00:00$219.54null15367.8197.58599999999998{\"id\": 1021, \"category\": \"Clothing\", \"rating\": 1.7}221022Zeta-222024-01-20 00:00:00763.6220.01.1This is a sample description for item 22. It contains details about the product.ClothingPinkTrueadmin253.67null9.042.2650.0CODE-3588https://example.com/product/22user22@example.com(305)-450-77042024-10-23 00:00:002024-01-19 00:00:00$763.62This is a sample description for item 22. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 15272.4687.258{\"id\": 1022, \"category\": \"Clothing\", \"rating\": 1.1}231023Beta-232024-10-17 00:00:00754.4919.01.2This is a sample description for item 23. It contains details about the product.ToysBlackTrueuser185.4585.211.691.7532.0nullhttps://example.com/product/23user23@example.com(568)-246-61832025-03-27 00:00:002024-10-01 00:00:00$754.49This is a sample description for item 23. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 14335.31679.041{\"id\": 1023, \"category\": \"Toys\", \"rating\": 1.2}241024null2022-04-22 00:00:00259.7522.02.7This is a sample description for item 24. It contains details about the product.HomeGreenTrueadmin168.0366.018.482.5983.0CODE-4602https://example.com/product/24user24@example.com(284)-897-23242023-03-02 00:00:002022-03-30 00:00:00$259.75This is a sample description for item 24. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 5714.5233.775{\"id\": 1024, \"category\": \"Home\", \"rating\": 2.7}251025Epsilon-252021-05-13 00:00:0029.1977.02.6This is a sample description for item 25. It contains details about the product.nullPinkTrueuser248.8878.28.421.81nullCODE-7728https://example.com/product/25user25@example.com(746)-281-61342021-06-04 00:00:002021-04-20 00:00:00$29.19This is a sample description for item 25. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 2247.6326.271null261026Delta-262022-08-11 00:00:00999.9173.01.7This is a sample description for item 26. It contains details about the product.ElectronicsPurpleFalseuser244.5585.5null2.3919.0CODE-9568https://example.com/product/26user26@example.com(104)-562-54812023-04-21 00:00:002022-07-18 00:00:00$999.91This is a sample description for item 26. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 72993.43899.919{\"id\": 1026, \"category\": \"Electronics\", \"rating\": 1.7}271027Eta-272020-01-15 00:00:00653.6457.03.4This is a sample description for item 27. It contains details about the product.nullYellowTrueadmin213.1996.514.061.9178.0CODE-2615https://example.com/product/27user27@example.com(892)-272-35952020-09-07 00:00:002020-01-10 00:00:00$653.64This is a sample description for item 27. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 37257.479999999996588.276null281028Zeta-282024-02-03 00:00:00655.2489.03.7This is a sample description for item 28. It contains details about the product.BooksWhitenullguest56.3960.611.922.1637.0CODE-3951https://example.com/product/28user28@example.com(881)-898-24422025-01-13 00:00:002024-01-31 00:00:00$655.24This is a sample description for item 28. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 58316.36589.716{\"id\": 1028, \"category\": \"Books\", \"rating\": 3.7}291029Zeta-292024-05-05 00:00:00950.1252.03.3This is a sample description for item 29. It contains details about the product.HomePurpleTrueguest67.2876.52.282.6922.0CODE-0916https://example.com/product/29user29@example.com(544)-748-80352024-06-23 00:00:002024-04-23 00:00:00$950.12This is a sample description for item 29. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 49406.24855.1080000000001{\"id\": 1029, \"category\": \"Home\", \"rating\": 3.3}301030Epsilon-302020-04-12 00:00:0030.1865.03.6This is a sample description for item 30. It contains details about the product.ClothingPinkTrueuser22.18null13.332.9471.0CODE-3594https://example.com/product/30user30@example.com(588)-492-10762020-06-10 00:00:002020-04-04 00:00:00$30.18This is a sample description for item 30. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 1961.727.162{\"id\": 1030, \"category\": \"Clothing\", \"rating\": 3.6}311031Beta-312023-09-23 00:00:00134.9657.03.7This is a sample description for item 31. It contains details about the product.nullPurpleFalseguest0.381.412.292.3949.0CODE-6363https://example.com/product/31user31@example.comnull2024-03-11 00:00:002023-09-01 00:00:00$134.96This is a sample description for item 31. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 7692.72121.46400000000001null321032Gamma-322024-01-22 00:00:00568.336.02.9This is a sample description for item 32. It contains details about the product.FoodWhiteTrueadmin17.380.112.942.5958.0CODE-2846https://example.com/product/32user32@example.com(482)-114-62792024-07-28 00:00:002023-12-26 00:00:00$568.33This is a sample description for item 32. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 3409.9800000000005511.49700000000007{\"id\": 1032, \"category\": \"Food\", \"rating\": 2.9}331033Delta-332022-11-04 00:00:00702.4795.03.2This is a sample description for item 33. It contains details about the product.BooksWhiteTrueuser270.0477.219.342.9427.0CODE-9707https://example.com/product/33user33@example.com(643)-295-12932023-08-02 00:00:002022-11-01 00:00:00$702.47This is a sample description for item 33. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 66734.65000000001632.2230000000001{\"id\": 1033, \"category\": \"Books\", \"rating\": 3.2}341034Eta-342021-05-04 00:00:00176.1133.04.8This is a sample description for item 34. It contains details about the product.BeautyWhiteTrueadmin26.0367.215.941.9933.0CODE-5878https://example.com/product/34user34@example.com(741)-851-50762021-11-10 00:00:002021-05-03 00:00:00$176.11This is a sample description for item 34. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 5811.63158.49900000000002{\"id\": 1034, \"category\": \"Beauty\", \"rating\": 4.8}351035Iota-352024-03-09 00:00:00750.45null2.2This is a sample description for item 35. It contains details about the product.ElectronicsGreenTrueuser398.6181.02.061.5724.0CODE-0462https://example.com/product/35user35@example.com(998)-452-61132024-08-31 00:00:002024-03-06 00:00:00$750.45This is a sample description for item 35. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. null675.4050000000001{\"id\": 1035, \"category\": \"Electronics\", \"rating\": 2.2}361036Zeta-362021-12-11 00:00:00821.1187.03.8This is a sample description for item 36. It contains details about the product.ToysnullTrueguest28.7184.814.961.2684.0CODE-5761https://example.com/product/36user36@example.com(492)-615-60152022-01-31 00:00:002021-12-06 00:00:00$821.11This is a sample description for item 36. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 71436.57738.999{\"id\": 1036, \"category\": \"Toys\", \"rating\": 3.8}371037Alpha-372022-09-16 00:00:00117.0274.02.2This is a sample description for item 37. It contains details about the product.BeautyOrangeTrueguest13.5474.213.782.5869.0CODE-9234https://example.com/product/37user37@example.com(943)-146-58852023-09-15 00:00:002022-09-01 00:00:00$117.02This is a sample description for item 37. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 8659.48105.318{\"id\": 1037, \"category\": \"Beauty\", \"rating\": 2.2}381038Alpha-382020-02-21 00:00:00null3.04.4nullSportsGreenTrueadmin172.9363.27.542.5277.0CODE-3084https://example.com/product/38user38@example.com(566)-845-95242020-04-16 00:00:002020-02-07 00:00:00nullnullnullnull{\"id\": 1038, \"category\": \"Sports\", \"rating\": 4.4}391039Eta-392022-01-01 00:00:00680.5787.02.5This is a sample description for item 39. It contains details about the product.SportsOrangeFalseuser237.4578.413.62null57.0CODE-8064https://example.com/product/39user39@example.com(445)-544-31922022-09-02 00:00:002021-12-29 00:00:00$680.57This is a sample description for item 39. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 59209.590000000004612.513{\"id\": 1039, \"category\": \"Sports\", \"rating\": 2.5}401040Delta-402021-11-09 00:00:00402.6974.02.2This is a sample description for item 40. It contains details about the product.SportsRedTrueuser344.8587.93.12.6788.0CODE-2876https://example.com/product/40user40@example.com(601)-467-40872022-09-25 00:00:002021-11-08 00:00:00$402.69This is a sample description for item 40. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 29799.06362.421{\"id\": 1040, \"category\": \"Sports\", \"rating\": 2.2}411041Kappa-412023-09-23 00:00:0085.24null1.1This is a sample description for item 41. It contains details about the product.nullRedFalseuser337.7593.19.71null65.0CODE-1478https://example.com/product/41user41@example.com(931)-513-13132024-02-01 00:00:002023-09-18 00:00:00$85.24This is a sample description for item 41. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. null76.716null421042Zeta-422020-02-03 00:00:00630.9761.02.6This is a sample description for item 42. It contains details about the product.ToysGreenFalseguest96.4199.718.421.3256.0CODE-9857https://example.com/product/42null(844)-234-44602020-12-06 00:00:002020-01-08 00:00:00$630.97This is a sample description for item 42. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 38489.17567.873{\"id\": 1042, \"category\": \"Toys\", \"rating\": 2.6}431043Theta-432021-09-02 00:00:00774.7657.0nullThis is a sample description for item 43. It contains details about the product.BeautyPinkTrueguest13.361.316.642.9849.0CODE-8625https://example.com/product/43user43@example.com(703)-616-37682022-01-15 00:00:002021-08-10 00:00:00$774.76This is a sample description for item 43. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 44161.32697.284null441044Theta-442024-09-05 00:00:00604.4124.04.1This is a sample description for item 44. It contains details about the product.ClothingBlackTrueadmin111.8675.17.032.9981.0CODE-5435https://example.com/product/44user44@example.com(828)-766-16122024-09-26 00:00:002024-08-22 00:00:00$604.41This is a sample description for item 44. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 14505.84543.9689999999999{\"id\": 1044, \"category\": \"Clothing\", \"rating\": 4.1}451045Gamma-452020-05-12 00:00:00984.0156.04.6This is a sample description for item 45. It contains details about the product.SportsBlueTrueguest39.3185.515.712.7643.0CODE-3300https://example.com/product/45user45@example.com(755)-361-97612021-04-06 00:00:002020-05-05 00:00:00$984.01This is a sample description for item 45. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 55104.56885.609{\"id\": 1045, \"category\": \"Sports\", \"rating\": 4.6}461046Iota-462020-06-07 00:00:00353.7363.04.3This is a sample description for item 46. It contains details about the product.SportsnullFalseuser197.8763.37.032.7587.0CODE-5582https://example.com/product/46user46@example.com(708)-149-27712020-11-21 00:00:002020-05-11 00:00:00$353.73This is a sample description for item 46. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 22284.99318.357{\"id\": 1046, \"category\": \"Sports\", \"rating\": 4.3}471047Kappa-472020-06-25 00:00:00376.8761.04.1This is a sample description for item 47. It contains details about the product.BeautyOrangeTrueadmin275.6285.814.471.1949.0CODE-5301https://example.com/product/47user47@example.com(120)-149-28972021-03-26 00:00:002020-06-16 00:00:00$376.87This is a sample description for item 47. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 22989.07339.183{\"id\": 1047, \"category\": \"Beauty\", \"rating\": 4.1}481048Kappa-482023-10-24 00:00:00347.8243.04.2This is a sample description for item 48. It contains details about the product.FoodOrangeTrueadmin146.399.11.441.1670.0CODE-2420https://example.com/product/48user48@example.com(489)-226-99662024-01-21 00:00:002023-10-09 00:00:00$347.82This is a sample description for item 48. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 14956.26313.038{\"id\": 1048, \"category\": \"Food\", \"rating\": 4.2}491049Delta-492021-03-26 00:00:00824.1114.01.8This is a sample description for item 49. It contains details about the product.ClothingRedTrueuser242.4195.8null2.3nullCODE-2320https://example.com/product/49user49@example.com(818)-256-43662022-02-05 00:00:002021-03-11 00:00:00$824.11This is a sample description for item 49. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 11537.54741.6990000000001{\"id\": 1049, \"category\": \"Clothing\", \"rating\": 1.8}501050Gamma-502023-01-06 00:00:00961.1843.04.0This is a sample description for item 50. It contains details about the product.BooksYellowTrueuser322.9769.20.552.2519.0CODE-8622https://example.com/product/50user50@example.com(745)-109-59332023-01-20 00:00:002022-12-11 00:00:00$961.18This is a sample description for item 50. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 41330.74865.062{\"id\": 1050, \"category\": \"Books\", \"rating\": 4.0}511051Beta-512022-07-26 00:00:00830.4264.03.7This is a sample description for item 51. It contains details about the product.BeautyBlackTrueuser115.5187.72.282.5118.0CODE-1188https://example.com/product/51user51@example.com(203)-778-95242023-03-25 00:00:002022-07-16 00:00:00$830.42This is a sample description for item 51. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 53146.88747.3779999999999{\"id\": 1051, \"category\": \"Beauty\", \"rating\": 3.7}521052Beta-522021-05-06 00:00:00441.1352.01.4This is a sample description for item 52. It contains details about the product.FoodWhiteTrueadmin187.5270.818.611.1866.0CODE-6481https://example.com/product/52user52@example.com(720)-817-32472022-01-26 00:00:002021-04-08 00:00:00$441.13This is a sample description for item 52. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 22938.76397.017{\"id\": 1052, \"category\": \"Food\", \"rating\": 1.4}531053Gamma-532020-07-28 00:00:00965.8271.01.2This is a sample description for item 53. It contains details about the product.FoodPurpleTrueguest3.2287.410.371.4357.0CODE-5514https://example.com/product/53user53@example.com(207)-623-29282021-07-09 00:00:002020-07-15 00:00:00$965.82This is a sample description for item 53. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 68573.22869.238{\"id\": 1053, \"category\": \"Food\", \"rating\": 1.2}541054Eta-54null810.12null1.1This is a sample description for item 54. It contains details about the product.FoodBlueFalseuser261.4981.76.692.1985.0CODE-2235https://example.com/product/54user54@example.com(903)-208-9154nullnull$810.12This is a sample description for item 54. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. null729.1080000000001{\"id\": 1054, \"category\": \"Food\", \"rating\": 1.1}551055Eta-552021-08-12 00:00:00488.6633.01.5This is a sample description for item 55. It contains details about the product.ClothingWhitenulluser388.967.29.291.8971.0CODE-7157https://example.com/product/55null(304)-896-53082022-07-18 00:00:002021-08-05 00:00:00$488.66This is a sample description for item 55. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 16125.78439.79400000000004{\"id\": 1055, \"category\": \"Clothing\", \"rating\": 1.5}561056Theta-562021-04-01 00:00:00733.7299.02.3This is a sample description for item 56. It contains details about the product.ToysPinkTrueguest6.7360.612.63null25.0CODE-7982https://example.com/product/56user56@example.com(306)-466-77132021-07-02 00:00:002021-03-15 00:00:00$733.72This is a sample description for item 56. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 72638.28660.3480000000001{\"id\": 1056, \"category\": \"Toys\", \"rating\": 2.3}571057Iota-572023-06-09 00:00:00853.266.01.8This is a sample description for item 57. It contains details about the product.ToysPinkTrueuser144.6465.813.181.2436.0CODE-2854https://example.com/product/57user57@example.com(140)-661-91322024-02-28 00:00:002023-05-20 00:00:00$853.20This is a sample description for item 57. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 56311.200000000004767.8800000000001{\"id\": 1057, \"category\": \"Toys\", \"rating\": 1.8}581058Alpha-582022-06-21 00:00:0095.81.04.5This is a sample description for item 58. It contains details about the product.BeautyPurpleTrueadmin287.2292.814.772.3929.0CODE-9207https://example.com/product/58user58@example.com(804)-592-64162022-07-26 00:00:002022-05-23 00:00:00$95.80This is a sample description for item 58. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 95.886.22{\"id\": 1058, \"category\": \"Beauty\", \"rating\": 4.5}591059Beta-592024-06-01 00:00:00251.4270.03.0This is a sample description for item 59. It contains details about the product.BeautyRedTrueuser14.4272.09.051.46nullCODE-5846https://example.com/product/59user59@example.com(113)-787-24582024-12-24 00:00:002024-05-28 00:00:00$251.42This is a sample description for item 59. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 17599.399999999998226.278{\"id\": 1059, \"category\": \"Beauty\", \"rating\": 3.0}601060Iota-602022-03-19 00:00:00614.237.01.7This is a sample description for item 60. It contains details about the product.ClothingWhiteTrueadmin253.9882.70.192.6184.0CODE-9649https://example.com/product/60user60@example.com(955)-687-90952022-06-08 00:00:002022-03-14 00:00:00$614.23This is a sample description for item 60. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 4299.610000000001552.807{\"id\": 1060, \"category\": \"Clothing\", \"rating\": 1.7}611061Zeta-612022-01-04 00:00:0038.4147.04.8This is a sample description for item 61. It contains details about the product.HomeRedTrueuser277.6187.15.641.7465.0CODE-6199https://example.com/product/61user61@example.com(192)-898-93832022-02-23 00:00:002021-12-12 00:00:00$38.41This is a sample description for item 61. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 1805.269999999999834.568999999999996{\"id\": 1061, \"category\": \"Home\", \"rating\": 4.8}621062Eta-622022-12-26 00:00:00190.0264.02.2This is a sample description for item 62. It contains details about the product.ElectronicsBlueFalseadmin199.3575.415.941.7728.0CODE-4825https://example.com/product/62user62@example.com(840)-203-51922023-12-21 00:00:002022-12-19 00:00:00$190.02This is a sample description for item 62. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 12161.28171.018{\"id\": 1062, \"category\": \"Electronics\", \"rating\": 2.2}631063Alpha-632024-09-04 00:00:00799.8598.04.2This is a sample description for item 63. It contains details about the product.ClothingBlueFalseadmin191.6491.616.012.7582.0CODE-0029https://example.com/product/63user63@example.com(567)-403-53272024-09-06 00:00:002024-08-22 00:00:00$799.85This is a sample description for item 63. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 78385.3719.865{\"id\": 1063, \"category\": \"Clothing\", \"rating\": 4.2}641064Epsilon-642024-04-09 00:00:00881.5235.01.3This is a sample description for item 64. It contains details about the product.ToysWhiteTrueuser257.7563.17.382.3568.0CODE-9385https://example.com/product/64user64@example.com(202)-932-38622024-06-02 00:00:002024-03-25 00:00:00$881.52This is a sample description for item 64. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 30853.2793.368{\"id\": 1064, \"category\": \"Toys\", \"rating\": 1.3}651065Gamma-65null283.5137.01.9This is a sample description for item 65. It contains details about the product.nullWhiteTruenull81.8473.82.372.1325.0CODE-9297https://example.com/product/65user65@example.com(972)-397-6901nullnull$283.51This is a sample description for item 65. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 10489.869999999999255.159null661066Beta-662024-09-22 00:00:00null90.03.7This is a sample description for item 66. It contains details about the product.HomeGreenTrueuser287.6369.39.841.3334.0nullhttps://example.com/product/66user66@example.com(420)-536-28982025-06-09 00:00:002024-08-24 00:00:00nullThis is a sample description for item 66. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. nullnull{\"id\": 1066, \"category\": \"Home\", \"rating\": 3.7}671067Epsilon-672021-06-14 00:00:0094.764.04.4This is a sample description for item 67. It contains details about the product.ClothingOrangeTrueadmin152.1384.26.341.4351.0CODE-3619https://example.com/product/67user67@example.com(876)-146-11582022-04-20 00:00:002021-06-10 00:00:00$94.76This is a sample description for item 67. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 379.0485.284{\"id\": 1067, \"category\": \"Clothing\", \"rating\": 4.4}681068Iota-682022-10-11 00:00:00493.6978.01.4This is a sample description for item 68. It contains details about the product.SportsOrangeFalseuser227.5283.87.912.8468.0CODE-1705https://example.com/product/68user68@example.com(779)-762-37932023-04-20 00:00:002022-09-28 00:00:00$493.69This is a sample description for item 68. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 38507.82444.321{\"id\": 1068, \"category\": \"Sports\", \"rating\": 1.4}691069Gamma-692020-12-10 00:00:00693.791.02.1This is a sample description for item 69. It contains details about the product.ElectronicsGreenTrueuser359.1773.17.11.8859.0CODE-2304https://example.com/product/69user69@example.com(548)-736-53952021-07-16 00:00:002020-12-03 00:00:00$693.70This is a sample description for item 69. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 63126.700000000004624.33{\"id\": 1069, \"category\": \"Electronics\", \"rating\": 2.1}701070Gamma-702023-06-13 00:00:00495.9171.02.5This is a sample description for item 70. It contains details about the product.BooksOrangeTrueguest17.5191.33.532.7952.0CODE-1274https://example.com/product/70user70@example.com(195)-481-66152023-07-27 00:00:002023-05-17 00:00:00$495.91This is a sample description for item 70. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 35209.61446.319{\"id\": 1070, \"category\": \"Books\", \"rating\": 2.5}711071Iota-712024-03-07 00:00:00102.12null2.0This is a sample description for item 71. It contains details about the product.ToysnullFalseuser155.5969.116.031.2543.0CODE-5234https://example.com/product/71user71@example.com(510)-509-84792024-08-16 00:00:002024-02-17 00:00:00$102.12This is a sample description for item 71. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. null91.908{\"id\": 1071, \"category\": \"Toys\", \"rating\": 2.0}721072Kappa-722023-11-24 00:00:00694.8729.03.6This is a sample description for item 72. It contains details about the product.ElectronicsYellowFalseuser2null75.011.841.731.0CODE-1453https://example.com/product/72user72@example.com(494)-806-88352024-06-13 00:00:002023-11-02 00:00:00$694.87This is a sample description for item 72. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 20151.23625.383{\"id\": 1072, \"category\": \"Electronics\", \"rating\": 3.6}731073Alpha-732024-10-10 00:00:00585.7415.03.7This is a sample description for item 73. It contains details about the product.BeautyWhiteTrueguest64.3383.46.332.5839.0CODE-5500https://example.com/product/73user73@example.com(764)-916-37322025-07-07 00:00:002024-09-12 00:00:00$585.74This is a sample description for item 73. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 8786.1527.166{\"id\": 1073, \"category\": \"Beauty\", \"rating\": 3.7}741074Zeta-742020-05-10 00:00:00535.6116.03.5This is a sample description for item 74. It contains details about the product.ClothingPinkTrueuser152.4590.92.432.2958.0CODE-4345https://example.com/product/74user74@example.com(812)-843-19602021-04-26 00:00:002020-05-08 00:00:00$535.61This is a sample description for item 74. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 8569.76482.04900000000004{\"id\": 1074, \"category\": \"Clothing\", \"rating\": 3.5}751075Epsilon-752024-10-21 00:00:00748.4820.02.3This is a sample description for item 75. It contains details about the product.FoodPinkTrueuser240.8369.92.912.286.0CODE-1110https://example.com/product/75user75@example.com(964)-263-26992025-08-21 00:00:002024-10-11 00:00:00$748.48This is a sample description for item 75. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 14969.6673.6320000000001{\"id\": 1075, \"category\": \"Food\", \"rating\": 2.3}761076Delta-762023-08-22 00:00:00705.8392.03.6This is a sample description for item 76. It contains details about the product.nullRedTrueuser218.3970.711.971.5744.0CODE-1770https://example.com/product/76null(704)-594-81782024-05-19 00:00:002023-08-09 00:00:00$705.83This is a sample description for item 76. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 64936.36635.2470000000001null771077Beta-772023-12-06 00:00:00258.8991.02.9This is a sample description for item 77. It contains details about the product.ToysYellowTrueuser124.191.48.521.0577.0CODE-0788https://example.com/product/77user77@example.com(488)-897-68752024-07-08 00:00:002023-11-11 00:00:00$258.89This is a sample description for item 77. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 23558.989999999998233.001{\"id\": 1077, \"category\": \"Toys\", \"rating\": 2.9}781078Theta-782022-09-22 00:00:00200.6662.03.6This is a sample description for item 78. It contains details about the product.FoodRedTrueadmin185.7575.610.792.7533.0CODE-6875https://example.com/product/78user78@example.com(983)-800-31402023-06-01 00:00:002022-09-01 00:00:00$200.66This is a sample description for item 78. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 12440.92180.594{\"id\": 1078, \"category\": \"Food\", \"rating\": 3.6}791079Iota-792022-02-27 00:00:00540.5132.01.2This is a sample description for item 79. It contains details about the product.HomeWhiteFalseuser246.363.718.622.2156.0CODE-6783https://example.com/product/79null(599)-215-77382022-09-05 00:00:002022-02-21 00:00:00$540.51This is a sample description for item 79. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 17296.32486.459{\"id\": 1079, \"category\": \"Home\", \"rating\": 1.2}801080Gamma-802021-11-10 00:00:00193.319.0nullThis is a sample description for item 80. It contains details about the product.BeautyWhiteTrueuser341.2777.83.421.7823.0CODE-6204https://example.com/product/80user80@example.com(164)-367-61482022-09-02 00:00:002021-10-13 00:00:00$193.30This is a sample description for item 80. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 3672.7000000000003173.97000000000003null811081Iota-812021-03-01 00:00:00489.3677.01.5This is a sample description for item 81. It contains details about the product.ElectronicsPurpleFalseuser153.064.13.822.5759.0CODE-6149https://example.com/product/81user81@example.comnull2021-09-30 00:00:002021-02-01 00:00:00$489.36This is a sample description for item 81. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 37680.72440.42400000000004{\"id\": 1081, \"category\": \"Electronics\", \"rating\": 1.5}821082Iota-82null976.8418.03.2This is a sample description for item 82. It contains details about the product.BeautyPurpleFalseadmin225.577.212.431.4849.0CODE-7855https://example.com/product/82user82@example.com(637)-885-4669nullnull$976.84This is a sample description for item 82. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 17583.12879.1560000000001{\"id\": 1082, \"category\": \"Beauty\", \"rating\": 3.2}831083Eta-832024-02-15 00:00:00290.1736.02.5This is a sample description for item 83. It contains details about the product.ClothingGreenTrueuser37.9180.519.041.4868.0CODE-9470https://example.com/product/83null(785)-994-65422024-10-09 00:00:002024-02-03 00:00:00$290.17This is a sample description for item 83. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 10446.12261.153{\"id\": 1083, \"category\": \"Clothing\", \"rating\": 2.5}841084Iota-842024-04-23 00:00:00202.1854.03.8This is a sample description for item 84. It contains details about the product.SportsOrangeTrueguest37.1163.63.981.1456.0CODE-3695https://example.com/product/84user84@example.com(804)-817-25602025-03-27 00:00:002024-04-03 00:00:00$202.18This is a sample description for item 84. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 10917.720000000001181.96200000000002{\"id\": 1084, \"category\": \"Sports\", \"rating\": 3.8}851085Delta-852023-07-08 00:00:00187.6565.03.3This is a sample description for item 85. It contains details about the product.FoodPinkTrueadmin182.9483.911.281.0240.0CODE-1207https://example.com/product/85user85@example.com(844)-354-88952023-08-30 00:00:002023-06-27 00:00:00$187.65This is a sample description for item 85. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 12197.25168.88500000000002{\"id\": 1085, \"category\": \"Food\", \"rating\": 3.3}861086Eta-862020-02-25 00:00:00null1.04.5This is a sample description for item 86. It contains details about the product.FoodBlueTrueuser237.7294.58.241.63nullCODE-0547https://example.com/product/86user86@example.com(792)-756-73052020-12-01 00:00:002020-01-30 00:00:00nullThis is a sample description for item 86. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. nullnull{\"id\": 1086, \"category\": \"Food\", \"rating\": 4.5}871087Zeta-872021-12-17 00:00:00281.5739.04.3This is a sample description for item 87. It contains details about the product.HomeBlueTrueadmin13.95null14.232.6875.0CODE-5221https://example.com/product/87user87@example.com(946)-671-18942022-07-22 00:00:002021-11-24 00:00:00$281.57This is a sample description for item 87. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 10981.23253.413{\"id\": 1087, \"category\": \"Home\", \"rating\": 4.3}881088Iota-882024-04-28 00:00:00557.8375.04.8This is a sample description for item 88. It contains details about the product.SportsGreenTrueuser180.4791.23.432.0988.0CODE-1715https://example.com/product/88user88@example.comnull2025-01-05 00:00:002024-03-31 00:00:00$557.83This is a sample description for item 88. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 41837.25502.047{\"id\": 1088, \"category\": \"Sports\", \"rating\": 4.8}891089Delta-892021-08-22 00:00:00135.0263.03.8This is a sample description for item 89. It contains details about the product.ClothingWhiteFalseguest67.1582.415.941.9477.0CODE-2787https://example.com/product/89user89@example.com(617)-969-86882022-02-26 00:00:002021-07-28 00:00:00$135.02This is a sample description for item 89. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 8506.26121.51800000000001{\"id\": 1089, \"category\": \"Clothing\", \"rating\": 3.8}901090Zeta-902024-10-11 00:00:00495.9758.02.8This is a sample description for item 90. It contains details about the product.SportsWhiteFalseuser320.8969.70.751.1535.0CODE-2636https://example.com/product/90user90@example.com(990)-269-22262024-11-21 00:00:002024-10-04 00:00:00$495.97This is a sample description for item 90. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 28766.260000000002446.37300000000005{\"id\": 1090, \"category\": \"Sports\", \"rating\": 2.8}911091null2023-10-27 00:00:00578.3245.02.5This is a sample description for item 91. It contains details about the product.BeautyBlackTrueuser130.3287.45.841.0325.0CODE-8962https://example.com/product/91user91@example.com(428)-346-64212024-04-01 00:00:002023-10-26 00:00:00$578.32This is a sample description for item 91. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 26024.4520.488{\"id\": 1091, \"category\": \"Beauty\", \"rating\": 2.5}921092Kappa-922024-07-27 00:00:00446.0698.02.5This is a sample description for item 92. It contains details about the product.ClothingPurpleTrueuser321.1870.811.362.8546.0CODE-1525https://example.com/product/92user92@example.com(151)-619-46482024-08-11 00:00:002024-07-26 00:00:00$446.06This is a sample description for item 92. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 43713.88401.454{\"id\": 1092, \"category\": \"Clothing\", \"rating\": 2.5}931093Beta-932023-03-24 00:00:0060.8659.03.6This is a sample description for item 93. It contains details about the product.SportsOrangeFalseuser274.1565.89.961.869.0CODE-7909https://example.com/product/93user93@example.com(487)-512-48502024-01-19 00:00:002023-02-27 00:00:00$60.86This is a sample description for item 93. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 3590.7454.774{\"id\": 1093, \"category\": \"Sports\", \"rating\": 3.6}941094Alpha-942023-05-25 00:00:00482.9525.02.1This is a sample description for item 94. It contains details about the product.ElectronicsRedTrueguest75.4371.011.52.4328.0CODE-7066https://example.com/product/94user94@example.com(554)-920-83412023-07-29 00:00:002023-04-28 00:00:00$482.95This is a sample description for item 94. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 12073.75434.655{\"id\": 1094, \"category\": \"Electronics\", \"rating\": 2.1}951095Alpha-952022-09-16 00:00:00406.731.04.8This is a sample description for item 95. It contains details about the product.FoodWhiteTrueadmin169.5691.310.911.8551.0CODE-2703https://example.com/product/95user95@example.com(610)-557-61782023-08-27 00:00:002022-08-22 00:00:00$406.70This is a sample description for item 95. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 12607.699999999999366.03{\"id\": 1095, \"category\": \"Food\", \"rating\": 4.8}961096Beta-962022-08-03 00:00:00633.1530.01.4This is a sample description for item 96. It contains details about the product.BeautyPurpleTrueuser139.6697.810.892.2827.0CODE-7113https://example.com/product/96user96@example.com(609)-863-12912023-06-26 00:00:002022-07-14 00:00:00$633.15This is a sample description for item 96. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 18994.5569.835{\"id\": 1096, \"category\": \"Beauty\", \"rating\": 1.4}971097Epsilon-972021-05-11 00:00:00157.7153.03.6This is a sample description for item 97. It contains details about the product.ToysRedTrueuser132.1679.815.221.31nullCODE-1659https://example.com/product/97user97@example.com(522)-136-99962021-05-20 00:00:002021-04-28 00:00:00$157.71This is a sample description for item 97. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 8358.630000000001141.93900000000002{\"id\": 1097, \"category\": \"Toys\", \"rating\": 3.6}981098Delta-982024-02-05 00:00:00310.8null1.2This is a sample description for item 98. It contains details about the product.FoodPurpleFalseuser246.6378.34.451.1674.0CODE-3112https://example.com/product/98user98@example.comnull2025-01-14 00:00:002024-01-27 00:00:00$310.80This is a sample description for item 98. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. null279.72{\"id\": 1098, \"category\": \"Food\", \"rating\": 1.2}991099Kappa-992023-12-05 00:00:00127.2791.05.0This is a sample description for item 99. It contains details about the product.HomeGreenTrueuser230.8460.314.752.9529.0CODE-9078https://example.com/product/99user99@example.com(718)-733-59902024-04-12 00:00:002023-12-02 00:00:00$127.27This is a sample description for item 99. It contains details about the product. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. This is additional text to make the description much longer. 11581.57114.54299999999999{\"id\": 1099, \"category\": \"Home\", \"rating\": 5.0}"
  },
  {
    "objectID": "project/2025-07-19-interactive-pandas/index.html#installation",
    "href": "project/2025-07-19-interactive-pandas/index.html#installation",
    "title": "FrameDisplay: Enhanced DataFrame Display",
    "section": "3 Installation",
    "text": "3 Installation\npip install framedisplay"
  },
  {
    "objectID": "project/2025-07-19-interactive-pandas/index.html#usage",
    "href": "project/2025-07-19-interactive-pandas/index.html#usage",
    "title": "FrameDisplay: Enhanced DataFrame Display",
    "section": "4 Usage",
    "text": "4 Usage\nTo display a DataFrame, simply import framedisplay and use the frame_display function:\nimport pandas as pd\nimport numpy as np\nimport framedisplay as fd\n\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', np.nan],\n    'Age': [25, np.nan, 35],\n    'Score': [95.5, 87.2, np.nan]\n})\n\nfd.frame_display(df)\nYou can also enable FrameDisplay globally for all DataFrames in Jupyter by calling fd.integrate_with_pandas():\nimport pandas as pd\nimport framedisplay as fd\n\n# Enable FrameDisplay for all DataFrames\nfd.integrate_with_pandas()\n\n# This will now display using FrameDisplay\ndf"
  },
  {
    "objectID": "project/2025-07-19-interactive-pandas/index.html#how-it-works",
    "href": "project/2025-07-19-interactive-pandas/index.html#how-it-works",
    "title": "FrameDisplay: Enhanced DataFrame Display",
    "section": "5 How it Works",
    "text": "5 How it Works\nFrameDisplay renders your Pandas DataFrame into an HTML table and injects custom CSS and JavaScript to enable interactive features directly in your Jupyter Notebook or browser."
  },
  {
    "objectID": "project/2025-07-19-interactive-pandas/index.html#configuration-optional",
    "href": "project/2025-07-19-interactive-pandas/index.html#configuration-optional",
    "title": "FrameDisplay: Enhanced DataFrame Display",
    "section": "6 Configuration (Optional)",
    "text": "6 Configuration (Optional)\nYou can customize the behavior and appearance by setting a global window.FrameDisplayConfig object in a Jupyter cell before displaying:\nfrom IPython.display import display, HTML\n\ndisplay(HTML(\"\"\"\n&lt;script&gt;\nwindow.FrameDisplayConfig = {\n    minColumnWidth: 30,\n    resizerWidth: 8,\n    resizerHoverColor: 'rgba(0,0,0,0.1)',\n    showHoverEffect: true,\n    autoInit: true,\n    allowReInit: true\n};\n&lt;/script&gt;\n\"\"\"))"
  },
  {
    "objectID": "project/2025-07-19-interactive-pandas/index.html#offline-mode",
    "href": "project/2025-07-19-interactive-pandas/index.html#offline-mode",
    "title": "FrameDisplay: Enhanced DataFrame Display",
    "section": "7 Offline Mode",
    "text": "7 Offline Mode\nIf you are working in an environment without internet access, you can inject the necessary JavaScript and CSS locally by calling initialize() at the start of your notebook. This bundles the required assets into the notebook itself.\nimport framedisplay as fd\nfd.initialize()\n\n# Now you can use fd.frame_display(df) without needing an internet connection"
  },
  {
    "objectID": "project/2025-07-19-interactive-pandas/index.html#license",
    "href": "project/2025-07-19-interactive-pandas/index.html#license",
    "title": "FrameDisplay: Enhanced DataFrame Display",
    "section": "8 License",
    "text": "8 License\nMIT"
  },
  {
    "objectID": "project/2024-10-15-water-supply-forecast-rodeo/index.html#the-competition",
    "href": "project/2024-10-15-water-supply-forecast-rodeo/index.html#the-competition",
    "title": "Water Supply Forecast Rodeo",
    "section": "1 The Competition",
    "text": "1 The Competition\nI participated in the Water Supply Forecast Rodeo, an ML competition with the objective of forecasting water supply in the western United States. I was able to make it to the third (final) round, where our models were evaluated in real-time on unseen data from Jan to Jun 2024. I was able to achieve the 5th place and overall it was a great experience. There were other aspects to the competition such as writing and presenting the implementation details, but my only focus was on the forecasting aspect.\nOne of the biggest challenges for me was the variety of datasets and their different formats, which was overwhelming. On one hand, it was valuable to learn about different climate and hydrology datasets and spend time on pre-processing, feature engineering, and feature selection. On the other hand, I would’ve preferred to spend more time on the actual modeling. Interestingly enough, the model I ended up submitting was something I had created as a baseline to compare against the actual models I was planning to build. My final model, which was a neural network, wasn’t ready before the deadline. I was surprised that the baseline model was able to achieve such a good score.\nI put all of my development work on Github  here. The latest commit has also my incomplete work which didn’t make it to the final submission. If you want to see the codebase in the state when I created the baseline model, you can checkout to git checkout v1.0.0.\nIf you’ve read so far, I wanted to introduce the problem and the datasets that I used."
  },
  {
    "objectID": "project/2024-10-15-water-supply-forecast-rodeo/index.html#problem-statement",
    "href": "project/2024-10-15-water-supply-forecast-rodeo/index.html#problem-statement",
    "title": "Water Supply Forecast Rodeo",
    "section": "2 Problem Statement",
    "text": "2 Problem Statement\nThe challenge was pretty straightforward on paper: predict seasonal water supply at 26 sites across the western United States. Specifically, we needed to forecast the cumulative streamflow volume (measured in thousand acre-feet) for the typical forecast season of April through July. The tricky part was that we had to issue these forecasts multiple times throughout the year - on the 1st, 8th, 15th, and 22nd of each month from January all the way through July.\nWhat made this interesting was that we weren’t just predicting a single number. Instead, we had to provide quantile forecasts - the 0.10, 0.50 (median), and 0.90 quantiles, basically giving a range of possible outcomes. The streamflow data we were predicting represented “natural flow” - essentially what the water flow would’ve been without any human interference like dams or diversions.\n\n\n\n\n                                                \n\n\nFigure 1: Streamflow volume over time at Colville R at Kettle Falls.\n\n\n\n\nThe following interactive map shows the locations and boundaries of the river basins for all 26 sites. I’ve also included supplementary NRCS sites to provide a broader context of the monitoring network and its geographical distribution in the region.\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 2: Interactive map of the 26 competition basins (blue polygons) and supplementary NRCS monitoring sites (red circles). Use the layer control in the top right to switch between different basemaps."
  },
  {
    "objectID": "project/2024-10-15-water-supply-forecast-rodeo/index.html#datasets",
    "href": "project/2024-10-15-water-supply-forecast-rodeo/index.html#datasets",
    "title": "Water Supply Forecast Rodeo",
    "section": "3 Datasets",
    "text": "3 Datasets\nThe competition provided access to a ridiculous amount of data sources. Here’s what was available:\nStreamflow & Antecedent Data:\n\nNRCS/RFCs monthly naturalized flow - historical monthly naturalized flow at forecast sites\nUSGS streamflow - daily observed streamflow measurements from USGS streamgages\nUSBR reservoir inflow - inflow data into Bureau of Reclamation reservoirs\n\nSnowpack Data:\n\nNRCS SNOTEL - automated snow telemetry from 900+ high-elevation sites\nCDEC Snow Sensor Network - California’s snowpack monitoring stations\nSNODAS - daily 1km snow data assimilation system covering continental US\nUA/SWANN - University of Arizona’s neural network-based snow water equivalent data\nMODIS Snow Cover - satellite-based snow cover data at 500m resolution\n\nWeather & Climate:\n\nRCC-ACIS - historical and near real-time climate observations\nCPC Seasonal Outlooks - seasonal temperature and precipitation forecasts\nCopernicus seasonal forecasts - multi-system seasonal meteorological forecasts\nERA5-Land reanalysis - global reanalysis of land variables\nNLDAS-2 forcing data - meteorological variables from North American Land Data Assimilation System\nNCEP/NCAR Reanalysis - gridded atmospheric and land variables\nUSGS SSEBop Evapotranspiration - satellite-based evapotranspiration estimates\n\nDrought & Moisture:\n\nPalmer Drought Severity Index - gridded drought index from gridMET data\nGRACE soil moisture - satellite-based soil moisture and groundwater indicators\n\nClimate Indices:\n\nOceanic Niño Index - El Niño/La Niña indicator from sea surface temperatures\nSouthern Oscillation Index - pressure-based ENSO indicator\nPacific Decadal Oscillation - longer-term Pacific climate pattern\nMadden-Julian Oscillation - tropical weather pattern indices\nPacific North American Index - large-scale atmospheric circulation pattern\n\nVegetation & Land:\n\nMODIS Vegetation Indices - satellite-based vegetation measurements\nCopernicus DEM - 90m resolution digital elevation model\nNLCD Urban Imperviousness - water-resistant surface measurements\nBasinATLAS - comprehensive basin attributes database\n\n\n3.1 Examples\nLet’s look into some of the datasets. It took me a while to create the visualizations when I was working on the competition, so I thought it would be nice to share them here.\n\n\n\n\n\n    \n    \n      \n        \n      \n      \n    \n    \n\n\nFigure 3: Animation of the Daily Mean Palmer Drought Severity Index (PDSI) for the 2022 water year (October 2021 - July 2022). Red areas indicate drier conditions, while blue areas represent wetter conditions.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Snapshot of groundwater storage across the United States on March 24, 2014, from GRACE satellite data. Red areas indicate higher-than-average groundwater levels, while blue areas show lower levels.\n\n\n\n\n\nNext is hydrological analysis of a sample basin using the Copernicus Digital Elevation Model (DEM). These visualizations demonstrate how raw elevation data is processed to model water flow paths, which is a key step in understanding watershed behavior for streamflow forecasting.\n\n\n\n\n\n\n\n\nFigure 5: Original digital elevation model with hillshade overlay\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Depression-filled DEM using RichDEM - Elevation data with small pits and sinks removed\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: D8 flow accumulation on depression-filled DEM - Predicted water flow patterns where brighter areas indicate higher flow accumulation."
  },
  {
    "objectID": "project/2019-06-12-dc-gan/index.html",
    "href": "project/2019-06-12-dc-gan/index.html",
    "title": "DCGAN on MNIST",
    "section": "",
    "text": "A Keras implementation of a custom Deep Convolutional Generative Adversarial Networks (DCGAN) on MNIST dataset. The code is based on the Francois Chollet’s Deep Learning with Python, page 308.\nDCGAN is an extension of the GAN architecture, where the generator and discriminator are deep convolutional networks. The discriminator that takes an image as input and outputs a single scalar value representing the probability that the input image is real (as opposed to fake). The generator takes a random vector as input and decodes it into a synthetic image, and is trained to generate images that the discriminator identifies as real.\nThe code is available on my GitHub.\n\nWhat worked\n\nUsing dropout in the discriminator\n\nAdding noise to the labels\n\nUsing strided convolutions\n\nNormalizing the data to \\([-1, 1]\\)\nRMSprop optimizer\n\n\n\nWhat didn’t work\n\nUsing high learning rates (&gt;1e-3)\nI initially normalized the data between 0 and 1, but since I was using “tanh” as the last layer of the generator’s output, it made the convergence a lot harder\nConstructing separate batches for real and fake images\nTraining the discriminator with more steps than the generator\nUsing SGD optimizer\n\n\n\nResults\nThis is the first 5000 iterations of the training history. The random vectors are fixed at the beginning of the training. As the training progresses, the generator learns to generate more realistic images."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nima Sarang",
    "section": "",
    "text": "Welcome to my personal website! This is where I share what I’m up to - my latest projects, random musings, and adventures.\nI’m always eager to learn new things and connect with people. If you’re up for a collaboration or just want to chat, drop me a line!\nLearn more about me →"
  },
  {
    "objectID": "blog/2025-12-14-gbt-algorithms/index.html#gradient-boosted-model-gbm",
    "href": "blog/2025-12-14-gbt-algorithms/index.html#gradient-boosted-model-gbm",
    "title": "Implementing Gradient Boosted Tree Algorithms from Scratch - LightGBM, XGBoost, CatBoost",
    "section": "4.1 Gradient Boosted Model (GBM)",
    "text": "4.1 Gradient Boosted Model (GBM)\nThis is the main class that ties everything together. It initializes the distribution, data handler, and manages the training process. I think it’s helpful to have a high-level view first before diving into the implementation of each component. The API deliberately mirrors the fit/predict pattern from scikit-learn and familiar GBT libraries but the internals are simple enough to experiment with.\n\n\ngbm.py\n        \n            \n        \n            \n                \n                    #\n                \n                Required imports\n\n            \n            \n                1import numpy as np\n2import pandas as pd\n3from tqdm.auto import tqdm\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                DataPreprocessor handles feature encoding and binning and train/test transformations\n\n            \n            \n                4from .data import DataPreprocessor\n5from .distributions import Distribution\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                TreeGrower handles the logic of growing individual trees\n\n            \n            \n                6from .grower import TreeGrower\n7from .utils import trunc\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Gradient Boosted Model (GBM)\nThis is the user-facing class for training and making predictions.\nParameters\n\ndistribution : Distribution\n\nThe loss distribution to optimize.\n\n\nlearning_rate : float, default=0.1\n\nThe learning rate (shrinkage factor) for each tree's contribution.\n\n\nn_trees : int, default=100\n\nThe number of trees to grow in the ensemble.\n\n\nn_leaves : int, default=20\n\nThe maximum number of leaves per tree.\n\n\nmax_depth : int, default=6\n\nThe maximum depth of each tree.\n\n\nmin_samples_split : int, default=2\n\nThe minimum number of samples required to split a node.\n\n\nmin_weight_leaf : float, default=20\n\nThe minimum sum of instance weights required in a leaf node for a split to be considered.\n\n\nmin_gain_to_split : float, default=0.0\n\nThe minimum gain required to perform a split. Referred to as \\(\\gamma\\) in some literature.\n\n\nobjective_weight : List[float], optional\n\nWeights for multi-output objectives. If the distribution is multivariate,\nthis specifies the relative importance of each output dimension.\n\n\nl1_regularization : float, default=0.0\n\nL1 regularization term on leaf values.\n\n\nl2_regularization : float, default=0.0\n\nL2 regularization term on leaf values.\n\n\nfeature_fraction : float, default=1.0\n\nThe fraction of features to subsample for each tree (Values between 0 and 1). This helps with regularization.\n\n\nsamples_fraction : float, default=1.0\n\nThe fraction of training samples to subsample for each tree. Values between 0 and 1.\n\n\ngoss_top_rate : float, optional\n\nWhether to use GOSS sampling. If set, specifies the top fraction of samples (by absolute gradient) to keep.\n\n\ngoss_bottom_rate : float, optional\n\nIf using GOSS, specifies the bottom fraction of samples (by absolute gradient) to keep.\n\n\ngoss_rescale_bottom : bool, optional\n\nWhether to rescale the weights of the bottom samples in GOSS to maintain the overall weight sum.\n\n\npath_smoothing_strength : float, default=0.0\n\nStrength of path smoothing to apply to leaf values after tree is grown.\n\n\nmax_bins : int, default=255\n\nThe maximum number of bins to use for numerical features.\n\n\nmax_categories : int, default=100\n\nThe maximum number of unique categories for a feature to be treated as categorical. Otherwise, it is encoded as numerical using label encoding.\n\n\nn_permutations : int, default=10\n\nNumber of random permutations to use for ordered target statistics encoding of categorical features (CatBoost-style).\n\n\nprior_strength : float, default=1.0\n\nThe strength of the prior for target statistics encoding.\n\n\nseed : int, optional\n\nRandom seed for reproducibility.\n\n\nverbose : bool, default=True\n\nWhether to print progress messages during training.\n\n\n\n\n            \n            \n                 8class GradientBoostedModel:\n 9    def __init__(\n10        self,\n11        distribution: Distribution,\n12        learning_rate: float = 0.1,\n13        n_trees: int = 100,\n14        n_leaves: int = 20,\n15        max_depth: int = 6,\n16        min_samples_split: int = 2,\n17        min_weight_leaf: float = 20,\n18        min_gain_to_split: float = 0.0,\n19        objective_weight: list[float] = None,\n20        l1_regularization: float = 0.0,\n21        l2_regularization: float = 0.0,\n22        feature_fraction: float = 1.0,\n23        samples_fraction: float = 1.0,\n24        goss_top_rate: float = None,\n25        goss_bottom_rate: float = None,\n26        goss_rescale_bottom: bool = False,\n27        path_smoothing_strength: float = 0.0,\n28        max_bins: int = 255,\n29        max_categories: int = 100,\n30        n_permutations: int = 10,\n31        prior_strength: float = 1.0,\n32        seed: int = None,\n33        verbose: bool = True,\n34    ):\n35        self.distribution = distribution\n36        self.learning_rate = learning_rate\n37        self.n_trees = n_trees\n38        self.n_leaves = n_leaves\n39        self.max_depth = max_depth\n40        self.min_samples_split = min_samples_split\n41        self.min_weight_leaf = min_weight_leaf\n42        self.min_gain_to_split = min_gain_to_split\n43        self.objective_weight = objective_weight\n44        self.l1_regularization = l1_regularization\n45        self.l2_regularization = l2_regularization\n46        self.feature_fraction = feature_fraction\n47        self.samples_fraction = samples_fraction\n48        self.goss_top_rate = goss_top_rate\n49        self.goss_bottom_rate = goss_bottom_rate\n50        self.goss_rescale_bottom = goss_rescale_bottom\n51        self.path_smoothing_strength = path_smoothing_strength\n52        self.max_bins = max_bins\n53        self.max_categories = max_categories\n54        self.n_permutations = n_permutations\n55        self.prior_strength = prior_strength\n56        self.seed = seed\n57        self.verbose = verbose\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Attributes to be set during fitting\n\n            \n            \n                58        self.data_processor_ = None\n59        self.initial_params_ = None\n60        self.trees_ = []\n61        self.is_fitted_ = False\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Fit the Gradient Boosted Model to the data. This method should give you a high-level\noverview of the training process.\n\nX : pd.DataFrame\n\nThe input features.\n\n\ny : np.ndarray\n\nThe target values.\n\n\nsample_weight : np.ndarray, optional\n\nSample weights for each instance.\n\n\n\n\n            \n            \n                62    def fit(self, X: pd.DataFrame, y: np.ndarray, sample_weight: np.ndarray = None):\n63        if sample_weight is None:\n64            sample_weight = np.ones(len(X))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Preprocess data and compute feature info\n\n            \n            \n                65        self.data_processor_ = DataPreprocessor(\n66            max_categories=self.max_categories,\n67            max_bins=self.max_bins,\n68            n_permutations=self.n_permutations,\n69            prior_strength=self.prior_strength,\n70            seed=self.seed,\n71        )\n72        self.data_processor_.fit(X, y, sample_weight)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Transform input data into numerical array\n\n            \n            \n                73        inputs, targets = self.data_processor_.transform(\n74            X, y, sample_weight, mode=\"train\"\n75        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize tree grower. This handles the logic of growing individual trees.\n\n            \n            \n                76        grower = TreeGrower(\n77            distribution=self.distribution,\n78            learning_rate=self.learning_rate,\n79            objective_weight=self.objective_weight,\n80            l1_regularization=self.l1_regularization,\n81            l2_regularization=self.l2_regularization,\n82            min_weight_leaf=self.min_weight_leaf,\n83            max_leaves=self.n_leaves,\n84            max_depth=self.max_depth,\n85            goss_top_rate=self.goss_top_rate,\n86            goss_bottom_rate=self.goss_bottom_rate,\n87            goss_rescale_bottom=self.goss_rescale_bottom,\n88            feature_fraction=self.feature_fraction,\n89            samples_fraction=self.samples_fraction,\n90            min_samples_split=self.min_samples_split,\n91            min_gain_to_split=self.min_gain_to_split,\n92            seed=self.seed,\n93        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize the ensemble. Query the Distribution object for initial parameters.\n\n            \n            \n                94        self.initial_params_ = self.distribution.init_params(y)\n95        predictions = np.full((len(y), len(self.initial_params_)), self.initial_params_)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Print initial loss\n\n            \n            \n                96        if self.verbose:\n97            loss = -self.distribution.log_prob(y, predictions).sum()\n98            print(f\"Initial loss: {trunc(loss)}\")\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Grow trees one by one\n\n            \n            \n                 99        for i in tqdm(range(self.n_trees)):\n100            tree = grower.grow(\n101                inputs=inputs,\n102                features_info=self.data_processor_.features_info_,\n103                targets=targets,\n104                sample_weight=sample_weight,\n105                current_predictions=predictions,\n106            )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                If no valid tree could be grown, stop early.\nCould happen due to insufficient data or no tangible gain.\n\n            \n            \n                107            if tree is None:\n108                break\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Apply path smoothing if specified\n\n            \n            \n                109            if self.path_smoothing_strength &gt; 0.0:\n110                tree.apply_smoothing(self.path_smoothing_strength)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Append the new tree and update predictions\n\n            \n            \n                111            self.trees_.append(tree)\n112            predictions += self.learning_rate * tree.predict(inputs)\n113\n114            if self.verbose:\n115                loss = -self.distribution.log_prob(y, predictions).sum()\n116                print(f\"Loss after tree {i + 1}: {trunc(loss)}\")\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                117        self.is_fitted_ = True\n118        return self\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Make inferences using the fitted model.\n\nX : pd.DataFrame\n\nThe input features.\n\n\nreturn_type : str or None, default=\"predict\"\n\nTransformation to apply to raw predictions. Options depend on the distribution.\nIt basically passes the raw predictions to the distribution's corresponding method.\nIf None, returns raw predictions.\n\n\n\n\n            \n            \n                119    def predict(self, X, return_type=\"predict\"):\n120        if not self.is_fitted_:\n121            raise ValueError(\"Model not fitted. Call fit() first.\")\n122\n123        X = self.data_processor_.transform(X)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Start with the initial parameters and add contributions from each tree\n\n            \n            \n                124        raw_prediction = np.full(\n125            (len(X), len(self.initial_params_)), self.initial_params_\n126        )\n127        for tree in self.trees_:\n128            raw_prediction += self.learning_rate * tree.predict(X)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Return the desired output type. Calls the corresponding implementation in the Distribution class.\n\n            \n            \n                129        if return_type is not None:\n130            return getattr(self.distribution, return_type)(raw_prediction)\n131        return raw_prediction"
  },
  {
    "objectID": "blog/2025-12-14-gbt-algorithms/index.html#data-preprocessing",
    "href": "blog/2025-12-14-gbt-algorithms/index.html#data-preprocessing",
    "title": "Implementing Gradient Boosted Tree Algorithms from Scratch - LightGBM, XGBoost, CatBoost",
    "section": "4.2 Data Preprocessing",
    "text": "4.2 Data Preprocessing\nThis module handles categorical feature encoding and numerical feature binning. The transformations are used both during training and inference time.\n\n\ndata.py\n        \n            \n        \n            \n                \n                    #\n                \n                Data preprocessing utilities module. Defines classes and functions for handling\nfeature information and preprocessing datasets.\n\n            \n            \n                1from dataclasses import dataclass\n2\n3import numpy as np\n4import pandas as pd\n5\n6from .utils import map_array\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                FeatureInfo\nA class to store information about a feature in the dataset.\nAttributes\n\nname : str\n\nThe name of the feature/column.\n\n\nindex : int\n\nThe position of the feature in the input array.\n\n\ntype : str, optional\n\nThe type of the feature: numerical, categorical, or category_as_numeric.\n\n\nbins : np.ndarray, optional\n\nThe bin edges for numerical features or categorical features treated as\nnumeric.\n\n\ncategories : np.ndarray, optional\n\nThe unique categories for categorical features.\n\n\ntarget_statistics : dict, optional\n\nA mapping from category to its target statistic for categorical features\nencoded as numeric.\n\n\n\n\n            \n            \n                 7@dataclass\n 8class FeatureInfo:\n 9\n10    name: str\n11    index: int\n12    type: str = None\n13    bins: np.ndarray = None\n14    categories: np.ndarray = None\n15    target_statistics: dict = None\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                DataPreprocessor\nA data preprocessor for handling numerical and categorical features. It's used\nduring both training and prediction.\nParameters\n\nmax_categories : int, default=100\n\nMaximum number of unique categories for a feature to be treated as\ncategorical. Otherwise, it is encoded as numerical using label encoding.\n\n\nmax_bins : int, default=255\n\nNumber of bins to use for numerical features and categorical features\ntreated as numeric.\n\n\nn_permutations : int, default=10\n\nNumber of random permutations to use for ordered target statistics\nencoding of categorical features.\n\n\nseed : int, optional\n\nRandom seed for reproducibility.\n\n\n\n\n            \n            \n                16class DataPreprocessor:\n17    def __init__(\n18        self,\n19        max_categories: int = 100,\n20        max_bins: int = 255,\n21        n_permutations: int = 10,\n22        prior_strength: float = 1.0,\n23        seed: int = None,\n24    ):\n25        self.max_categories = max_categories\n26        self.max_bins = max_bins\n27        self.n_permutations = n_permutations\n28        self.prior_strength = prior_strength\n29        self.random = np.random.default_rng(seed)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                A list of FeatureInfo objects, one for each feature in the dataset. To be populated\nduring the fit method.\n\n            \n            \n                30        self.features_info_ = None\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Create histograms of features in the dataset. Categorical and numerical\nfeatures are handled differently.\n\n            \n            \n                31    def fit(\n32        self, X: pd.DataFrame, y: np.ndarray = None, sample_weight: np.ndarray = None\n33    ) -&gt; \"DataPreprocessor\":\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Create feature info for each column\n\n            \n            \n                34        features_info = []\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Identify categorical features\n\n            \n            \n                35        categorical_features = X.select_dtypes(\n36            include=[\"category\", \"object\", \"string\"]\n37        ).columns\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Process each feature individually\n\n            \n            \n                38        for index, col in enumerate(X.columns):\n39            info = FeatureInfo(name=col, index=index)\n40            values = None\n41            category_as_numeric = False\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Handle categorical features\n\n            \n            \n                42            if col in categorical_features:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Use pandas' categorical type to get the unique categories.\nNaN values are considered as a separate category (-1 code)\n\n            \n            \n                43                category_col = X[col].astype(\"category\")\n44                info.categories = category_col.cat.categories\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                If too many categories, use CatBoost-style target encoding\n\n            \n            \n                45                if len(info.categories) &gt; self.max_categories:\n46                    category_as_numeric = True\n47                    assert y is not None and sample_weight is not None\n48                    target_mean = np.average(y, weights=sample_weight)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Convert categories to codes for grouping\n\n            \n            \n                49                    codes = category_col.cat.codes.to_numpy()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Aggregate \\(y * w\\) and \\(w\\) by category. Ignore -1 (NaN) codes.\n\n            \n            \n                50                    valid = codes &gt;= 0\n51                    w_sum = np.bincount(\n52                        codes[valid], weights=(y * sample_weight)[valid]\n53                    )\n54                    w_total = np.bincount(codes[valid], weights=sample_weight[valid])\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute TS for category \\(c\\) as:\n$$TS(c) = \\frac{\\sum_{i \\in c} w_i y_i + \\alpha \\cdot \\mu}{\\sum_{i \\in c} w_i + \\alpha}$$\nwhere \\(\\mu\\) is the global weighted mean of the target, and \\(\\alpha\\) is\nthe prior strength.\n\n            \n            \n                55                    info.target_statistics = dict(\n56                        enumerate(\n57                            (w_sum + self.prior_strength * target_mean)\n58                            / (w_total + self.prior_strength)\n59                        )\n60                    )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Map the codes to target statistics for binning in the next step\n\n            \n            \n                61                    values = map_array(codes, info.target_statistics)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                It's a categorical feature. No binning needed.\n\n            \n            \n                62                else:\n63                    info.type = \"categorical\"\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Bin numerical and categorical-as-numeric features\n\n            \n            \n                64            if col not in categorical_features or category_as_numeric:\n65                if values is None:\n66                    values = X[col].to_numpy()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute quantile-based bins with no interpolation\n\n            \n            \n                67                quantiles = np.nanquantile(\n68                    values,\n69                    np.linspace(0.0, 1.0, endpoint=True, num=self.max_bins + 1),\n70                    method=\"nearest\",\n71                )\n72                info.bins = np.unique(quantiles).astype(\"float64\")\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Set feature type\n\n            \n            \n                73                if col in categorical_features:\n74                    info.type = \"category_as_numeric\"\n75                else:\n76                    info.type = \"numerical\"\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                77            features_info.append(info)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                78        self.features_info_ = features_info\n79        return self\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Transform the dataset into a numerical numpy array based on the fitted\nfeature information.\n\n            \n            \n                80    def transform(\n81        self,\n82        X: pd.DataFrame,\n83        y: np.ndarray = None,\n84        sample_weight: np.ndarray = None,\n85        mode=\"test\",\n86    ) -&gt; np.ndarray:\n87        array = X[[info.name for info in self.features_info_]].values.copy()\n88        if y is not None:\n89            if isinstance(y, pd.Series):\n90                y = y.to_numpy()\n91            y = y.astype(\"float64\")\n92\n93        for i, info in enumerate(self.features_info_):\n94            if info.type == \"categorical\":\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Transform categorical features to integer codes. The reason we use\ninteger code rather than keeping original string values is that the transformed\ndataset is supposed to be a homogeneous float64 numpy array.\n\n            \n            \n                95                codes = pd.Categorical(\n96                    X[info.name], categories=info.categories\n97                ).codes.astype(\"float64\")\n98                codes[codes == -1.0] = np.nan\n99                array[:, i] = codes\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                CatBoost-style target statistics encoding. Train and test modes differ.\n\n            \n            \n                100            elif info.type == \"category_as_numeric\":\n101                category_col = pd.Categorical(X[info.name], categories=info.categories)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                If test mode, use the precomputed target statistics mapping\n\n            \n            \n                102                if mode == \"test\":\n103                    array[:, i] = map_array(category_col.codes, info.target_statistics)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                For training mode, compute ordered target statistics averaged over\nmultiple random permutations\n\n            \n            \n                104                elif mode == \"train\":\n105                    assert y is not None and sample_weight is not None\n106                    target_mean = np.average(y, weights=sample_weight)\n107                    codes = category_col.codes.astype(\"int64\")\n108                    encodings = np.empty((len(X), self.n_permutations))\n109                    n_cats = max(codes) + 1\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                We impose a random order on the data and for every sample \\(i\\),\ncompute the target statistic using only samples that come before\nit in this random order. This is to avoid target leakage.\n\n            \n            \n                110                    for p in range(self.n_permutations):\n111                        group_sum = np.zeros(n_cats)\n112                        group_weight = np.zeros(n_cats)\n113                        for idx in self.random.permutation(len(X)):\n114                            c = codes[idx]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                If the category is -1 (NaN), assign NaN and continue\n\n            \n            \n                115                            if c &lt; 0:\n116                                encodings[idx, p] = np.nan\n117                                continue\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Same formula as in fit(), but using only previous samples\n\n            \n            \n                118                            encodings[idx, p] = (\n119                                group_sum[c] + self.prior_strength * target_mean\n120                            ) / (group_weight[c] + self.prior_strength)\n121                            group_sum[c] += y[idx] * sample_weight[idx]\n122                            group_weight[c] += sample_weight[idx]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Average over all permutations\n\n            \n            \n                123                    array[:, i] = np.mean(encodings, axis=1)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                124        array = array.astype(\"float64\")\n125        if y is not None:\n126            return array, y\n127        return array"
  },
  {
    "objectID": "blog/2025-12-14-gbt-algorithms/index.html#node-and-tree-structures",
    "href": "blog/2025-12-14-gbt-algorithms/index.html#node-and-tree-structures",
    "title": "Implementing Gradient Boosted Tree Algorithms from Scratch - LightGBM, XGBoost, CatBoost",
    "section": "4.3 Node and Tree Structures",
    "text": "4.3 Node and Tree Structures\nHere we define the skeleton of a decision tree and its nodes. Each node stores information about split criterion, value, …, and child nodes.\nThe implementation is kept lean to focus more on the structure rather than optimization details.\n\n\ntree.py\n        \n            \n        \n            \n                \n                    #\n                \n                \n            \n            \n                1from dataclasses import dataclass, field\n2from typing import Any, Dict, List, Tuple, Union\n3\n4import numpy as np\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                A class representing a node in a decision tree.\nAttributes\n\ndepth : int\n\nThe depth of the node in the tree.\n\n\nsample_indices : List[int]\n\nThe indices of training samples that reach this node. Used during training.\n\n\nparent : Node, optional\n\nThe parent node. None for the root node.\n\n\nchildren : Dict[str, Node]\n\nThe child nodes, typically with keys \"left\" and \"right\".\n\n\ngradient_sum : np.ndarray\n\nThe sum of gradients for samples in this node.\n\n\nhessian_sum : np.ndarray\n\nThe sum of hessians for samples in this node.\n\n\nl1_regularization : float\n\nThe L1 regularization term.\n\n\nl2_regularization : float\n\nThe L2 regularization term.\n\n\nfeature : str, optional\n\nFeature name used for splitting at this node.\n\n\nfeature_index : int, optional\n\nIndex of the feature in the input array.\n\n\nsplit_point : Union[float, Tuple], optional\n\nThe split point for numerical features or a tuple of categories for categorical features.\n\n\nnulls_to_left : bool, optional\n\nWhether null values go to the left child.\n\n\nsmoothed_value : np.ndarray, optional\n\nThe smoothed value for the node, used for prediction. Only set for leaf nodes.\n\n\ninfo : Dict[str, Any]\n\nAdditional information about the node.\n\n\n\n\n            \n            \n                 5@dataclass\n 6class Node:\n 7\n 8    depth: int\n 9    sample_indices: List[int] = field(default_factory=list)\n10    parent: \"Node\" = None\n11    children: Dict[str, \"Node\"] = field(default_factory=dict)\n12    gradient_sum: np.ndarray = None\n13    hessian_sum: np.ndarray = None\n14    l1_regularization: float = 0.0\n15    l2_regularization: float = 0.0\n16    feature: str = None\n17    feature_index: int = None\n18    split_point: Union[float, Tuple] = None\n19    nulls_to_left: bool = None\n20    smoothed_value: np.ndarray = None\n21    info: Dict[str, Any] = field(default_factory=dict)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the leaf value using the formula:\n$$ \\text{Value} = - \\frac{\\text{sign}(G) \\cdot \\max(|G| - \\lambda_{\\text{L1}}, 0)}{H + \\lambda_{\\text{L2}}} $$\n\n            \n            \n                22    def value(self, smooth: bool = True) -&gt; float:\n23        if smooth and self.smoothed_value is not None:\n24            return self.smoothed_value\n25\n26        gradient = self.gradient_sum\n27        if self.l1_regularization:\n28            gradient = np.sign(gradient) * np.maximum(\n29                np.abs(gradient) - self.l1_regularization, 0.0\n30            )\n31        return -gradient / (self.hessian_sum + self.l2_regularization + 1e-16)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Returns the type of the node: InternalNode, Leaf, or VirtualNode.\nVirtualNode is a special node used only during training to represent nodes that\nhave not been instantiated yet.\n\n            \n            \n                32    @property\n33    def type(self) -&gt; str:\n34        if self.children:\n35            return \"InternalNode\"\n36        elif self.gradient_sum is not None and self.hessian_sum is not None:\n37            return \"Leaf\"\n38        else:\n39            return \"VirtualNode\"\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Returns a boolean mask indicating which samples go to the left child.\nParameters\n\narray : np.ndarray\n\nThe input feature array.\n\n\ninput_type : str, default=\"all\"\n\nSpecifies whether the input array contains all features (\"all\") or just the feature\nused for splitting (\"feature\").\n\n\n\n\n            \n            \n                40    def criterion(self, array: np.ndarray, input_type: str = \"all\") -&gt; np.ndarray:\n41        if input_type == \"all\":\n42            array = array[:, self.feature_index]\n43        null_mask = np.isnan(array) & self.nulls_to_left\n44        if isinstance(self.split_point, tuple):\n45            return null_mask | np.isin(array, self.split_point[0])\n46        else:\n47            return null_mask | (array &lt;= self.split_point)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                String representation of the node. Used for visualization and debugging.\n\n            \n            \n                48    def __repr__(self):\n49        if self.children:\n50            return f\"{self.type}(feature='{self.feature}', split_point={self.split_point}, samples={len(self.sample_indices)})\"\n51        elif self.gradient_sum is not None and self.hessian_sum is not None:\n52            return f\"{self.type}(value={self.value().round(decimals=4)}, samples={len(self.sample_indices)})\"\n53        else:\n54            return f\"{self.type}(samples={len(self.sample_indices)})\"\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Helper method to update a dataclass's fields.\n\n            \n            \n                55    def update(self, **kwargs):\n56        for key, value in kwargs.items():\n57            if not hasattr(self, key):\n58                raise AttributeError(f\"Node has no attribute '{key}'\")\n59            setattr(self, key, value)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                A structure representing a decision tree.\n\n            \n            \n                60class Tree:\n61    def __init__(self, root: Node = None):\n62        self.root = root\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Make predictions for the input samples x starting from the given node (or root if None).\nWe traverse the tree from the root to the leaves based on the split criteria at each node, and aggregate\nthe leaf values to produce the final predictions.\nParameters\n\nx : np.ndarray\n\nThe input samples for which predictions are to be made, with shape (n_samples, n_features).\n\n\nnode : Node, optional\n\nThe current node in the tree from which to start predictions. If None, starts from the root node.\n\n\n\nReturns\n\noutput : np.ndarray\n\nThe leaf values for each input sample, with shape (n_samples, n_outputs).\n\n\n\n\n            \n            \n                63    def predict(self, x: np.ndarray, node: Node = None):\n64        node = node or self.root\n65        n_samples = len(x)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                If the node is an internal node, split the samples and recurse\n\n            \n            \n                66        if node.children:\n67            mask = node.criterion(x)\n68            n_output = len(node.children[\"left\"].value())\n69            output = np.zeros((n_samples, n_output))\n70            output[mask] = self.predict(x[mask], node.children[\"left\"])\n71            output[~mask] = self.predict(x[~mask], node.children[\"right\"])\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                If in the leaf node, return the leaf value for all samples\n\n            \n            \n                72        else:\n73            output = np.tile(node.value(), (n_samples, 1))\n74        return output\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Returns the number of leaves in the tree.\n\n            \n            \n                75    def __len__(self):\n76        return len(self.leaves)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Iterator to traverse all nodes in the tree using depth-first search.\n\n            \n            \n                77    def __iter__(self):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Utility for traversal of the tree. Since it's a recursive generator, we need\nto use yield instead of return to yield nodes one by one. On a similar note,\nyield from is used for calling the generator recursively.\nIt's a rarely used syntax but I find it a neat feature of Python :)\n\n            \n            \n                78        def dfs(node: Node):\n79            yield node\n80            for child in node.children.values():\n81                yield from dfs(child)\n82\n83        if self.root:\n84            yield from dfs(self.root)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Returns a list of all leaf nodes in the tree. We use the iter method to traverse the tree.\n\n            \n            \n                85    @property\n86    def leaves(self) -&gt; List[Node]:\n87        return [node for node in self if node.type == \"Leaf\"]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Apply exponential path smoothing to all leaf values (in-place).\nBlends each leaf with ancestors using exponential decay: leaf gets weight 1,\nparent gets \\(\\beta\\), grandparent gets \\(\\beta^2\\), etc.\n$$v_{\\text{smoothed}} = \\frac{\\sum_{i=0}^{d} \\beta^{d-i} \\cdot v_i}{\\sum_{i=0}^{d} \\beta^{d-i}}$$\nParameters\n\nbeta : float in (0, 1], default=0.5\n\nDecay factor. Lower values = stronger regularization toward shallow predictions.\n\n\n\n\n            \n            \n                88    def apply_smoothing(self, beta: float):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                89        def dfs(node: Node, weighted_sum: np.ndarray, weight_total: float):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Add current node's contribution\n\n            \n            \n                90            weighted_sum = weighted_sum + node.value(smooth=False)\n91            weight_total = weight_total + 1.0\n92\n93            if node.type == \"Leaf\":\n94                node.smoothed_value = weighted_sum / weight_total\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Discount contributions from ancestors and recurse\n\n            \n            \n                95            else:\n96                weighted_sum = weighted_sum * beta\n97                weight_total = weight_total * beta\n98                for child in node.children.values():\n99                    dfs(child, weighted_sum, weight_total)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                100        initial_sum = np.zeros_like(self.root.value(smooth=False))\n101        dfs(self.root, initial_sum, 0.0)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Print the tree structure in a readable format.\n\n            \n            \n                102    def print(self, node=None, level=0, prefix=\"Root: \"):\n103        node = node or self.root\n104        print(\" \" * (level * 4) + prefix + str(node))\n105        if node.children:\n106            self.print(node.children.get(\"left\"), level + 1, prefix=\"L--- \")\n107            self.print(node.children.get(\"right\"), level + 1, prefix=\"R--- \")"
  },
  {
    "objectID": "blog/2025-12-14-gbt-algorithms/index.html#tree-optimization",
    "href": "blog/2025-12-14-gbt-algorithms/index.html#tree-optimization",
    "title": "Implementing Gradient Boosted Tree Algorithms from Scratch - LightGBM, XGBoost, CatBoost",
    "section": "4.4 Tree Optimization",
    "text": "4.4 Tree Optimization\nThis module implements the core logic for creating decision trees used in Gradient Boosted Trees. The class TreeGrower is responsible for growing the tree by finding the best splits based on the gradients and hessians. I stole the name “TreeGrower” from Sklearn’s implementation, but not sure if they were the first.\nThe flow of the algorithm is as follows:\n\nSubsample the data if specified. This is to introduce randomness and prevent overfitting. Training on different subsets of sample and features results in a more diverse set of trees.\nInitialize the root node.\nUse a priority queue to keep track of all node candidates to be split, and prioritize them based on their potential to reduce loss.\nIteratively:\n\nEvaluate the best split and gain for the unprocessed nodes, and update the priority queue.\nSelect the node with the highest gain from the priority queue to split. Update its information, and create its child nodes to be processed in the next iteration.\nStop when the maximum number of leaves is reached, or no more beneficial splits are found.\n\n\nThis is called leaf-wise growth, as opposed to level-wise growth in traditional XGBoost, and also CatBoost since it uses oblivious trees (as far as I know).\n\n\ngrower.py\n        \n            \n        \n            \n                \n                    #\n                \n                \n            \n            \n                 1from dataclasses import dataclass\n 2from queue import PriorityQueue\n 3from typing import List, Tuple, Union\n 4\n 5import numpy as np\n 6\n 7from .data import FeatureInfo\n 8from .tree import Node, Tree\n 9from .distributions import Distribution\n10from .utils import groupby_sum_2d\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                SplitCandidate\nA class to store information about a potential node split during tree growth.\nAttributes\n\ngain : float\n\nThe gain achieved by this split.\n\n\nfeature : str\n\nThe feature on which to split.\n\n\nfeature_index : int\n\nThe index of the feature in the input array.\n\n\nsplit_point : Union[float, Tuple]\n\nFor numerical features, the threshold value. For categorical features, a tuple of\ntwo arrays representing the left and right category groups.\n\n\nnulls_to_left : bool\n\nWhether null values go to the left child.\n\n\ngradient_sum : np.ndarray\n\nThe sum of gradients for samples in the node.\n\n\nhessian_sum : np.ndarray\n\nThe sum of hessians for samples in the node.\n\n\nleft_grad_sum : np.ndarray\n\nThe sum of gradients for samples going to the left child.\n\n\nleft_hess_sum : np.ndarray\n\nThe sum of hessians for samples going to the left child.\n\n\n\n\n            \n            \n                11@dataclass\n12class SplitCandidate:\n13\n14    gain: float\n15    feature: str = None\n16    feature_index: int = None\n17    split_point: Union[float, Tuple] = None\n18    nulls_to_left: bool = None\n19    gradient_sum: np.ndarray = None\n20    hessian_sum: np.ndarray = None\n21    left_grad_sum: np.ndarray = None\n22    left_hess_sum: np.ndarray = None\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                23    @property\n24    def right_grad_sum(self) -&gt; np.ndarray:\n25        return self.gradient_sum - self.left_grad_sum\n26\n27    @property\n28    def right_hess_sum(self) -&gt; np.ndarray:\n29        return self.hessian_sum - self.left_hess_sum\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                TreeGrower\nA class to grow decision trees. The core logic for finding the best splits and constructing\nthe tree structure is implemented here.\nParameters\n\ndistribution : Distribution\n\nThe objective function defining the loss and gradient/hessian computations.\n\n\nlearning_rate : float\n\nThe learning rate for boosting.\n\n\nmax_leaves : int\n\nThe maximum number of leaves in the tree.\n\n\nmax_depth : int\n\nThe maximum depth of the tree.\n\n\nmin_samples_split : int\n\nThe minimum number of samples required to split a node.\n\n\nmin_gain_to_split : float\n\nThe minimum gain required to perform a split.\n\n\nl1_regularization : float\n\nThe L1 regularization term for leaf value calculation.\n\n\nl2_regularization : float\n\nThe L2 regularization term for leaf value calculation.\n\n\nmin_weight_leaf : float\n\nThe minimum sum of instance weights required in a leaf node.\n\n\nfeature_fraction : float\n\nThe fraction of features to consider when looking for the best split.\n\n\nsamples_fraction : float\n\nThe fraction of samples to consider when growing the tree.\n\n\ngoss_top_rate : float\n\nThe top rate for Gradient-based One-Side Sampling (GOSS).\n\n\ngoss_bottom_rate : float\n\nThe bottom rate for GOSS.\n\n\ngoss_rescale_bottom : bool\n\nWhether to rescale the bottom samples in GOSS.\n\n\nobjective_weight : List[float]\n\nWeights for multi-output objectives.\n\n\nn_workers : int\n\nThe number of parallel workers to use for finding splits.\n\n\nseed : int\n\nRandom seed for reproducibility.\n\n\n\n\n            \n            \n                30@dataclass\n31class TreeGrower:\n32\n33    distribution: Distribution\n34    learning_rate: float\n35    max_leaves: int\n36    max_depth: int = 100\n37    min_samples_split: int = 2\n38    min_gain_to_split: float = 0.0\n39    l1_regularization: float = 0.0\n40    l2_regularization: float = 0.0\n41    min_weight_leaf: float = float(\"-inf\")\n42    feature_fraction: float = 1.0\n43    samples_fraction: float = 1.0\n44    goss_top_rate: float = None\n45    goss_bottom_rate: float = None\n46    goss_rescale_bottom: bool = False\n47    objective_weight: List[float] = None\n48    n_workers: int = None\n49    seed: int = None\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize the random state for rng operations\n\n            \n            \n                50    def __post_init__(self):\n51        self.random = np.random.default_rng(self.seed)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Train a decision tree using the provided dataset.\nParameters\n\ninputs : np.ndarray\n\nThe input feature array.\n\n\nfeatures_info : List[FeatureInfo]\n\nList of FeatureInfo objects describing each feature.\n\n\ntargets : np.ndarray\n\nThe target values.\n\n\nsample_weight : np.ndarray\n\nSample weights for each instance.\n\n\ncurrent_predictions : np.ndarray\n\nThe current predictions from the ensemble. The goal of adding a new tree is to\ncorrect these predictions.\n\n\n\n\n            \n            \n                52    def grow(\n53        self,\n54        inputs: np.ndarray,\n55        features_info: List[\"FeatureInfo\"],\n56        targets: np.ndarray,\n57        sample_weight: np.ndarray,\n58        current_predictions: np.ndarray,\n59    ) -&gt; Tree:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Subsample features and samples if required\n\n            \n            \n                60        inputs, features_info, targets, sample_weight, current_predictions = (\n61            self.subsample_dataset(\n62                inputs,\n63                features_info,\n64                targets,\n65                sample_weight,\n66                current_predictions,\n67            )\n68        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Default sample weights to 1 if not provided\n\n            \n            \n                69        if sample_weight is None:\n70            sample_weight = np.ones(len(inputs), dtype=np.float64)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute gradients and hessians for the samples in the node\n\n            \n            \n                71        gradient, hessian = self.distribution.gradient_hessian(\n72            targets,\n73            current_predictions,\n74            sample_weight,\n75        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                NOTE: I don't have a scientific justification for this clipping, but I wanted to avoid\nhaving division by zero errors during gain calculation, and I also saw XGBoost do something\nsimilar. Will have to revisit this later.\n\n            \n            \n                76        hessian = np.maximum(hessian, 1e-6)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                GOSS Sampling\n\n            \n            \n                77        if self.goss_top_rate is not None and self.goss_bottom_rate is not None:\n78            (\n79                gradient,\n80                hessian,\n81                inputs,\n82                targets,\n83                sample_weight,\n84            ) = self.apply_goss_sampling(\n85                gradient, hessian, inputs, targets, sample_weight\n86            )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize the tree with a root node\n\n            \n            \n                87        root = Node(\n88            depth=0,\n89            l2_regularization=self.l2_regularization,\n90            sample_indices=np.arange(len(inputs)),\n91        )\n92        tree = Tree(root=root)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Priority queue to store split candidates. Higher gain splits have higher priority.\n\n            \n            \n                93        candidates = PriorityQueue()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Start growing the tree\n\n            \n            \n                94        leaves_to_process = [root]\n95        n_processed = 0\n96        for _ in range(self.max_leaves):\n97            for node in leaves_to_process:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Check if node can be split\n\n            \n            \n                 98                if (\n 99                    node.depth &gt;= self.max_depth\n100                    or len(node.sample_indices) &lt; self.min_samples_split\n101                ):\n102                    continue\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Find the best split for the given node\n\n            \n            \n                103                split_candidate = self.find_best_split(\n104                    inputs[node.sample_indices],\n105                    features_info,\n106                    gradient[node.sample_indices],\n107                    hessian[node.sample_indices],\n108                    sample_weight[node.sample_indices],\n109                )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Add the split proposal to the queue if it satisfies min gain\n\n            \n            \n                110                if (split_candidate is not None) and (\n111                    split_candidate.gain &gt; self.min_gain_to_split\n112                ):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                The queue stores tuples of the form (-gain, node_index, node, split_candidate) since\nPriorityQueue in Python sorts in ascending order.\n\n            \n            \n                113                    candidates.put(\n114                        (\n115                            -split_candidate.gain,\n116                            n_processed,  # tie-breaker\n117                            node,\n118                            split_candidate,\n119                        )\n120                    )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                121                n_processed += 1\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                122            leaves_to_process = []\n123            if candidates.empty():\n124                break\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Get the global best split candidate\n\n            \n            \n                125            _, _, node, split_info = candidates.get()\n126            self.split_node(node, inputs, features_info, split_info)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Add children to the list for processing in the next iteration\n\n            \n            \n                127            leaves_to_process.extend(node.children.values())\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                128        if len(tree) == 0:\n129            print(\"No splits were made; returning None\")\n130            return None\n131        return tree\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Find the best split for the given node. This is a helper method that can evaluate each\nfeature in parallel and aggregate the results.\n\n            \n            \n                132    def find_best_split(\n133        self, inputs, features_info, gradient, hessian, sample_weight\n134    ) -&gt; SplitCandidate:\n135        tasks = [\n136            (\n137                info,\n138                inputs[:, index],\n139                gradient,\n140                hessian,\n141                sample_weight,\n142            )\n143            for index, info in enumerate(features_info)\n144        ]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Parallelize feature processing if n_workers &gt; 1. Otherwise, process sequentially.\n\n            \n            \n                145        if self.n_workers is not None and self.n_workers &gt; 1:\n146            from concurrent.futures import ThreadPoolExecutor\n147\n148            with ThreadPoolExecutor(max_workers=self.n_workers) as executor:\n149                feature_results = list(\n150                    executor.map(lambda t: self._process_feature(*t), tasks)\n151                )\n152        else:\n153            feature_results = [self._process_feature(*task) for task in tasks]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Return the best result\n\n            \n            \n                154        return max(\n155            (r for r in feature_results if r is not None),\n156            key=lambda r: r.gain,\n157            default=None,\n158        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Find the best split for a given feature. This part is the backbone of the optimization.\n\n            \n            \n                159    def _process_feature(\n160        self,\n161        info: \"FeatureInfo\",\n162        values: np.ndarray,\n163        gradient: np.ndarray,\n164        hessian: np.ndarray,\n165        sample_weight: np.ndarray,\n166    ):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Handle categorical and numerical features differently\n\n            \n            \n                167        if info.type == \"categorical\":\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate group-wise sums of gradients and hessians for each category\n\n            \n            \n                168            cats, cat_indices = np.unique(values, return_inverse=True)\n169            groups = cats\n170            grad_group, hess_group, weight_group = groupby_sum_2d(\n171                n_groups=len(groups),\n172                group_indices=cat_indices,\n173                gradients=gradient,\n174                hessians=hessian,\n175                sample_weight=sample_weight,\n176            )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Sort categories by gain to convert categorical split into a\nordered split problem: categories[:k] vs categories[k:].\n\n            \n            \n                177            fisher_order = self.calculate_gain(grad_group, hess_group)\n178            sort_indices = np.argsort(fisher_order)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Reorder arrays based on the calculated order\n\n            \n            \n                179            groups, grad_group, hess_group, weight_group = (\n180                groups[sort_indices],\n181                grad_group[sort_indices],\n182                hess_group[sort_indices],\n183                weight_group[sort_indices],\n184            )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                185        else:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                For numerical features, bin the values first and then compute group-wise sums\n\n            \n            \n                186            bins = info.bins\n187            bin_indices = np.digitize(values, bins, right=True)\n188            bin_indices[np.isnan(values)] = len(bins)\n189            groups = np.concatenate([bins, [np.nan]])\n190\n191            grad_group, hess_group, weight_group = groupby_sum_2d(\n192                n_groups=len(groups),\n193                group_indices=bin_indices,\n194                gradients=gradient,\n195                hessians=hessian,\n196                sample_weight=sample_weight,\n197            )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Separate out the null group\n\n            \n            \n                198        null_group = np.isnan(groups)\n199        grad_null_sum = grad_group[null_group].sum(axis=0)\n200        hess_null_sum = hess_group[null_group].sum(axis=0)\n201        weight_null_sum = weight_group[null_group].sum(axis=0)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate cumulative sums excluding the null group\n\n            \n            \n                202        groups = groups[~null_group]\n203        grad_group_csum = np.cumsum(grad_group[~null_group], axis=0)\n204        hess_group_csum = np.cumsum(hess_group[~null_group], axis=0)\n205        weight_group_csum = np.cumsum(weight_group[~null_group], axis=0)\n206\n207        if len(groups) &lt;= 1:\n208            return None\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Node total sums\n\n            \n            \n                209        node_grad_sum = grad_group_csum[-1] + grad_null_sum\n210        node_hess_sum = hess_group_csum[-1] + hess_null_sum\n211        node_weight_sum = weight_group_csum[-1] + weight_null_sum\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate split gains for every possible split point and null direction\n\n            \n            \n                212        grad_group_csum = grad_group_csum[:-1]\n213        hess_group_csum = hess_group_csum[:-1]\n214        weight_group_csum = weight_group_csum[:-1]\n215\n216        parent_gain = self.calculate_gain(node_grad_sum, node_hess_sum)\n217        gains_with_nulls_left = (\n218            self.calculate_gain(\n219                grad_group_csum + grad_null_sum,\n220                hess_group_csum + hess_null_sum,\n221            )\n222            + self.calculate_gain(\n223                node_grad_sum - (grad_group_csum + grad_null_sum),\n224                node_hess_sum - (hess_group_csum + hess_null_sum),\n225            )\n226            - parent_gain\n227        )\n228        gains_with_nulls_right = (\n229            self.calculate_gain(\n230                grad_group_csum,\n231                hess_group_csum,\n232            )\n233            + self.calculate_gain(\n234                node_grad_sum - grad_group_csum,\n235                node_hess_sum - hess_group_csum,\n236            )\n237            - parent_gain\n238        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Determine the best split for this feature\n\n            \n            \n                239        split_gain_combined = np.concatenate(\n240            [gains_with_nulls_left, gains_with_nulls_right]\n241        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Enforce minimum weight per leaf constraint\n\n            \n            \n                242        valid_null_left = (\n243            weight_group_csum + weight_null_sum &gt;= self.min_weight_leaf\n244        ) & (\n245            node_weight_sum - (weight_group_csum + weight_null_sum)\n246            &gt;= self.min_weight_leaf\n247        )\n248        valid_null_right = (weight_group_csum &gt;= self.min_weight_leaf) & (\n249            node_weight_sum - weight_group_csum &gt;= self.min_weight_leaf\n250        )\n251        split_gain_combined[~np.concatenate([valid_null_left, valid_null_right])] = (\n252            -np.inf\n253        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Find the overall best split and store the details in a SplitCandidate object\n\n            \n            \n                254        best_gain_idx = np.argmax(split_gain_combined)\n255        feature_gain = split_gain_combined[best_gain_idx]\n256\n257        nulls_to_left = True if best_gain_idx &lt; (len(groups) - 1) else False\n258        split_idx = best_gain_idx % (len(groups) - 1)\n259\n260        left_grad_sum = grad_group_csum[split_idx]\n261        left_hess_sum = hess_group_csum[split_idx]\n262        if nulls_to_left:\n263            left_grad_sum += grad_null_sum\n264            left_hess_sum += hess_null_sum\n265\n266        if info.type == \"categorical\":\n267            split_point = (\n268                groups[: split_idx + 1],\n269                groups[split_idx + 1 :],\n270            )\n271        else:\n272            split_point = groups[split_idx]\n273\n274        return SplitCandidate(\n275            gain=feature_gain,\n276            feature=info.name,\n277            feature_index=info.index,\n278            split_point=split_point,\n279            nulls_to_left=nulls_to_left,\n280            gradient_sum=node_grad_sum,\n281            hessian_sum=node_hess_sum,\n282            left_grad_sum=left_grad_sum,\n283            left_hess_sum=left_hess_sum,\n284        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Split the given node using the provided split information. Creates left and right child nodes and\nupdates the parent node accordingly.\n\n            \n            \n                285    def split_node(\n286        self,\n287        node: Node,\n288        inputs: np.ndarray,\n289        features_info: List[\"FeatureInfo\"],\n290        split_info: SplitCandidate,\n291    ):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Update the split criteria for the parent node\n\n            \n            \n                292        node.update(\n293            feature=split_info.feature,\n294            feature_index=split_info.feature_index,\n295            split_point=split_info.split_point,\n296            nulls_to_left=split_info.nulls_to_left,\n297            gradient_sum=split_info.gradient_sum,\n298            hessian_sum=split_info.hessian_sum,\n299        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Determine the feature index in the subsampled dataset\n\n            \n            \n                300        feature_index = next(\n301            i for i, f in enumerate(features_info) if f.name == split_info.feature\n302        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Find out which samples go to the left and right child nodes. This information is used later\nfor further splits.\n\n            \n            \n                303        mask = node.criterion(\n304            inputs[node.sample_indices, feature_index], input_type=\"feature\"\n305        )\n306        left_child = Node(\n307            parent=node,\n308            depth=node.depth + 1,\n309            l2_regularization=node.l2_regularization,\n310            sample_indices=node.sample_indices[mask],\n311            gradient_sum=split_info.left_grad_sum,\n312            hessian_sum=split_info.left_hess_sum,\n313        )\n314        right_child = Node(\n315            parent=node,\n316            depth=node.depth + 1,\n317            l2_regularization=node.l2_regularization,\n318            sample_indices=node.sample_indices[~mask],\n319            gradient_sum=split_info.right_grad_sum,\n320            hessian_sum=split_info.right_hess_sum,\n321        )\n322        node.children[\"left\"] = left_child\n323        node.children[\"right\"] = right_child\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the per-node gain based on gradients and hessians. The gain is calculated as:\n$$\\text{gain} = \\frac{(T(G))^2}{H + \\lambda}$$\nwhere:\n\n\\(T(G) = \\text{sign}(G) \\cdot \\max(0, |G| - \\alpha)\\) is the soft-thresholding operator\n\\(G\\) is the sum of gradients\n\\(H\\) is the sum of hessians\n\\(\\alpha\\) is the L1 regularization parameter (induces sparsity)\n\\(\\lambda\\) is the L2 regularization parameter (prevents overfitting)\n\nParameters\n\ngradients : np.ndarray\n\nThe sum of gradients for the node(s) with shape (n_nodes, n_outputs).\n\n\nhessians : np.ndarray\n\nThe sum of hessians for the node(s) with shape (n_nodes, n_outputs).\nReturns\n\n\ngain_outputs : np.ndarray\n\nThe calculated gain for each node, shape (n_nodes,).\n\n\n\n\n            \n            \n                324    def calculate_gain(self, gradients, hessians):\n325        if self.l1_regularization &gt; 0:\n326            gradients = np.sign(gradients) * np.maximum(\n327                0, np.abs(gradients) - self.l1_regularization\n328            )\n329        gain_outputs = (gradients**2) / (hessians + self.l2_regularization + 1e-16)\n330        if self.objective_weight is not None:\n331            gain_outputs *= self.objective_weight / np.sum(self.objective_weight)\n332        return gain_outputs.sum(axis=-1)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Subsample features and samples based on the specified fractions. This is useful for\ncreating a variety of trees in the ensemble and preventing overfitting.\n\n            \n            \n                333    def subsample_dataset(\n334        self,\n335        inputs: np.ndarray,\n336        features_info: List[\"FeatureInfo\"],\n337        targets: np.ndarray,\n338        sample_weight: np.ndarray,\n339        current_predictions: np.ndarray,\n340    ) -&gt; Tuple[np.ndarray]:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Take a random subset of features if specified\n\n            \n            \n                341        if self.feature_fraction &lt; 1:\n342            n_features = inputs.shape[1]\n343            n_subsample_features = int(n_features * self.feature_fraction)\n344            feature_indices = self.random.choice(\n345                n_features, size=n_subsample_features, replace=False\n346            )\n347            inputs = inputs[:, feature_indices]\n348            features_info = [features_info[i] for i in feature_indices]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Subsample data points if specified\n\n            \n            \n                349        if self.samples_fraction &lt; 1:\n350            n_samples = inputs.shape[0]\n351            subsample_size = int(n_samples * self.samples_fraction)\n352            subsample_indices = self.random.choice(\n353                n_samples, size=subsample_size, replace=False\n354            )\n355            inputs = inputs[subsample_indices]\n356            targets = targets[subsample_indices]\n357            if sample_weight is not None:\n358                sample_weight = sample_weight[subsample_indices]\n359            current_predictions = current_predictions[subsample_indices]\n360\n361        return (\n362            inputs,\n363            features_info,\n364            targets,\n365            sample_weight,\n366            current_predictions,\n367        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Apply Gradient-based One-Side Sampling. The idea is to retain instances with large gradients\nwhile randomly sampling from instances with small gradients. This helps focus the model\non hard-to-predict instances while still maintaining a representative sample of the overall data.\nInput: \\(g\\) (gradients), \\(h\\) (hessians), \\(X\\) (inputs), \\(y\\) (targets),\n\\(w\\) (sample weights), \\(a\\) (top rate), \\(b\\) (bottom rate), \\(\\text{rescale}\\) (rescale-bottom flag)\n$$\n\\begin{aligned}\n& n = |g|,\\ n_{\\text{top}} = \\lfloor a n \\rfloor,\\ n_{\\text{bot}} = \\lfloor b n \\rfloor \\\\\n& r_i = \\lVert g_i \\rVert_1,\\ \\pi = \\operatorname{argsort}(r;\\ \\text{descending}) \\\\\n& T = \\{\\pi_1,\\dots,\\pi_{n_{\\text{top}}}\\},\\ C = \\{\\pi_{n_{\\text{top}}+1},\\dots,\\pi_n\\} \\\\\n& B \\sim \\text{UniformSubset}(C,\\ n_{\\text{bot}}),\\ S = T \\cup B \\\\\n& \\text{if rescale:} \\\\\n&\\quad s = \\frac{\\sum_{i \\notin T} w_i}{\\sum_{i \\in B} w_i},\\\n\\forall i \\in B:\\ (g_i,h_i,w_i) \\leftarrow s (g_i,h_i,w_i)\n\\end{aligned}\n$$\nOutput: \\( \\{(x_i,y_i,g_i,h_i,w_i)\\}_{i \\in S} \\)\n\n            \n            \n                368    def apply_goss_sampling(self, gradient, hessian, inputs, targets, sample_weight):\n369        n_samples = len(gradient)\n370        n_top = int(n_samples * self.goss_top_rate)\n371        n_bottom = int(n_samples * self.goss_bottom_rate)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Sort the samples by absolute gradient values in descending order\n\n            \n            \n                372        abs_grad = np.abs(gradient).sum(axis=1)\n373        sorted_indices = np.argsort(-abs_grad)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Select top and bottom samples\n\n            \n            \n                374        top_indices = sorted_indices[:n_top]\n375        bottom_indices = self.random.choice(\n376            sorted_indices[n_top:], n_bottom, replace=False\n377        )\n378        selected_indices = np.concatenate([top_indices, bottom_indices])\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                I added an argument to make it optional whether to rescale the bottom instances or not. I found\nthat in some cases, not rescaling gives better results.\n\n            \n            \n                379        scale = 1.0\n380        if self.goss_rescale_bottom:\n381            scale = (\n382                np.sum(sample_weight) - np.sum(sample_weight[top_indices])\n383            ) / np.sum(sample_weight[bottom_indices])\n384\n385        gradient = gradient[selected_indices]\n386        hessian = hessian[selected_indices]\n387        sample_weight = sample_weight[selected_indices]\n388\n389        gradient[n_top:] *= scale\n390        hessian[n_top:] *= scale\n391        sample_weight[n_top:] *= scale\n392\n393        return (\n394            gradient,\n395            hessian,\n396            inputs[selected_indices],\n397            targets[selected_indices],\n398            sample_weight,\n399        )"
  },
  {
    "objectID": "blog/2025-12-14-gbt-algorithms/index.html#loss-functions-and-distributions",
    "href": "blog/2025-12-14-gbt-algorithms/index.html#loss-functions-and-distributions",
    "title": "Implementing Gradient Boosted Tree Algorithms from Scratch - LightGBM, XGBoost, CatBoost",
    "section": "4.5 Loss Functions and Distributions",
    "text": "4.5 Loss Functions and Distributions\nThis part took a while to get right, but I had a lot of inspiration from xgboost-distribution, torch.distributions which helped in the design.\nHere, I take the probabilistic approach by using Maximum Likelihood Estimation (MLE) to introduce the two loss functions (Squared and BCE) that I implemented here, but in reality as long as you can compute the gradients and hessians for a given loss function, you can plug it in. The reason I like the probabilistic approach is that it opens the door to more advanced use-cases such as uncertainty estimation (variance parameter), and also gives an intuitive understanding behind the loss functions we use but rarely question.\n\n\ndistributions.py\n        \n            \n        \n            \n                \n                    #\n                \n                \n            \n            \n                1from abc import ABC, abstractmethod\n2from typing import Tuple\n3\n4import numpy as np\n5\n6from .utils import sigmoid, to_2d\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Base class for loss distributions.\n\n            \n            \n                7class Distribution(ABC):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Number of parameters/output dimensions required by the distribution.\n\n            \n            \n                 8    @property\n 9    @abstractmethod\n10    def n_outputs(self) -&gt; int:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute the log probability of the true values given the predictions.\n\n            \n            \n                11    @abstractmethod\n12    def log_prob(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Return interpretable predictions (e.g., [mu, sigma] for Gaussian, [prob] for Bernoulli).\n\n            \n            \n                13    @abstractmethod\n14    def predict(self, y_pred: np.ndarray) -&gt; np.ndarray:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute the gradient and hessian of the loss function w.r.t. predictions.\n\n            \n            \n                15    @abstractmethod\n16    def gradient_hessian(\n17        self, y_true: np.ndarray, y_pred: np.ndarray, weight: np.ndarray = None\n18    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize the parameters based on the target values.\n\n            \n            \n                19    @abstractmethod\n20    def init_params(self, y: np.ndarray) -&gt; np.ndarray:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Generate samples from the distribution given predictions.\n\n            \n            \n                21    @abstractmethod\n22    def sample(\n23        self, y_pred: np.ndarray, n_samples: int, random_state: np.random.RandomState\n24    ) -&gt; np.ndarray:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Gaussian distribution objective for regression tasks. Uses natural\nparameterization for faster convergence.\nParameters\n\nlearn_variance : bool, default=False\n\nWhether to learn the variance parameter or assume it is fixed.\nIf False: equivalent to Squared loss (homoscedastic regression).\nIf True: models both mean and variance (heteroscedastic regression).\n\n\n\n\n            \n            \n                25class Gaussian(Distribution):\n26    def __init__(self, learn_variance: bool = False):\n27        self.learn_variance = learn_variance\n28        if not learn_variance:\n29            self.eta2_fixed = -0.5\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                30    @property\n31    def n_outputs(self) -&gt; int:\n32        return 2 if self.learn_variance else 1\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Log probability of Gaussian distribution. Meaning, for each data point, we calculate\n\\(\\log P(y_{\\text{true}} \\mid \\theta)\\) where \\(\\theta\\) are the parameters predicted by the model. During learning,\nwe find the optimal \\(\\theta\\) that maximizes this log probability over the training set.\nThis implementation uses the natural parameterization of the Gaussian distribution, where\nthe learnable parameters are:\n$$\\begin{aligned}\n\\eta_1 &= \\frac{\\mu}{\\sigma^2} \\\\\n\\eta_2 &= -\\frac{1}{2\\sigma^2}\n\\end{aligned}$$\nIn practice, we don't learn \\(\\eta_2\\) directly, since it must be negative, but the model output\nis unbounded. Instead, we model \\(z\\) where \\(\\eta_2 = -\\exp(z)\\). This ensures \\(\\eta_2\\) is always negative, since\n\\(\\exp(z) &gt; 0\\) for all real \\(z\\).\n\n            \n            \n                33    def log_prob(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Convert the raw predictions (\\(\\eta_1, z\\)) to (\\(\\mu, \\sigma\\))\n\n            \n            \n                34        mu, sigma = self._format_prediction(y_pred).T\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute log probability using the standard Gaussian formula\n\n            \n            \n                35        log_prob = (\n36            -0.5 * np.log(2 * np.pi)\n37            - np.log(sigma)\n38            - 0.5 * ((y_true - mu) ** 2) / (sigma**2)\n39        )\n40        return log_prob\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Return predictions \\((\\mu, \\sigma)\\).\nOutput Shape: (n_samples, 1 or 2) depending on whether variance is learnable.\n\n            \n            \n                41    def predict(self, y_pred: np.ndarray) -&gt; np.ndarray:\n42        return self._format_prediction(y_pred)[:, : self.n_outputs]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute the gradient and hessian of the negative log likelihood loss\nw.r.t. the natural parameters \\((\\eta_1, \\eta_2)\\).\n\n            \n            \n                43    def gradient_hessian(\n44        self, y_true: np.ndarray, y_pred: np.ndarray, weight: np.ndarray = None\n45    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n46        eta1, eta2 = self._format_prediction(y_pred, natural=True).T\n47\n48        grad = np.zeros_like(y_pred)\n49        hess = np.zeros_like(grad)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Derivatives w.r.t. \\(\\eta_1\\)\n\n            \n            \n                50        grad[:, 0] = -y_true - eta1 / (2 * eta2)\n51        hess[:, 0] = -1 / (2 * eta2)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                If learning variance, then \\(z\\) is also a learnable parameter\n\n            \n            \n                52        if self.learn_variance:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Derivatives w.r.t. \\(\\eta_2\\)\n\n            \n            \n                53            grad_eta2 = -(y_true**2) + (eta1**2 / (4 * eta2**2)) - 1 / (2 * eta2)\n54            hess_eta2 = -(eta1**2) / (2 * eta2**3) + 1 / (2 * eta2**2)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Derivatives w.r.t \\(z\\), which is the raw output of the model\nChain rule: \\(\\frac{dL}{dz} = \\frac{dL}{d\\eta_2} \\cdot \\frac{d\\eta_2}{dz}\\)\nwhere \\(\\frac{d\\eta_2}{dz} = -\\exp(z) = \\eta_2\\)\n\n            \n            \n                55            grad[:, 1] = grad_eta2 * eta2\n56            hess[:, 1] = hess_eta2 * (eta2**2) + grad_eta2 * eta2\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                57        if weight is not None:\n58            weight = to_2d(weight)\n59            grad *= weight\n60            hess *= weight\n61\n62        return grad, hess\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Returns initial \\((\\eta_1, \\eta_2)\\) or \\((\\eta_1)\\) if variance is fixed.\n\n            \n            \n                63    def init_params(self, y):\n64        mu = np.mean(y)\n65        if self.learn_variance:\n66            sigma = np.std(y)\n67            eta1, eta2 = self.to_natural(mu, sigma)\n68            z = np.log(-eta2)  # Inverse of eta2 = -exp(z)\n69            return np.array([eta1, z])\n70        return np.array([mu])\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Mean is \\(\\mu\\) parameter of the Gaussian.\n\n            \n            \n                71    def mean(self, y_pred: np.ndarray) -&gt; np.ndarray:\n72        mu, _ = self._format_prediction(y_pred).T\n73        return mu\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Standard deviation is \\(\\sigma\\) parameter of the Gaussian.\n\n            \n            \n                74    def stdev(self, y_pred: np.ndarray) -&gt; np.ndarray:\n75        _, sigma = self._format_prediction(y_pred).T\n76        return sigma\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Generate samples from the Gaussian distribution given predictions.\n\n            \n            \n                77    def sample(\n78        self, y_pred, n_samples: int, random_state: np.random.RandomState\n79    ) -&gt; np.ndarray:\n80        mu, sigma = self._format_prediction(y_pred).T\n81        return random_state.normal(mu, sigma, size=(mu.shape[0], n_samples))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute the entropy of the Gaussian distribution given predictions.\n\n            \n            \n                82    def entropy(self, y_pred: np.ndarray) -&gt; np.ndarray:\n83        _, sigma = self._format_prediction(y_pred).T\n84        return 0.5 * np.log(2 * np.pi * np.e * sigma**2)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Convert raw predictions to \\((\\mu, \\sigma)\\) or \\((\\eta_1, \\eta_2)\\).\n\n            \n            \n                85    def _format_prediction(\n86        self, y_pred: np.ndarray, natural: bool = False\n87    ) -&gt; np.ndarray:\n88        eta1 = y_pred[:, 0]\n89        if self.learn_variance:\n90            z = y_pred[:, 1]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                model \\(z\\) instead of \\(\\eta_2\\) directly to ensure negativity\n\n            \n            \n                91            eta2 = -np.exp(z)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                92        else:\n93            eta2 = np.full_like(eta1, self.eta2_fixed)\n94\n95        if natural:\n96            return np.column_stack([eta1, eta2])\n97        mu, sigma = self.from_natural(eta1, eta2)\n98        return np.column_stack([mu, sigma])\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Helper function to convert \\((\\mu, \\sigma)\\) to \\((\\eta_1, \\eta_2)\\).\n\n            \n            \n                 99    @staticmethod\n100    def to_natural(mu, sigma):\n101        eta1 = mu / sigma**2\n102        eta2 = -1 / (2 * sigma**2)\n103        return eta1, eta2\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Helper function to convert \\((\\eta_1, \\eta_2)\\) to \\((\\mu, \\sigma)\\).\n\n            \n            \n                104    @staticmethod\n105    def from_natural(eta1, eta2):\n106        mu = -eta1 / (2 * eta2)\n107        sigma = np.sqrt(-1 / (2 * eta2))\n108        return mu, sigma\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Logistic objective for binary classification tasks.\n\n            \n            \n                109class Bernoulli(Distribution):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                110    @property\n111    def n_outputs(self) -&gt; int:\n112        return 1\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Log probability of Bernoulli distribution.\nThe Bernoulli distribution is:\n$$P(y \\mid p) = p^y (1-p)^{1-y}$$\nWhere \\(p\\) is the probability of success, and \\(y \\in \\{0, 1\\}\\). Taking the log, we get:\n$$\\log P(y \\mid p) = y \\log p + (1-y) \\log(1-p)$$\nWhich is the standard binary cross-entropy loss.\nLogit Parameterization:\nWe model \\(z \\in \\mathbb{R}\\) (logit) instead of \\(p\\) directly, using the sigmoid transform:\n$$p = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\nThe rationale behind this is that since the model outputs are unbounded, using sigmoid\nsquashes them into the valid probability range (0, 1).\nBy substituting \\(p = \\sigma(z)\\) into the log probability, we get:\n$$\\log P(y \\mid z) = y \\cdot z - \\log(1 + e^{z})$$\n\n            \n            \n                113    def log_prob(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; np.ndarray:\n114        logit = y_pred[:, 0]\n115        return y_true * logit - np.logaddexp(0, logit)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Returns the probability \\(p\\) of the positive class.\nOutput Shape: (n_samples, 1)\n\n            \n            \n                116    def predict(self, y_pred: np.ndarray) -&gt; np.ndarray:\n117        logit = y_pred[:, 0]\n118        prob = sigmoid(logit)\n119        return prob.reshape(-1, 1)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                The derivatives of the loss function (negative log likelihood) w.r.t. the model output\n(logit parameter \\(z\\)) are:\n\nGradient: \\(\\sigma(\\text{z}) - y\\)\nHessian: \\(\\sigma(\\text{z}) \\cdot (1 - \\sigma(\\text{z}))\\)\n\nwhere \\(\\sigma\\) is the sigmoid function.\n\n            \n            \n                120    def gradient_hessian(\n121        self, y_true: np.ndarray, y_pred: np.ndarray, weight: np.ndarray = None\n122    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n123        logit = y_pred[:, 0]\n124        prob = sigmoid(logit)\n125\n126        grad = np.zeros_like(y_pred)\n127        hess = np.zeros_like(grad)\n128\n129        grad[:, 0] = prob - y_true\n130        hess[:, 0] = prob * (1 - prob)\n131\n132        if weight is not None:\n133            weight = to_2d(weight)\n134            grad *= weight\n135            hess *= weight\n136\n137        return grad, hess\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Returns initial logit based on mean probability (a priori).\nNote: $$p = \\sigma(\\text{z}) = \\frac{1}{1 + e^{-\\text{z}}} \\implies \\text{z} = \\log \\frac{p}{1-p}$$\n\n            \n            \n                138    def init_params(self, y):\n139        mean_prob = np.clip(np.mean(y), 1e-7, 1 - 1e-7)\n140        logit = np.log(mean_prob / (1 - mean_prob))\n141        return np.array([logit])\n142\n143    mean = predict\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Generate samples from the Bernoulli distribution given predictions.\n\n            \n            \n                144    def sample(\n145        self, y_pred, n_samples: int, random_state: np.random.RandomState\n146    ) -&gt; np.ndarray:\n147        logit = y_pred[:, 0]\n148        prob = sigmoid(logit)\n149        return random_state.binomial(1, prob, size=(len(prob), n_samples))"
  },
  {
    "objectID": "blog/2025-12-14-gbt-algorithms/index.html#utility-functions",
    "href": "blog/2025-12-14-gbt-algorithms/index.html#utility-functions",
    "title": "Implementing Gradient Boosted Tree Algorithms from Scratch - LightGBM, XGBoost, CatBoost",
    "section": "4.6 Utility Functions",
    "text": "4.6 Utility Functions\nThis module contains a few utility functions that are used throughout the codebase. Nothing fancy here, except for the Numba aggregation function used to speed up histogram construction.\n\n\nutils.py\n        \n            \n        \n            \n                \n                    #\n                \n                \n            \n            \n                1import math\n2\n3import numba as nb\n4import numpy as np\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute grouped sums of gradients and hessians for 2D arrays. Uses Numba for speedup.\nInput Shape: (n_samples, n_outputs)\nOutput Shape: (n_groups, n_outputs)\n\n            \n            \n                 5@nb.njit\n 6def groupby_sum_2d(\n 7    n_groups: int,\n 8    group_indices: np.ndarray,\n 9    gradients: np.ndarray,\n10    hessians: np.ndarray,\n11    sample_weight: np.ndarray,\n12):\n13    n_outputs = gradients.shape[1]\n14\n15    grad_sums = np.empty((n_groups, n_outputs))\n16    hess_sums = np.empty((n_groups, n_outputs))\n17    weight_sums = np.bincount(group_indices, weights=sample_weight, minlength=n_groups)\n18\n19    for j in range(n_outputs):\n20        grad_sums[:, j] = np.bincount(\n21            group_indices, weights=gradients[:, j], minlength=n_groups\n22        )\n23        hess_sums[:, j] = np.bincount(\n24            group_indices, weights=hessians[:, j], minlength=n_groups\n25        )\n26\n27    return grad_sums, hess_sums, weight_sums\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Map the values in the input array according to the provided mapping dictionary.\n\n            \n            \n                28def map_array(arr: np.ndarray, mapping: dict, missing_value=np.nan) -&gt; np.ndarray:\n29    return np.vectorize(lambda x: mapping.get(x, missing_value))(arr)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Ensure the input array is 2D. If 1D, reshape to (n_samples, 1).\n\n            \n            \n                30def to_2d(arr: np.ndarray) -&gt; np.ndarray:\n31    if arr.ndim == 1:\n32        return arr.reshape(-1, 1)\n33    elif arr.ndim &gt; 2:\n34        raise ValueError(f\"Input array must be 1D or 2D, but got {arr.ndim}D.\")\n35    return arr\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute the sigmoid function for each element in the input array.\n$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n\n            \n            \n                36@nb.njit\n37def sigmoid(x: np.ndarray) -&gt; np.ndarray:\n38    return 1 / (1 + np.exp(-x))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Adaptively truncate a number based on a percentage of its magnitude. I use this\npretty much everywhere to make sure numerical outputs don't look ugly.\n\n            \n            \n                39def trunc(num, threshold_percent=0.01, truncate_integer=False):\n40    if num == 0:\n41        return 0\n42\n43    sign = 1 if num &gt; 0 else -1\n44    num = abs(num)\n45\n46    min_change = num * (threshold_percent / 100)\n47    decimal_places = math.ceil(-math.log10(min_change))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Check if we need to lose an additional decimal place\n\n            \n            \n                48    scale_d = 10 ** (decimal_places - 1)\n49    digit_value_to_lose = ((num * scale_d) % 1) / scale_d\n50    if min_change &gt;= digit_value_to_lose:\n51        decimal_places -= 1\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Don't truncate integer digits unless explicitly allowed\n\n            \n            \n                52    if not truncate_integer:\n53        decimal_places = max(decimal_places, 0)\n54\n55    scale = 10**decimal_places\n56    truncated = int(num * scale) / scale\n57    return sign * truncated"
  },
  {
    "objectID": "blog/2025-12-14-gbt-algorithms/index.html#census-income-dataset",
    "href": "blog/2025-12-14-gbt-algorithms/index.html#census-income-dataset",
    "title": "Implementing Gradient Boosted Tree Algorithms from Scratch - LightGBM, XGBoost, CatBoost",
    "section": "5.1 Census Income Dataset",
    "text": "5.1 Census Income Dataset\nThis is a binary classification task where the goal is to predict whether an individual’s income exceeds $50K/year based on a number of attributes such as age, education, occupation, etc. The dataset contains both numerical and categorical features, making it a good fit to test the capabilities of our implementation.\nLoad the data from Huggingface\n\nimport pandas as pd\nimport numpy as np\n\ndf_census = pd.read_csv(\n    \"https://huggingface.co/datasets/scikit-learn/adult-census-income/raw/main/adult.csv\"\n)\ndf_census.head(20)\n\n\n            \n            \n            \n        \n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation.num\nmarital.status\noccupation\nrelationship\nrace\nsex\ncapital.gain\ncapital.loss\nhours.per.week\nnative.country\nincome\n\n\n\n\n0\n90\n?\n77053\nHS-grad\n9\nWidowed\n?\nNot-in-family\nWhite\nFemale\n0\n4356\n40\nUnited-States\n&lt;=50K\n\n\n1\n82\nPrivate\n132870\nHS-grad\n9\nWidowed\nExec-managerial\nNot-in-family\nWhite\nFemale\n0\n4356\n18\nUnited-States\n&lt;=50K\n\n\n2\n66\n?\n186061\nSome-college\n10\nWidowed\n?\nUnmarried\nBlack\nFemale\n0\n4356\n40\nUnited-States\n&lt;=50K\n\n\n3\n54\nPrivate\n140359\n7th-8th\n4\nDivorced\nMachine-op-inspct\nUnmarried\nWhite\nFemale\n0\n3900\n40\nUnited-States\n&lt;=50K\n\n\n4\n41\nPrivate\n264663\nSome-college\n10\nSeparated\nProf-specialty\nOwn-child\nWhite\nFemale\n0\n3900\n40\nUnited-States\n&lt;=50K\n\n\n5\n34\nPrivate\n216864\nHS-grad\n9\nDivorced\nOther-service\nUnmarried\nWhite\nFemale\n0\n3770\n45\nUnited-States\n&lt;=50K\n\n\n6\n38\nPrivate\n150601\n10th\n6\nSeparated\nAdm-clerical\nUnmarried\nWhite\nMale\n0\n3770\n40\nUnited-States\n&lt;=50K\n\n\n7\n74\nState-gov\n88638\nDoctorate\n16\nNever-married\nProf-specialty\nOther-relative\nWhite\nFemale\n0\n3683\n20\nUnited-States\n&gt;50K\n\n\n8\n68\nFederal-gov\n422013\nHS-grad\n9\nDivorced\nProf-specialty\nNot-in-family\nWhite\nFemale\n0\n3683\n40\nUnited-States\n&lt;=50K\n\n\n9\n41\nPrivate\n70037\nSome-college\n10\nNever-married\nCraft-repair\nUnmarried\nWhite\nMale\n0\n3004\n60\n?\n&gt;50K\n\n\n10\n45\nPrivate\n172274\nDoctorate\n16\nDivorced\nProf-specialty\nUnmarried\nBlack\nFemale\n0\n3004\n35\nUnited-States\n&gt;50K\n\n\n11\n38\nSelf-emp-not-inc\n164526\nProf-school\n15\nNever-married\nProf-specialty\nNot-in-family\nWhite\nMale\n0\n2824\n45\nUnited-States\n&gt;50K\n\n\n12\n52\nPrivate\n129177\nBachelors\n13\nWidowed\nOther-service\nNot-in-family\nWhite\nFemale\n0\n2824\n20\nUnited-States\n&gt;50K\n\n\n13\n32\nPrivate\n136204\nMasters\n14\nSeparated\nExec-managerial\nNot-in-family\nWhite\nMale\n0\n2824\n55\nUnited-States\n&gt;50K\n\n\n14\n51\n?\n172175\nDoctorate\n16\nNever-married\n?\nNot-in-family\nWhite\nMale\n0\n2824\n40\nUnited-States\n&gt;50K\n\n\n15\n46\nPrivate\n45363\nProf-school\n15\nDivorced\nProf-specialty\nNot-in-family\nWhite\nMale\n0\n2824\n40\nUnited-States\n&gt;50K\n\n\n16\n45\nPrivate\n172822\n11th\n7\nDivorced\nTransport-moving\nNot-in-family\nWhite\nMale\n0\n2824\n76\nUnited-States\n&gt;50K\n\n\n17\n57\nPrivate\n317847\nMasters\n14\nDivorced\nExec-managerial\nNot-in-family\nWhite\nMale\n0\n2824\n50\nUnited-States\n&gt;50K\n\n\n18\n22\nPrivate\n119592\nAssoc-acdm\n12\nNever-married\nHandlers-cleaners\nNot-in-family\nBlack\nMale\n0\n2824\n40\n?\n&gt;50K\n\n\n19\n34\nPrivate\n203034\nBachelors\n13\nSeparated\nSales\nNot-in-family\nWhite\nMale\n0\n2824\n50\nUnited-States\n&gt;50K\n\n\n\n\n    \n        \n\n\n\nX = df_census.replace(\"?\", np.nan).drop(columns=[\"income\"])\ny = (df_census[\"income\"] == \"&gt;50K\").astype(int).to_numpy()\n\nSplit the data into training and testing sets with stratification to preserve class distribution.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, test_size=0.2, random_state=42\n)\n\nFit the model with Bernoulli distribution, since this is a binary classification task.\n\nfrom src.gbm import GradientBoostedModel\nfrom src.distributions import Bernoulli\n\ndist_bernoulli = Bernoulli()\ngbm = GradientBoostedModel(\n    learning_rate=0.1,\n    n_trees=50,\n    n_leaves=20,\n    max_depth=10,\n    max_bins=255,\n    max_categories=15,\n    l2_regularization=0.0,\n    min_weight_leaf=10,\n    min_gain_to_split=0.0,\n    goss_top_rate=0.3,\n    goss_bottom_rate=0.1,\n    goss_rescale_bottom=False,\n    path_smoothing_strength=0.25,\n    seed=42,\n    feature_fraction=0.9,\n    samples_fraction=0.8,\n    distribution=dist_bernoulli,\n    verbose=False,\n)\ngbm.fit(X_train, y_train);\n\n\n\n\nThis dataset has a few categorical features. Since max_categories is set to 15, those with more than 15 unique values will be treated as continuous features by encoding them using target statistics.\n\narray = gbm.data_processor_.transform(X_test)\npd.DataFrame(array, columns=X_test.columns)[[\"native.country\", \"education\", \"race\", \"sex\"]].head(10)\n\n\n            \n            \n            \n        \n\n\n\n\nnative.country\neducation\nrace\nsex\n\n\n\n\n0\n0.24591967894420672\n0.4130081709599835\n4.0\n1.0\n\n\n1\n0.24591967894420672\n0.1595181706586484\n4.0\n1.0\n\n\n2\n0.24591967894420672\n0.4130081709599835\n4.0\n1.0\n\n\n3\n0.24591967894420672\n0.7424649388331821\n4.0\n0.0\n\n\n4\n0.24591967894420672\n0.1595181706586484\n4.0\n0.0\n\n\n5\n0.24591967894420672\n0.1595181706586484\n4.0\n1.0\n\n\n6\nnull\n0.06636229734696716\n4.0\n1.0\n\n\n7\n0.24591967894420672\n0.4130081709599835\n2.0\n0.0\n\n\n8\n0.24591967894420672\n0.05338530176077435\n2.0\n1.0\n\n\n9\n0.24591967894420672\n0.1595181706586484\n4.0\n0.0\n\n\n\n\n    \n        \n\n\nWe can see that some features like “race” have been transformed into integer codes while others with more unique values have been target-statistic encoded. The reason we use integer code rather than keeping original string values is that the transformed dataset is supposed to be a homogeneous float64 numpy array. So it’s more about implementation convenience.\nThis was just an example demonstrating the capabilities of the data processor.\nNow let’s evaluate on the test set using F1-score and ROC-AUC.\n\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom src.utils import trunc\n\ntest_probs = gbm.predict(X_test)[:, 0]\nclass_labels = (test_probs &gt;= 0.5).astype(int)\nf1 = f1_score(y_test, class_labels, average=\"binary\")\nroc_auc = roc_auc_score(y_test, test_probs)\nprint(f\"Test F1 Score: {trunc(f1)}\")\nprint(f\"Test ROC AUC Score: {trunc(roc_auc)}\")\n\nTest F1 Score: 0.6942\nTest ROC AUC Score: 0.9194\n\n\nPlot the confusion matrix\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_test, class_labels)\nfig, ax = plt.subplots(figsize=(6, 5), dpi=300)\nConfusionMatrixDisplay(cm, display_labels=[\"&lt;=50K\", \"&gt;50K\"]).plot(ax=ax)\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\n\nPlot the ROC curve. The ROC curve summarizes the trade-off between true positive rate and false positive rate as we vary the decision threshold.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thresholds = roc_curve(y_test, test_probs)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(7, 6), dpi=200)\nplt.plot(fpr, tpr, color=\"blue\", label=f\"ROC curve (area = {roc_auc:.2f})\")\nplt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s now train and evaluate an LightGBM model with similar hyperparameters.\n\n%pip install lightgbm --quiet\n\nimport lightgbm as lgb\n\ncategorical_cols = list(X_train.select_dtypes(include=[\"object\", \"category\"]).columns)\n\nX_train2 = X_train.copy()\nX_test2 = X_test.copy()\nX_train2[categorical_cols] = X_train2[categorical_cols].astype(\"category\")\nfor col in categorical_cols:\n    X_test2[col] = pd.Categorical(X_test2[col], categories=X_train2[col].cat.categories)\n\nlgb_train = lgb.Dataset(X_train2, y_train, categorical_feature=categorical_cols)\nlgb_eval = lgb.Dataset(X_test2, y_test, reference=lgb_train, categorical_feature=categorical_cols)\nlgbm = lgb.LGBMClassifier(\n    objective=\"binary\",\n    learning_rate=0.1,\n    n_estimators=50,\n    num_leaves=20,\n    max_depth=10,\n    min_data_in_leaf=20,\n    reg_lambda=0.0,\n    data_sample_strategy=\"goss\",\n    top_rate=0.3,\n    other_rate=0.1,\n    feature_fraction=0.9,\n    bagging_fraction=0.8,\n    seed=42,\n    verbose=-1,\n    path_smooth=0.25,\n)\nlgbm.fit(X_train2, y_train)\n\n\nlgbm_test_probs = lgbm.predict_proba(X_test2)[:, 1]\nclass_labels = (lgbm_test_probs &gt;= 0.5).astype(int)\nf1 = f1_score(y_test, class_labels, average=\"binary\")\nroc_auc = roc_auc_score(y_test, lgbm_test_probs)\nprint(f\"Test F1 Score: {trunc(f1)}\")\nprint(f\"Test ROC AUC Score: {trunc(roc_auc)}\")\n\nNote: you may need to restart the kernel to use updated packages.\nTest F1 Score: 0.69808\nTest ROC AUC Score: 0.9208\n\n\nWe can see they have similar performance. To make it complete let’s include CatBoost in the comparison as well:\n\n%pip install catboost --quiet\n\nimport catboost as cb\n\ncategorical_cols = list(X_train.select_dtypes(include=[\"object\", \"category\"]).columns)\n\nX_train_cb = X_train.copy()\nX_test_cb = X_test.copy()\nfor col in categorical_cols:\n    X_train_cb[col] = X_train_cb[col].replace({np.nan: None}).astype(str)\n    X_test_cb[col] = X_test_cb[col].replace({np.nan: None}).astype(str)\n\ntrain_pool = cb.Pool(X_train_cb, y_train, cat_features=categorical_cols)\nval_pool = cb.Pool(X_test_cb, y_test, cat_features=categorical_cols)\n\ncatboost_model = cb.CatBoostClassifier(\n    loss_function=\"Logloss\",\n    iterations=50,\n    learning_rate=0.1,\n    depth=10,\n    l2_leaf_reg=0.0,\n    random_seed=42,\n    verbose=0,\n    allow_writing_files=False,\n)\ncatboost_model.fit(train_pool)\n\n\ncb_test_probs = catboost_model.predict_proba(val_pool)[:, 1]\nclass_labels = (cb_test_probs &gt;= 0.5).astype(int)\nf1 = f1_score(y_test, class_labels, average=\"binary\")\nroc_auc = roc_auc_score(y_test, cb_test_probs)\nprint(f\"Test F1 Score: {trunc(f1)}\")\nprint(f\"Test ROC AUC Score: {trunc(roc_auc)}\")\n\nNote: you may need to restart the kernel to use updated packages.\nTest F1 Score: 0.6861\nTest ROC AUC Score: 0.9162\n\n\nThe performance across all three implementations is comparable, demonstrating that our implementation captures the essential components of modern GBT algorithms while remaining simple and extensible."
  },
  {
    "objectID": "blog/2025-12-14-gbt-algorithms/index.html#california-housing-price",
    "href": "blog/2025-12-14-gbt-algorithms/index.html#california-housing-price",
    "title": "Implementing Gradient Boosted Tree Algorithms from Scratch - LightGBM, XGBoost, CatBoost",
    "section": "5.2 California Housing Price",
    "text": "5.2 California Housing Price\nThis dataset contains information about house prices in California, including features like median income, average house age, average number of rooms, etc. The target variable is the median house value for each district. All features are numerical.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\n\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\nX.head(20)\n\n\n            \n            \n            \n        \n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984126984126984\n1.0238095238095237\n322.0\n2.5555555555555554\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137082601054\n0.9718804920913884\n2401.0\n2.109841827768014\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288135593220339\n1.073446327683616\n496.0\n2.8022598870056497\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.8173515981735155\n1.0730593607305936\n558.0\n2.547945205479452\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853281853282\n1.0810810810810811\n565.0\n2.1814671814671813\n37.85\n-122.25\n\n\n5\n4.0368\n52.0\n4.761658031088083\n1.1036269430051813\n413.0\n2.139896373056995\n37.85\n-122.25\n\n\n6\n3.6591\n52.0\n4.9319066147859925\n0.9513618677042801\n1094.0\n2.1284046692607004\n37.84\n-122.25\n\n\n7\n3.12\n52.0\n4.797527047913447\n1.061823802163833\n1157.0\n1.7882534775888717\n37.84\n-122.25\n\n\n8\n2.0804\n42.0\n4.294117647058823\n1.1176470588235294\n1206.0\n2.026890756302521\n37.84\n-122.26\n\n\n9\n3.6912\n52.0\n4.970588235294118\n0.9901960784313726\n1551.0\n2.172268907563025\n37.84\n-122.25\n\n\n10\n3.2031\n52.0\n5.477611940298507\n1.0796019900497513\n910.0\n2.263681592039801\n37.85\n-122.26\n\n\n11\n3.2705\n52.0\n4.772479564032698\n1.0245231607629428\n1504.0\n2.0490463215258856\n37.85\n-122.26\n\n\n12\n3.075\n52.0\n5.322649572649572\n1.0128205128205128\n1098.0\n2.3461538461538463\n37.85\n-122.26\n\n\n13\n2.6736\n52.0\n4.0\n1.0977011494252873\n345.0\n1.9827586206896552\n37.84\n-122.26\n\n\n14\n1.9167\n52.0\n4.262903225806451\n1.0096774193548388\n1212.0\n1.9548387096774194\n37.85\n-122.26\n\n\n15\n2.125\n50.0\n4.242424242424242\n1.071969696969697\n697.0\n2.640151515151515\n37.85\n-122.26\n\n\n16\n2.775\n52.0\n5.9395770392749245\n1.0483383685800605\n793.0\n2.395770392749245\n37.85\n-122.27\n\n\n17\n2.1202\n52.0\n4.052805280528053\n0.966996699669967\n648.0\n2.1386138613861387\n37.85\n-122.27\n\n\n18\n1.9911\n50.0\n5.343675417661098\n1.0859188544152745\n990.0\n2.3627684964200477\n37.84\n-122.26\n\n\n19\n2.6033\n52.0\n5.465454545454546\n1.0836363636363637\n690.0\n2.5090909090909093\n37.84\n-122.27\n\n\n\n\n    \n        \n\n\nSplit the data into training and testing sets, with stratification to ensure the target distribution is (somewhat) preserved in both sets.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    stratify=np.digitize(y, bins=np.histogram_bin_edges(y, bins=10)),\n    test_size=0.2,\n    random_state=42,\n)\n\nFit the model to the training data using Gaussian distribution. By using learn_variance=True, the model will also learn to predict the variance of the target for heteroscedastic uncertainty estimation.\n\nfrom src.gbm import GradientBoostedModel\nfrom src.distributions import Gaussian\n\ndist_normal = Gaussian(learn_variance=True)\ngbm = GradientBoostedModel(\n    learning_rate=0.1,\n    n_trees=50,\n    n_leaves=20,\n    max_depth=10,\n    max_bins=255,\n    l2_regularization=0,\n    min_weight_leaf=20,\n    min_gain_to_split=0.0,\n    goss_top_rate=0.3,\n    goss_bottom_rate=0.1,\n    seed=42,\n    feature_fraction=0.9,\n    samples_fraction=0.8,\n    distribution=dist_normal,\n    verbose=False,\n)\ngbm.fit(X_train, y_train);\n\n\n\n\n\nimport numpy as np\n\n# Shape (n_samples, 2), having (mean, stdev) for each sample\ntest_predictions = gbm.predict(X_test)\nsquared_errors = (test_predictions[:, 0] - y_test.values) ** 2\ntest_rmse = np.sqrt(np.mean(squared_errors))\nprint(f\"Test RMSE: {trunc(test_rmse)}\")\n\ntest_pearson = np.corrcoef(test_predictions[:, 0], y_test.values)[0, 1]\nprint(f\"Test Pearson Correlation: {trunc(test_pearson)}\")\n\nTest RMSE: 0.4914\nTest Pearson Correlation: 0.9087\n\n\nPlot the calibration curve to visualize how well the predictions align with the true values across different ranges. Deviations from the line indicate regions where the model tends to under- or over-predict.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize=(7, 6), dpi=300)\n# Calibration plot\nsns.scatterplot(x=y_test, y=test_predictions[:, 0], alpha=0.9, ax=ax)\nax.set_xlabel(\"True Values\")\nax.set_ylabel(\"Predicted Values\")\n# Set equal limits for x and y axes\nmin_val = min(y_test.min(), test_predictions[:, 0].min())\nmax_val = max(y_test.max(), test_predictions[:, 0].max())\nax.set_xlim(min_val, max_val)\nax.set_ylim(min_val, max_val)\nax.plot([min_val, max_val], [min_val, max_val], color=\"red\", linestyle=\"--\");\n\n\n\n\n\n\n\n\n\n\n\nCode\nmu, sigma = test_predictions.T\ncv = sigma / mu\n\nfig, ax = plt.subplots(figsize=(12, 5), dpi=300)\n# mu-cv plot\nsns.scatterplot(x=mu, y=cv, alpha=0.5, ax=ax)\nax.set_xlabel(\"Predicted Mean (μ)\")\nax.set_ylabel(\"Coefficient of Variation (σ/μ)\");"
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#introduction",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#introduction",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "1 Introduction",
    "text": "1 Introduction\nHello there!\nToday I’ll be going over some of the most important and influential time series forecasting models in the machine learning community. My focus will be to capture the overall intuition behind each model and give a concise overview of their architecture. I’ll be skipping over the basics of time series forecasting, otherwise this post would have been far too long. You can find the definition of some of the terminologies I’ll be using in the Appendix.\nPapers that are covered are:\n\n\nDeepAR: An early deep recurrent model for time series forecasting that worked quite well on a variety of datasets.\n\nN-BEATS: A deep residual model that uses basis functions to decompose the target variable.\n\nTFT: Google’s Transformer-based model.\nDLinear: A linear model that blew the transformers out of the water.\nFITS: A univariate linear model that operates on the frequency domain.\nPatchTST: A transformer-based model that uses patches to model the time series data.\nDAM: A univariate model that uses multi-head attention, fourier basis functions, and leverages irregularly spaced data.\n\nLightGBM: This is not a new model, but I have to mention it because it’s still widely used in practice and has a comparable performance.\n\n\n\n\n\n\n\n\nNoteDisclaimer: Opinions Ahead\n\n\n\n\n\nBefore diving in, I should note that this review is deliberately opinionated. I’ll be sharing my takes on these models based on my personal experience implementing and working with them. I also used the opportunity to dust off some old memes from my meme folder :) If that sounds good to you, let’s get started!"
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#deepar",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#deepar",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "2 DeepAR",
    "text": "2 DeepAR\nAmazon’s DeepAR (Salinas et al., 2020) was one of the early papers designed with having to be trained on large-scale data in mind. The first version of the paper was published in 2017, and this was the era where deep learning was still gaining traction as a solution that could benefit and scale to larger datasets.\n\n\n\n\n\n\nThe architecture uses a vanilla LSTM with the forget cell’s bias set to 1, encouraging the model to retain information from the previous time steps.\nThe model outputs a probability distribution, from which both uncertainty estimates and point forecasts can be derived. In my opinion, this is the highlight of the paper. The appeal behind probabilistic forecasts is that they can be used in downstream applications, such as minimizing risk functions.\nA few ideas regarding the handling of different scales of data are explored.\n\n\nHow to make probabilistic forecasts?\nThe recipe is as follows:\n\nChoose a distribution (e.g. Gaussian, Poisson, etc.) that best fits the target variable.\n\nFor real-valued data, such as stock prices or temperature readings, a Gaussian distribution is used: \\(\\mathcal{N}(\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}\\).\nFor positive natural numbers, such as counts of ad impressions or number of sales, a Negative Binomial distribution \\(\\text{NB}(\\mu, \\alpha) = \\binom{y+\\alpha-1}{y} \\left(\\frac{\\mu}{\\mu+\\alpha}\\right)^y \\left(\\frac{\\alpha}{\\mu+\\alpha}\\right)^\\alpha\\) can be used, where \\(\\mu\\) is the mean and \\(\\alpha\\) is the dispersion parameter. 1 Poisson distribution is also an alternative (Chapados, 2014).\nFor classification tasks, a categorical distribution \\(\\text{Cat}(p) = \\prod_{i=1}^K p_i^{y_i}\\) is used, where \\(y\\) is the one-hot encoded target variable.2\n\nTrain the model to output the parameters of the chosen distribution by maximizing the likelihood of the observed data under the distribution, or in other words, minimize the negative log-likelihood (NLL) of the data. For example, for a Gaussian distribution the model would output the mean \\(\\mu_x\\) and variance \\(\\sigma_x^2\\) for input \\(x\\). For a categorical distribution the model would output the probabilities of each class.\n\nSome parameters may be constrained, like the variance of a Gaussian distribution which must be positive. The paper uses a softplus activation function (see Figure 1) to ensure this:\n\\[\\sigma = \\log(1 + e^{\\sigma_{\\text{raw}}})\\]\nIn classification tasks, softmax is used to ensure the probabilities sum to 1.\n\nSample from the predicted distribution to make a point prediction.\n\n\n\n\n\n\n\n\n\nFigure 1: Comparison of the ReLU and Softplus activation functions.\n\n\n\n\n\n\n\nScale handling\nThe main idea behind DeepAR is that instead of independently modeling each time series, e.g. a product’s sales in a specific store, the model can learn from all time series in the dataset, allowing the model to learn from the similarities between the time series. This is done by adding the product-specific features to the input along with the time series data. Categorical features such as these are embedded using an embedding layer.\nOne challenge is that time series may have different scales. For example, sales of a product in one store might be in the range of 0-10, while sales of another product in another store might be in the range of 1000-10000. To handle this:\n\nInstance normalization is used to normalize the time series, both in the context and the horizon, by dividing by the entire series by the mean of the series in the context window.\nSince the input and output are now scale-free, to give more weight to the series with larger scales, a non-uniform sampling regime is used to assign a higher probability to the series with larger scales.\n\n\n\nThoughts\nThe model is simple and straightforward. Having probabilistic forecasts is a nice touch, but it’s more nuanced than what is presented; Learning a distribution rather than a point estimate can sometimes backfire and lead to worse mean fits:\n\nLearning a multi-parameter distribution includes all the drawbacks of multi-task learning, where the rate of convergence and loss scaling can be different for each parameter. Gradnorm (Chen et al., 2018) proposes a gradient magnitude balancing scheme, but introduces an additional hyperparameter. Kendall et al. (2018) adds the loss scaling to the optimization, both for regression and classification tasks.\n\nIn the case of Gaussian distributions, Seitzer et al. (2022) and Immer et al. (2024) discuss the natural parameterization of the distribution and posterior predictive. I especially recommend Immer et al. (2024) for a more in-depth discussion on the topic.\n\nRegarding the scale handling, an alternative approach could be using a sample-weighted loss function instead of non-uniform sampling. On top of that, assigning higher importance to the series with larger scales isn’t something that is applicable to all datasets.\nCode:  Pytorch Forecasting"
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#n-beats",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#n-beats",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "3 N-BEATS",
    "text": "3 N-BEATS\nN-BEATS (Oreshkin et al., 2019) is a univariate time series forecasting model consisting of a stack of linear layers, with skip connections between them. The model is claimed to be interpretable since the horizon function (considering the target values as a function of time) can be decomposed into a set of basis functions, such as polynomials and sinusoids. N-BEATS has been proven to be competitive in the M3 and M4 forecasting competitions, so the results speak for themselves.\n\nResidual Blocks\n\n\n\n\n\n\nFigure 2: A basic building block in N-BEATS (Oreshkin et al., 2019).\n\n\n\nBuilding on the success of residual networks, which was first popularized through ResNet (He et al., 2016) and now used almost universally in deep learning, N-BEATS uses a stack of MLP blocks with skip connections. Each block (Figure 2) produces two outputs: a forecast and a residual.\n\nThe forecast output of every block, irrespective of the block’s depth, is summed up to produce the final prediction.\nThe residual output is subtracted from the block’s input and passed on to the next block in the stack as input. In the paper, this is referred to as the “backcast” signal, but I’ll get to that in a bit.\n\n\n\nBasis Functions\nBy the Universal Approximation Theorem, a dense feedforward network can approximate any continuous function on a compact subset of R^n to arbitrary precision. However, this theoretical guarantee doesn’t speak to learnability or efficiency (Felbert, 2023). One approach is to constrain the model to learn the parameters of a basis function that best approximates the target variable. The basis function can be a polynomial (like Taylor expansion), a Fourier series, etc. The intuition is to add an inductive bias to the model which can help in generalizations and interpretability. This is not a novel idea and has been used extensively in the past.\nFor example, to model the target variable \\(y \\in \\mathbb{R}^{T}\\) with \\(T\\) being the horizon length, instead of producing \\(T\\) outputs the model can learn the coefficients of a polynomial of degree \\(n\\) that best fits the variable. The inference is then done by evaluating the polynomial at \\(T\\) points:\n\\[\n\\begin{array}{c}\nf(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\cdots + \\theta_n x^n \\\\\\\\\n\\hat{y} = \\left[f(1), f(2), \\ldots, f(T)\\right]\n\\end{array}\n\\tag{1}\\]\nwhere \\(\\hat{y}\\) is the prediction. In practice, we use a normalized time domain between \\([0, 1]\\), so the evaluation becomes: \\[ \\hat{y} = \\left[f(0), f(\\frac{1}{T}), \\ldots, f(\\frac{T-1}{T})\\right] \\]\nFor a fourier basis, the function would be:\n\\[\n\\begin{gathered}\nf(x) = \\sum_{i=1}^{n} a_n\\cos(2 \\pi i x) + b_n \\sin(2 \\pi i x)\n\\end{gathered}\n\\tag{2}\\]\nwhere \\(i\\) is the frequency of the sinusoid, and \\(a_n\\) and \\(b_n\\) are the learnable parameters. The number of frequencies \\(n \\leq T/2\\) is a hyperparameter. If you’re new to Fourier series, this post is a good introduction.\n\n\n\n\n\n\nNoteNyquist-Shannon Sampling Theorem\n\n\n\n\n\nThe \\(T/2\\) upper bound is due to the Nyquist-Shannon sampling theorem, which states that the highest frequency that can be represented in a signal is half the sampling rate. This is because the signal can be perfectly reconstructed from its samples if the sampling rate is at least twice the highest frequency in the signal.\n\n\n\n\n\nA Stack of Stacks\nWhat if we made the network deeper by stacking multiple stacks of building blocks? Well, that’s exactly what the authors imagined.\n\n\n\n\n\nThe same recipe used to connect the blocks within a stack is used to connect the stacks. Each stack produces a forecast and a residual, where the forecast is summed up to produce the final prediction, and the residual, i.e. the residual output of the last block in the stack, is passed on to the next stack as input (Figure 3). Aside from making the network deeper, this also allows to have stacks with different basis functions. Notably, one variation of N-BEATS uses two stacks, the first one learning a polynomial function of a small degree, and the second one learning a fourier decomposition. The idea is that first stack learns the overall trend of the time series, while the second stack learns the seasonality.\n\n\n\n\n\n\nFigure 3: A stack of stacks in N-BEATS (Oreshkin et al., 2019).\n\n\n\nN-HiTS (Challu et al., 2023) is the follow up to N-BEATS, where hierarchical interpolation is added.\n\n\nThoughts\nThe “backcast” signal, aka the residual, threw me off big time. The paper mentions that the goal of having a backcast branch is to predict the input, forcing the model to retain information. On the other hand, when looking at the official implementation, the backcast output is not used in the loss function at all. At first I thought maybe I was missing something and having a backward forecasting is not the objective, but then I noticed even the Pytorch Forecasting library has implemented a backcast loss based on the paper. This is not universally true and some other libraries like Darts follow the official implementation. My main concern is:\n\nIf “backcast” is not meant to be backward forecasting, then why call it backcast? Why not just call it a residual? This is confusing.\nIf the goal is to predict the input, then it leads to another question. Based on Figure 2, the residual output of each block is subtracted from the input and passed on to the next block. Do we expect the network to learn to output zero-valued residuals so that information is retained post-subtraction?\n\nOn a side note, I was surprised that no data normalization is used in the model. At least that’s my understanding from the paper and the official implementation.\nCode:  ServiceNow/N-BEATS (Official)\nCode:  Pytorch Forecasting"
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#tft",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#tft",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "4 TFT",
    "text": "4 TFT\nGoogle’s Temporal Fusion Transformers (TFT) (Lim et al., 2021) is a transformer-based model that uses a combination of attention mechanisms to model the time series data, and it’s been popular in the field since 2019. The paper is well-written and easy to follow, so kudos to the authors for that. But the more you actually understand the architecture and the ablation studies, the more questioning it gets as to why certain design choices were made, and how they were presented in the paper.\n\n\n\n\n\n\nFigure 4: TFT’s architecture (Lim et al., 2021).\n\n\n\nIn a nutshell, TFT is a composition of:\n\nA sequence-to-sequence LSTM encoder-decoder. This component is actually the core of the model based on the ablation studies, not the transformer (Figure 5).\nA multi-head attention (MHA) mechanism takes the output of both the LSTM encoder and decoder as input.\n\nThere’s a shortcut connection that bypasses the MHA. So the model can choose to ignore MHA if it’s not needed.\nIt uses masked attention to maintain the causality of the time series.\n\nThe input features, are transformed, weighted, and then aggregated. The paper uses the term “Variable Selection” which in essence is a self-attention similar to (Qin et al., 2017). You can also go for a recurrent version like (Choi et al., 2016), but it wasn’t considered in the paper.\nGated residual blocks that resemble ResNet blocks are used in several places as computational blocks. They are present in static feature transformation, variable selection, before and after the MHA, etc. The gated block itself consists of a GLU (Shazeer, 2020), which in Noam’s own words, “we offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence” 🙂\nQuantile regression (Pinball loss) is used to predict the lower and upper quantiles of the target variable.\n\n\n\n\n\n\n\nFigure 5: Results of ablation analysis in TFT (Lim et al., 2021), where Local processing refers to the LSTM encoder-decoder. The removal of LSTM has the most significant impact on the model’s performance, with the Electricity dataset being the exception.\n\n\n\nCode:  Google Research (Official)\nCode:  Pytorch Forecasting"
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#sec-dlinear",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#sec-dlinear",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "5 DLinear",
    "text": "5 DLinear\n\n\n\n\n\nThis one is quite intriguing. Zeng et al. (2023) showed a simple linear model can outperform transformer-based models such as Autoformer (Wu et al., 2021) and Informer (Zhou et al., 2021) in long-term forecasting on several benchmarks.\nTwo variants of the model are proposed:\n\nDLinear: It first decomposes the input into trend and seasonal components; A moving average is calculated and dubbed as “trend”, and the “seasonal” component is calculated by subtracting the trend from the input. Two linear layers are applied to each component separately, and the outputs are summed up to get the final prediction. That’s it!\n\nNLinear: First, it subtracts the input by the last value in the input sequence. The input then goes through a linear layer, and the subtracted part is added back before making the final prediction.\n\n\n\n# Pseudo-code for DLinear\nimport torch.nn as nn\n\nclass DLinear(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size):\n        super().__init__()\n        self.trend_layer = nn.Linear(input_dim, output_dim)\n        self.seasonal_layer = nn.Linear(input_dim, output_dim)\n        self.kernel_size = kernel_size\n\n    def forward(self, x):\n        # moving_average = torch.nn.functional.avg_pool1d for example\n        trend = moving_average(x, self.kernel_size) \n        seasonal = x - trend\n        trend_out = self.trend_layer(trend)\n        seasonal_out = self.seasonal_layer(seasonal)\n        return trend_out + seasonal_out"
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#fits",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#fits",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "6 FITS",
    "text": "6 FITS\nFrequency Interpolation Time Series (FITS) (Xu et al., 2023) is the continuation of DLinear (Section 5) but instead the linear model operates on the frequency domain. The model first applies a fourier transform to the input time series, then applies a noise removal followed by a linear projection, and finally applies an inverse fourier transform to get the final prediction. Torch has a built-in fft module that can be used for this purpose. The papers shows that FITS outperforms DLinear on multiple benchmarks, but we’ll get to that in a bit.\n\n\n\n\n\n\nFigure 6: FITS Pipeline. The model predicts both the lookback window and the forecast horizon (backcast and forecast) to provide more supervision. The low-pass filter (LPF) is used to remove high-frequency components above a certain threshold, with the threshold being a hyperparameter (Xu et al., 2023).\n\n\n\nThe authors also claimed that they had used Reversible Instance Normalization (RevIN) (Kim et al., 2021) to stabilize the training process. But when going through the official code, I noticed it uses a simple standardization and de-standardization process, that is widely used in the field, and was first introduced by (Ogasawara et al., 2010). I notified one of the authors about this, but sadly they’re not acknowledging the mistake.\nIf you’ve been following so far, then you’ll love to know that Toner and Darlow (2024) showed that DLinear and FITS can be reduced to simple linear regression of form \\(y = Wx + b\\), and that the closed-form solution (Silva, 2020) has superior performance most of the time compared to the mentioned models! So take everything with a grain of salt, and thread lightly.\nCode:  VEWOXIC/FITS (Official)"
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#patchtst",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#patchtst",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "7 PatchTST",
    "text": "7 PatchTST\nPatchTST (Nie et al., 2022) is basically the idea borrowed from Vision Transformers (ViT) (Dosovitskiy et al., 2020) but applied to time series data instead. It takes the input series, divides it into partially overlapping patches (intervals), applies a light transformation to each patch, feeds them to a transformer backbone, and then passes the output to a prediction head to make forecast. There are two more things worth noting:\n\nIn a multivariate setting, each time series (channel) is treated independently, but they share the same model and are optimized jointly.\nThe authors applied masked pre-training for self-supervision, which is a common practice in language models. During training, random patches are set to zero, and the task would be to predict the missing patches. For this purpose a different prediction head is used, but the transformer backbone and the patch transformation remains the same.\n\n\n\n\n\n\n\nFigure 7: PatchTST’s architecture (Nie et al., 2022).\n\n\n\nPatchTST is shown to have superior performance, consistently beating DLinear on multiple benchmarks.\nThinking out loud here, since PatchTST is mostly a ViT applied to time series data, I wonder if the extensions of ViT such as Swin Transformer (Liu et al., 2021) or T2T-ViT (Yuan et al., 2021) could be applied here as well. Or perhaps someone has already done it and I’m not aware of it :)\nCode:  yuqinie98/PatchTST (Official)"
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#dam",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#dam",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "8 DAM",
    "text": "8 DAM\nDeep data-dependent approximate analytical model (DAM) (Darlow et al., 2024) is a univariate model that leverages fourier series decomposition and multi-head attention, and mixes a few ideas from the previous models we’ve reviewed so far.\n\n\n\n\n\n\nFigure 8: DAM’s architecture (Darlow et al., 2024).\n\n\n\n\nSomewhat similar to N-BEATS and FITS, it operates on the frequency domain and uses fourier basis functions to decompose both the input and the output.\nEquation 2 is used to decompose the input time series using a linear solver. The authors pre-selected 437 frequencies that covers from 1 minute to 10 years, and the solver find the best coefficients that fit the input. The same equation is used as a basis function to generate the output, but the coefficients are learned by the model. Similar to FITS, the model makes predictions on both the lookback window and the forecast horizon (backcast and forecast).\nThe fourier decomposition of the input enables us to have input series sampled at irregular intervals, and DAM leverages this by using a history sampling regime (HSR) (Figure 9) with longer lookback windows. During training, data points are sampled from a long-tailed distribution based on the temporal distance, where recent data points are assigned a higher probability: \\[p(t) = c^{-1} \\frac{1}{1 + \\frac{t}{\\sigma}^2}\\] where \\(t\\) is the normalized time relative to the last time step in the context window, \\(\\sigma\\) is the width hyperparameter, and \\(c\\) is a normalization constant which is the sum of all unnormalized probabilities. Note that \\(t &lt; 0\\) would refer to the context window, and \\(t \\geq 0\\) would refer to the forecast horizon.\nBesides the frequency transformation, DAM also embeds the time and value pairs of the input series, and then uses multi-head cross-attention to jointly learn from this and the frequency domain.\nRevIN (Kim et al., 2021) is used to normalize the input series before any processing is done, but instead of using mean and standard deviation, the authors used the median and the inter-quartile range (IQR) to normalize the series, which is more robust to outliers. Conversely, the output series is de-normalized using the aforementioned statistics.\n\n\n\n\n\n\n\nFigure 9: History Sampling Regime (HSR) in DAM (Darlow et al., 2024). Data points are sampled from both the context window and the forecast horizon, with recent data points (closer to now) having a higher probability.\n\n\n\nThe model is shown to outperform DLinear and PatchTST, but bear in mind that it’s a univariate model.\nYou can find the code in the appendix of the paper."
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#lightgbm",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#lightgbm",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "9 LightGBM",
    "text": "9 LightGBM\nThe uncrowned king, LightGBM (Ke et al., 2017), is the special mention here because even though it’s not brought up as a comparison model in recent research papers, it’s still one of the most used models in the industry and Kaggle-like competitions, and has considerable performance in time-series forecasting, even though it’s mostly well-suited for tabular data.\n\n\n\n\n\nLightGBM and its counterparts (Catboost and Xgboost), are easy to set up and tune, and fast to train. Did I mention that they work very well with categorical data? In the time series forecasting setting, they’re autoregressive models that make one-step predictions. Most of the development time is often spent on feature engineering, feature selection and cross-validation. A few examples of features are:\n\nLag features: The time series values at previous time steps, e.g. \\(x_{t-1}, x_{t-7}, \\ldots\\).\nRolling statistics: Rolling means, standard deviations, mins, and maxes over different windows can capture trends and volatility.\nTime-Based features: Cyclical encoding of time components (hour, day, week, month)\nDifferential features: First and second-order differences, percentage changes, and rates of change help capture dynamics and remove trends."
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#closing-thoughts",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#closing-thoughts",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "10 Closing Thoughts",
    "text": "10 Closing Thoughts\nMy takeaways from this review are that DLinear and LightGBM are strong baselines that are easy to set up and have decent performance. PatchTST also has a simple architecture, and could be a good starting point to extend and improve upon, for example by using the extensions of ViT.\nI’m eager to try probabilistic forecasting and see whether probabilistic approaches, when properly implemented with appropriate uncertainty calibration, can lead to better results. I like the idea of decomposing the input and output into fourier series like in DAM, which allows for irregularly-spaced and long lookback and horizon windows.\n\n\n\n\n\nOne thing I would like to see in future papers is the inclusion of the persistence forecast as a baseline in the comparison section. Depending on the dataset, it can not only be a strong baseline, but also help us better understand the added value by the more complex models.\nThere wasn’t much time to go over the evaluation metrics, but given that most papers only report the MAE and MSE, it would be nice to see scale-free metrics like MASE (Mean Absolute Scaled Error) and sMAPE (Symmetric Mean Absolute Percentage Error) as well."
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#references",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#references",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "11 References",
    "text": "11 References\n\n\n[1] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, and Artur Dubrawski. Nhits: Neural hierarchical interpolation for time series forecasting, In Proceedings of the AAAI conference on artificial intelligence, 6989–6997, 2023.\n\n\n[2] Nicolas Chapados. Effective bayesian modeling of groups of related count time series, In Proceedings of the 31st international conference on machine learning, Bejing, China: PMLR, 1395–1403, 2014. Available: https://proceedings.mlr.press/v32/chapados14.html.\n\n\n[3] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks, In International conference on machine learning, PMLR, 794–803, 2018.\n\n\n[4] Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and Walter Stewart. Retain: An interpretable predictive model for healthcare using reverse time attention mechanism, Advances in neural information processing systems, vol. 29, 2016.\n\n\n[5] Luke Darlow, Qiwen Deng, Ahmed Hassan, Martin Asenov, Rajkarn Singh, Artjom Joosen, Adam Barker, and Amos Storkey. Dam: Towards a foundation model for time series forecasting, arXiv preprint arXiv:2407.17880, 2024.\n\n\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others. An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020.\n\n\n[7] Alexander von Felbert. The universal approximation theorem, 2023. Available: https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/. Accessed: 26 March 2023.\n\n\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, In Proceedings of the IEEE conference on computer vision and pattern recognition, 770–778, 2016.\n\n\n[9] Alexander Immer, Emanuele Palumbo, Alexander Marx, and Julia Vogt. Effective bayesian heteroscedastic regression with deep neural networks, Advances in Neural Information Processing Systems, vol. 36, 2024.\n\n\n[10] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree, Advances in neural information processing systems, vol. 30, 2017.\n\n\n[11] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics, In Proceedings of the IEEE conference on computer vision and pattern recognition, 7482–7491, 2018.\n\n\n[12] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift, In International conference on learning representations, 2021.\n\n\n[13] Bryan Lim, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformers for interpretable multi-horizon time series forecasting, International Journal of Forecasting, vol. 37, no. 4, 1748–1764, 2021.\n\n\n[14] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows, In Proceedings of the IEEE/CVF international conference on computer vision, 10012–10022, 2021.\n\n\n[15] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers, arXiv preprint arXiv:2211.14730, 2022.\n\n\n[16] Eduardo Ogasawara, Leonardo C Martinez, Daniel De Oliveira, Geraldo Zimbrão, Gisele L Pappa, and Marta Mattoso. Adaptive normalization: A novel data normalization approach for non-stationary time series, In The 2010 international joint conference on neural networks (IJCNN), IEEE, 1–8, 2010.\n\n\n[17] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting, arXiv preprint arXiv:1905.10437, 2019.\n\n\n[18] Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison Cottrell. A dual-stage attention-based recurrent neural network for time series prediction, arXiv preprint arXiv:1704.02971, 2017.\n\n\n[19] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. DeepAR: Probabilistic forecasting with autoregressive recurrent networks, International journal of forecasting, vol. 36, no. 3, 1181–1191, 2020.\n\n\n[20] Maximilian Seitzer, Arash Tavakoli, Dimitrije Antic, and Georg Martius. On the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks, arXiv preprint arXiv:2203.09168, 2022.\n\n\n[21] Noam Shazeer. Glu variants improve transformer, arXiv preprint arXiv:2002.05202, 2020.\n\n\n[22] Thalles Santos Silva. Understanding linear regression using the singular value decomposition, https://sthalles.github.io, 2020. Available: https://sthalles.github.io/svd-for-regression/.\n\n\n[23] William Toner and Luke Darlow. An analysis of linear time series forecasting models, In Proceedings of the 41st international conference on machine learning, 48404–48427, 2024.\n\n\n[24] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting, Advances in neural information processing systems, vol. 34, 22419–22430, 2021.\n\n\n[25] Zhijian Xu, Ailing Zeng, and Qiang Xu. FITS: Modeling time series with \\(10 k\\) parameters, arXiv preprint arXiv:2307.03756, 2023.\n\n\n[26] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet, In Proceedings of the IEEE/CVF international conference on computer vision, 558–567, 2021.\n\n\n[27] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting?, In Proceedings of the AAAI conference on artificial intelligence, 11121–11128, 2023.\n\n\n[28] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting, In Proceedings of the AAAI conference on artificial intelligence, 11106–11115, 2021."
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#sec-terminologies",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#sec-terminologies",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "12 Appendix: Glossary of Terms",
    "text": "12 Appendix: Glossary of Terms\nAutoregressive: The prefix “auto” means “self” in Greek. So “autoregressive” literally means “self-regressing” - the model regresses (predicts) a variable using previous values of the same variable.”\nIterative Forecasting: Also known as recursive forecasting or one-step-ahead forecasting. Forecasting that makes predictions one time step at a time. The model uses its own predictions as input for multi-step forecasting.\nDirect Forecasting: Also known as direct multi-step forecasting. The model outputs the entire forecast horizon at once.\nContext Window: Also known as lookback window, history window, or observation window. The fixed interval of past time steps used as input to the forecasting model. The data within the window can expressed as \\(X = \\left[x_{t-l+1}, x_{t-l+2}, ..., x_t\\right]\\) where \\(l\\) is the context length and \\(t\\) is the current time step.\nForecast Horizon: Also known as prediction window, target window, forecast window, or decoder output. The future time steps for which predictions are made by the model. Often denoted as \\(Y = \\left[y_{t+1}, y_{t+2}, ..., y_{t+h}\\right]\\) where \\(h\\) is the horizon length.\nBackcast: Also known as backward forecasting or input reconstruction. The model’s attempt to reconstruct the input sequence often used as an intermediate step or auxiliary task for extra supervision.\nPoint Forecast: Also known as deterministic forecast or single-valued prediction. A single value prediction for each time step in the forecast horizon, representing the model’s best estimate.\nStatic Features/Covariates: Also known as time-invariant features, entity embeddings, or global features. Features that remain constant across all time steps for a given time series (e.g., store ID, product category).\nTemporal Features: Also known as time-varying features, dynamic features, or exogenous variables. Features that change over time and may or may not be known in the forecast horizon (e.g., day of week, holidays, promotions).\nPersistence Forecast: Also known as naive forecast. A simple forecasting method that assumes the future value will be the same as the last observed value: \\(\\hat{y}_{t+h} = y_t\\) for any horizon \\(h\\), where \\(\\hat{y}\\) is the forecast and \\(y\\) is the target variable. This baseline gives a lower bound for what we consider an acceptable model performance.\nUnivariate Forecasting: Also known as single-variable forecasting. Forecasting that considers only the past values of the target variable itself.\nMultivariate Forecasting: Also known as multi-variable forecasting or multi-channel forecasting. Forecasting that incorporates multiple related time series or covariates in a single model."
  },
  {
    "objectID": "blog/2025-02-28-time-series-forecasting/index.html#footnotes",
    "href": "blog/2025-02-28-time-series-forecasting/index.html#footnotes",
    "title": "A Review of ML Time Series Forecasting Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are several variations of the Negative Binomial distribution, and the one mentioned here is but one of them.↩︎\nMinimizing the cross-entropy loss used in classification tasks is essentially equivalent to maximizing the log-likelihood of the model parameters under a categorical distribution assumption. For a categorical distribution with parameters \\(p\\), the NLL is: \\[-\\log P(y|p) = -\\log \\prod_{i=1}^K p_i^{y_i} = -\\sum_{i=1}^K y_i \\log p_i\\]\nWhile the cross-entropy loss is:\n\\[H(y,p) = -\\sum_{i=1}^K y_i \\log p_i\\]↩︎"
  },
  {
    "objectID": "blog/2024-09-29-llama-in-browser/index.html",
    "href": "blog/2024-09-29-llama-in-browser/index.html",
    "title": "Running Llama 3 in the Browser!",
    "section": "",
    "text": "1 Introduction\nRecently I came across a few cool projects that allow running LLMs (large language models) in the browser and utilize GPUs for fast inference! This is exciting for two main reasons: first, it’s great to run LLMs locally without sending your data to any external server. You can use custom prompts, configure inference the way you want, and ensure your privacy. Second, you don’t have to deal with setting up Python environments, installing dependencies, and configuring the GPU. The browser takes care of all that! It’s incredibly convenient. With the release of smaller Llama 3.2 models, like the 1B parameter version, it’s now possible to generate quality text directly in your browser, perhaps even matching ChatGPT 3.5 on some tasks.\nThe chat UI below will allow you to download a number of models hosted on HuggingFace, including Llama and Qwen variants. Once you download the model you can disconnect and continue chatting offline. I’ve tried adding a bunch of customizations to play around, but the UI is meant more as a demonstration to showcase the potential of a web UI.\n\n\n2 Chat UI\nYour browser must support WebGPU for a fast inference. You can find the list of supported browsers here. I personally tested it on Apple M1-M3 and Nvidia GPUs, and it worked quite well. If WebGPU is not supported, the engine will fall back on WebAssembly, which is slower but still functional.\n\n\n\n\n\n\nNote\n\n\n\nBased on your browser’s information, you have the following backend ☞ \nFor more info, visit https://webgpureport.org/.\nIf you see “WebGPU not supported” in the status above but you’re certain your browser supports it, you can leave a comment below and I’ll try to help you troubleshoot the issue.\n\n\n\nHere are some models worth trying:\n\nLlama 3.2: Operates approximately three times faster than comparable models, making it suitable for tasks requiring quick responses.\nDeepSeek R1: Excels in reasoning, mathematics, and coding tasks, comparable to top proprietary models.\nQwen 2.5: Excels in complex tasks, including reasoning and comprehension, with an expanded context window.\n\n\n\n\n  \n    Model Settings\n    \n      Model\n      \n      \n      \n      The first download may take a little bit. Subsequent loads will read from cache.\n      \n        \n        \n      \n    \n    \n      \n      Initialize Model\n    \n    \n        \n        \n    \n\n    \n      System Prompt\n        \n      \n      \n        \n        Configure\n      \n    \n\n    \n      \n        \n          \n            Configure System Prompt\n            Define the AI assistant's base personality and behavior\n          \n          ×\n        \n        \n          \n            \n          \n        \n        \n          \n            \n            Reset to Default\n          \n          \n            \n            Save Changes\n          \n        \n      \n    \n    \n    \n      \n        Temperature\n        \n      \n      \n        \n        0.7\n      \n    \n    \n      \n        Max Tokens\n        \n      \n      \n    \n    \n      \n        Top P\n        \n      \n      \n        \n        0.9\n      \n    \n    \n      \n        Frequency Penalty\n        \n      \n      \n        \n        0\n      \n    \n    \n      \n        Presence Penalty\n        \n      \n      \n      \n        \n        0\n      \n    \n  \n  \n    \n      My Chat\n      \n        \n      \n    \n    \n    \n    \n      \n      \n        \n      \n    \n  \n\n\n\n\n3 How It Works\nThe code above runs using WebLLM, a library that enables running LLMs in browser without requiring a server. It’s built on top of the MLC-LLM (Machine Learning Compiler for LLM) framework, which optimizes models for efficient execution on various hardware platforms. MLC-LLM itself is an application of Apache TVM, a machine learning compiler stack that takes in pre-trained models, compiles and generates deployable modules that can be embedded and run everywhere. MLC-LLM specializes in LLM-specific graph transformations and optimized kernels for common operations.\nOne of the major advantages of WebLLM is that it can utilize WebGPU for accelerated inference, which is a web standard provided by most modern browsers that allows developers low-level access to the GPU for general-purpose computing tasks. Make sure your browser is up-to-date.\nTo initialize the engine and download the model, I use the following code:\n\n// Import WebLLM\nimport * as webllm from 'https://esm.run/@mlc-ai/web-llm';\n\n// Initialize the engine\nengine = new webllm.MLCEngine();\n// Download and initialize the model or load from cache if available\nawait engine.reload('Llama-3.2-1B-Instruct-q4f32_1-MLC', config);\n// List of available models can be found in\nconsole.log(webllm.prebuiltAppConfig.model_list);\n\nWith the engine initialized, I use the following to generate responses:\n\n/**\n * Generate a response using the engine\n * @param {Array} messages - Array of conversation history. Each message is a \n     dictionary with keys `role` and `content`.\n     - `role`: `user` or `assistant` or `system`. The prompt is encoded as the\n        first message with the role `system`.\n     - `content`: the message content\n * @param {Function} onUpdate - Callback function to update the UI with the generated message\n * @param {Function} onFinish - Callback function to handle the final message\n * @param {Function} onError - Callback function to handle errors    \n */\nconst streamingGenerating = async (messages, onUpdate, onFinish, onError) =&gt; {\n    try {\n        let curMessage = \"\";\n        let usage;\n        // The model configuration such as temperature, max_tokens, etc.\n        const config = modelConfig.getConfig();\n        const completion = await engine.chat.completions.create({\n            stream: true,\n            messages,\n            ...config,\n            stream_options: { include_usage: true },\n        });\n        // Stream the completion\n        for await (const chunk of completion) {\n            const curDelta = chunk.choices[0]?.delta.content;\n            if (curDelta) {\n                curMessage += curDelta;\n            }\n            if (chunk.usage) {\n                usage = chunk.usage;\n            }\n            // Update the UI\n            onUpdate(curMessage);\n        }\n        // Get the final message\n        const finalMessage = await engine.getMessage();\n        onFinish(finalMessage, usage);\n    } catch (err) {\n        onError(err);\n    }\n};\n\nYou learn more about the other serve options and how to use them in the documentation.\n\n\n4 Final Thoughts\nAs of writing this I can say WebLLM is the fastest library to run chat LLMs over the web. For other tasks like speech to text, image generation, etc., you can look into using the combination of Transformers.js and ONNX Runtime Web, or only ONNX Runtime Web alone if you’re looking for a more general-purpose solution. If you have any questions or feedback, you can leave a comment below.\n\n\n5 Change Log\n\n2025/01/22: Added prompt configuration settings and updated the UI.\n2024/10/12: Added “How It Works” section.\n2024/10/04: Added Markdown support for the output messages.\n2024/10/02: Added download size estimation for the models from the HF repo.\n2024/10/01: Model configuration settings added to the UI with tooltips.\n2024/09/29: Initial release.\n\n\n\n\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{sarang2024,\n  author = {Sarang, Nima},\n  title = {Running {Llama} 3 in the {Browser!}},\n  date = {2024-10-06},\n  url = {https://www.nimasarang.com/blog/2024-09-29-llama-in-browser/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSarang, Nima. 2024. “Running Llama 3 in the Browser!”\nOctober 6, 2024. https://www.nimasarang.com/blog/2024-09-29-llama-in-browser/."
  },
  {
    "objectID": "blog/2024-08-11-gbt-custom-loss/index.html#introduction",
    "href": "blog/2024-08-11-gbt-custom-loss/index.html#introduction",
    "title": "Custom Loss Functions for LightGBM and CatBoost",
    "section": "1 Introduction",
    "text": "1 Introduction\nAs a data scientist, I’ve often found myself pushing the boundaries of popular gradient boosting frameworks. Recently I’ve been exploring the implementation of custom loss functions in LightGBM and CatBoost, two powerful tools for learning from tabular data. These frameworks offer a wide range of built-in loss functions, but sometimes you need to optimize for a specific metric or tackle a unique problem that requires a custom loss.\nIn this blog post, I’ll walk through the process of creating custom loss functions, using Mean Squared Error (MSE) and Mean Squared Logarithmic Error (MSLE) as practical examples. We’ll start by deriving the gradients and Hessians for these functions, to provide the mathematical foundation for our implementations. Then we’ll move on to the code, showing how to integrate these custom losses into LightGBM and CatBoost. Each of these frameworks has its own API for custom losses, so we’ll cover the specifics for each one separately."
  },
  {
    "objectID": "blog/2024-08-11-gbt-custom-loss/index.html#lightgbm",
    "href": "blog/2024-08-11-gbt-custom-loss/index.html#lightgbm",
    "title": "Custom Loss Functions for LightGBM and CatBoost",
    "section": "2 LightGBM",
    "text": "2 LightGBM\n\n2.1 Interface\nThe loss function and the evaluation metric must have the following structure:\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                Objective Function\nCalculate the gradient and hessian of a custom loss function for LightGBM.\nParameters:\n\ntarget (np.ndarray) : The true target values\nprediction (np.ndarray) : The predicted values from the model\nweight (np.ndarray), optional : Sample weights. If None, uniform weights are assumed.\n\nReturns:\n\ngrad (np.ndarray) : First order gradient of the loss with respect to the predictions.\nhess (np.ndarray) : Second order gradient (Hessian) of the loss with respect to the predictions.\n\n\n            \n            \n                1def lgbm_objective_function(\n2    target: np.ndarray,\n3    prediction: np.ndarray,\n4    weight: np.ndarray = None,\n5):\n6    ...\n7\n8    return grad, hess\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Evaluation Metric\nCalculate a custom evaluation metric for LightGBM.\nParameters:\nSame as loss_function.\nReturns:\nA tuple containing three elements:\n\neval_name (str): The name of the metric.\neval_result (float): The value of the metric.\nis_higher_better (bool): Whether a higher value of the metric is better.\n\n\n            \n            \n                 9def lgbm_evaluation_metric(\n10    target: np.ndarray,\n11    prediction: np.ndarray,\n12    weight: np.ndarray = None,\n13):\n14    ...\n15\n16    return eval_name, eval_result, is_higher_better\n\n            \n        \n    \n        \n    \n\n\n\n\n2.2 Mean Squared Error\nMean Squared Error (MSE) is a commonly used loss function for regression and forecasting problems. It is defined as the average of the squared differences between the predicted and actual values: \\[\n\\text{MSE}(y, p) \\;=\\; {\\frac{1}{|\\mathcal{D}|} \\sum_{i=1}^{|\\mathcal{D}|} (y_i - p_i)^2}\n\\tag{1}\\]\nwhere \\(y_i\\) and \\(p_i\\) are the target and predicted values for the \\(i\\)-th sample respectively, and \\(|\\mathcal{D}|\\) is the number of samples in the dataset.\nTo implement MSE as a loss function, we need to derive the gradient and hessian of the loss value with respect to the predicted values.\nThe gradient for sample \\(i\\), ignoring the constant factor of \\(\\frac{1}{|\\mathcal{D}|}\\), is: \\[\n\\frac{\\partial}{\\partial p_i} \\text{MSE}(y, p) \\;=\\; \\frac{\\partial}{\\partial p_i} (y_i - p_i)^2 \\;=\\; -2(y_i - p_i) \\;=\\; 2 (p_i - y_i) \\;\\propto\\; p_i - y_i\n\\tag{2}\\]\nThe reason we can ignore the constant factors is that they will not affect the optimum solution and will be absorbed by the learning rate.\nThe hessian for sample \\(i\\) is: \\[\n\\frac{\\partial^2}{\\partial p_i^2} \\text{MSE}(y, p) \\;\\propto\\; \\frac{\\partial}{\\partial p_i} p_i - y_i \\;=\\; 1\n\\tag{3}\\]\n\n\n\n\n\n\nTipProportionality\n\n\n\n\n\nThe proportionality sign (\\(\\propto\\)) is used to indicate that the expression on the right-hand side is proportional to the expression on the left-hand side, up to a constant factor. Read more about it here.\n\n\n\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                MSE Objective Function\n\n            \n            \n                1import numpy as np\n2\n3\n4def lgbm_mse_objective_function(\n5    target: np.ndarray,\n6    prediction: np.ndarray,\n7    weight: np.ndarray = None,\n8):\n9    gradient = prediction - target\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Hessian is always 1 for MSE\n\n            \n            \n                10    hessian = np.ones_like(gradient)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Apply sample weights\n\n            \n            \n                11    if weight is not None:\n12        gradient *= weight\n13        hessian *= weight\n14    return gradient, hessian\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                MSE Metric\n\n            \n            \n                15def lgbm_mse_metric(\n16    target: np.ndarray,\n17    prediction: np.ndarray,\n18    weight: np.ndarray = None,\n19):\n20    squared_error = (prediction - target) ** 2\n21    mse = np.average(squared_error, weights=weight)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                (metric name, value, is_higher_better)\n\n            \n            \n                22    return \"MSE\", mse, False\n\n            \n        \n    \n        \n    \n\n\n\n\n2.3 Mean Squared Logarithmic Error\nMean Squared Logarithmic Error (MSLE) is defined as the average of the squared differences between the logarithm of the predicted and actual values. It’s a scale-invariant metric that is commonly used in regression problems where the target values have a wide range of values. It’s a good metric for modeling ratios or percentages.\n\\[\n\\text{MSLE}(y, p) \\;=\\; \\frac{1}{|\\mathcal{D}|} \\sum_{i=1}^{|\\mathcal{D}|} (\\log_e (1 + y_i) - \\log_e (1 + p_i) )^2\n\\tag{4}\\]\nwhere \\(y_i\\) and \\(p_i\\) are the target and predicted values for the \\(i\\)-th sample respectively, and \\(|\\mathcal{D}|\\) is the number of samples in the dataset. The +1 in the logarithm means the target values must be greater than -1, otherwise the logarithm will be undefined. If your target values are in a different range, you can either add a constant to the target values or use a different constant in the logarithm.\nThe gradient for sample \\(i\\) is:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial p_i} \\text{MSLE}(y, p)\n&= \\frac{\\partial}{\\partial p_i} \\Big(\\log_e (1 + y_i) - \\log_e (1 + p_i) \\Big)^2 \\\\[3ex]\n&= \\frac{-2}{1 + p_i} \\Big(\\log_e (1 + y_i) - \\log_e (1 + p_i)\\Big) \\\\[3ex]\n&\\propto \\frac{\\log_e (1 + p_i) - \\log_e (1 + y_i)}{1 + p_i} \\end{aligned}\n\\tag{5}\\]\nThe hessian is calculated by taking another derivative of the gradient:\n\\[\n\\begin{aligned}\n    \\frac{\\partial^2}{\\partial p_i^2} \\text{MSLE}(y, p)\n     & = \\frac{\\partial}{\\partial p_i} \\frac{\\log_e (1 + p_i) - \\log_e (1 + y_i)}{1 + p_i}  \\\\[3ex]\n     & = \\frac{\\splitfrac{(1 + p_i)\\frac{\\partial \\Big(\\log_e (1 + p_i) - \\log_e (1 + y_i)\\Big)}{\\partial p_i} }{ - \\Big(\\log_e (1 + p_i) - \\log_e (1 + y_i)\\Big) \\frac{\\partial (1 + p_i)}{\\partial p_i}}}{(1 + p_i)^2} \\\\[3ex]\n     & = \\frac{(1 + p_i)\\times \\frac{1}{1 + p_i}  - \\Big(\\log_e (1 + p_i) - \\log_e (1 + y_i)\\Big) \\times 1}{(1 + p_i)^2}   \\\\[3ex]\n     & = \\frac{1 - \\log_e (1 + p_i) + \\log_e (1 + y_i)}{(1 + p_i)^2}   \\\\[3ex]\n\\end{aligned}\n\\tag{6}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nSince there is \\(\\log_e{(1 + p_i)}\\) in the formulae, we need to ensure that \\(1 + p_i &gt; 0\\) for all \\(i\\). This can be achieved by clipping the predicted values to a minimum value greater than \\(-1\\).\n\n\n\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                MSLE Objective Function\n\n            \n            \n                1import numpy as np\n2\n3\n4def lgbm_msle_objective(\n5    target: np.ndarray,\n6    prediction: np.ndarray,\n7    weight: np.ndarray = None,\n8):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Ensure predictions are at least -1 + 1e-6 to avoid log(0)\n\n            \n            \n                9    prediction = np.maximum(prediction, -1 + 1e-6)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Gradient\n\n            \n            \n                10    gradient = (\n11        np.log1p(prediction) - np.log1p(target)\n12    ) / (prediction + 1)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Hessian\n\n            \n            \n                13    hessian = (\n14        -np.log1p(prediction) + np.log1p(target) + 1\n15    ) / ((prediction + 1) ** 2)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Apply sample weights\n\n            \n            \n                16    if weight is not None:\n17        gradient *= weight\n18        hessian *= weight\n19\n20    return gradient, hessian\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                MSLE Metric\n\n            \n            \n                21def lgbm_msle_metric(\n22    target: np.ndarray,\n23    prediction: np.ndarray,\n24    weight: np.ndarray = None,\n25):\n26    preds = np.maximum(prediction, -1 + 1e-6)\n27    squared_log_error = (\n28        np.log1p(preds) - np.log1p(target)\n29    ) ** 2\n30    msle = np.average(squared_log_error, weights=weight)\n31    return \"MSLE\", msle, False\n\n            \n        \n    \n        \n    \n\n\n\n\n2.4 Usage\nNow we can use the loss functions in LightGBM by setting the objective parameter in the model, and the evaluation metric by setting the eval_metric parameter in the fit method.\nimport lightgbm as lgb\n\nregressor = lgb.LGBMRegressor(objective=lgbm_msle_objective)\nregressor.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric=lgbm_msle_metric\n)"
  },
  {
    "objectID": "blog/2024-08-11-gbt-custom-loss/index.html#catboost",
    "href": "blog/2024-08-11-gbt-custom-loss/index.html#catboost",
    "title": "Custom Loss Functions for LightGBM and CatBoost",
    "section": "3 CatBoost",
    "text": "3 CatBoost\n\n3.1 Interface\nThe CatBoost interface has a few differences from LightGBM:\n\nThe objective function and the evaluation metric are implemented as class rather than a function, and must implement a few specific methods.\n\nCatBoost expects the negative gradient and hessian to be returned by the loss function.\n\nIt’s not necessary to use NumPy arrays for doing vectorized operations. Using for loops will suffice, since under the hood CatBoost will convert our function into machine code using Numba.\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nCatBoost requires the negative gradient and hessian to be returned by the loss function, so we need to apply a negation at the end of the calculations. Intuitively, this is because to minimize the loss function, we need to move in the opposite direction of the gradient, but I personally would’ve preferred CatBoost handled this internally similar to LightGBM and XGBoost.\n\n\n\nThe overall interface is as follows:\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                Objective Function\nCustom loss function for CatBoost.\n\n            \n            \n                1class CatBoostObjectiveFunction:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the derivatives (gradient and hessian) of the loss with respect to the predictions.\nParameters:\n\napproxes (list) : The predicted values from the model\ntargets (list) : The true target values\nweights (list), optional : Sample weights. If None, uniform weights are assumed\n\nReturns:\n\nList[Tuple[float, float]] : List of tuples, each containing the negative gradient and hessian for a sample\n\n\n            \n            \n                 2    def calc_ders_range(\n 3        self,\n 4        approxes: list,\n 5        targets: list,\n 6        weights: list = None,\n 7    ):\n 8        result = []\n 9        for index in range(len(targets)):\n10            grad_i = ...\n11            hess_i = ...\n12\n13            result.append((-grad_i, -hess_i))\n14        return result\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Evaluation Metric\nCustom evaluation metric for CatBoost.\n\n            \n            \n                15class CatBoostEvalMetric:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Parameters:\n\napproxes (List[List]) : List containing the predicted values from the model\ntarget (List) : The true target values\nweight (List), optional : Sample weights. If None, uniform weights are assumed.\n\nReturns:\nA tuple containing two elements:\n\nfloat : The accumulated weighted error\nfloat : The total weight\n\n\n            \n            \n                16    def evaluate(\n17        self,\n18        approxes: list[list],\n19        target: list,\n20        weight: list = None,\n21    ) -&gt; tuple[float, float]:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                I'm not sure why approxes is a list of lists in the first place, but\nwe'll just have to roll with it\n\n            \n            \n                22        assert len(approxes) == 1\n23        approx = approxes[0]\n24        assert len(target) == len(approx)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the weighted error and total weight\n\n            \n            \n                25        error_sum = 0.0\n26        weight_sum = 0.0\n27\n28        for i in range(len(approx)):\n29            error_i = ...\n30            weight_i = (\n31                1.0 if weight is None else weight[i]\n32            )\n33            weight_sum += weight_i\n34            error_sum += weight_i * error_i\n35\n36        return error_sum, weight_sum\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the final metric value from the accumulated error and total weight\nParameters:\n\nerror (float) : The accumulated weighted error\nweight (float) : The total weight\n\nReturns:\n\nfloat : The final metric value\n\n\n            \n            \n                37    def get_final_error(\n38        self, error: float, weight: float\n39    ) -&gt; float:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Indicate whether a higher metric value is better.\nReturns:\n\nbool : True if higher metric values are better, False otherwise\n\n\n            \n            \n                40    def is_max_optimal(self) -&gt; bool:\n\n            \n        \n    \n        \n    \n\n\nIf you’re interested in checking out the implementation of CatBoost’s official loss functions, you can find the C++ code here. It took some digging to find! 😄\n\n\n3.2 Mean Squared Error\nWe previously derived the gradient (Equation 2) and hessian (Equation 3) of the MSE loss function for LightGBM. We can use the same equations for CatBoost, but we need to implement them as a class.\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                MSE Objective Function\nMean Squared Error objective function for CatBoost.\n\n            \n            \n                1class MSEObjective:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                 2    def calc_ders_range(\n 3        self,\n 4        approx: list,\n 5        target: list,\n 6        weights: list = None,\n 7    ):\n 8        result = []\n 9        for i in range(len(target)):\n10            grad_i = approx[i] - target[i]\n11            hess_i = 1.0\n12\n13            if weights is not None:\n14                grad_i *= weights[i]\n15                hess_i *= weights[i]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Add the negation before appending to the result\n\n            \n            \n                16            result.append((-grad_i, -hess_i))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                17        return result\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                MSE Evaluation Metric\nMean Squared Error metric for CatBoost.\n\n            \n            \n                18class MSEMetric:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                19    def evaluate(\n20        self,\n21        approxes: list[list],\n22        target: list,\n23        weight: list = None,\n24    ) -&gt; tuple[float, float]:\n25        assert len(approxes) == 1\n26        approx = approxes[0]\n27        assert len(target) == len(approx)\n28\n29        error_sum = 0.0\n30        weight_sum = 0.0\n31\n32        for i in range(len(approx)):\n33            error_i = (approx[i] - target[i]) ** 2\n34            weight_i = (\n35                1.0 if weight is None else weight[i]\n36            )\n37            weight_sum += weight_i\n38            error_sum += weight_i * error_i\n39\n40        return error_sum, weight_sum\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the final metric value from the accumulated error and total weight\n\n            \n            \n                41    def get_final_error(\n42        self, error: float, weight: float\n43    ) -&gt; float:\n44        return error / weight\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Indicate whether a higher metric value is better. MSE is an error metric, so lower is better.\n\n            \n            \n                45    def is_max_optimal(self) -&gt; bool:\n46        return False\n\n            \n        \n    \n        \n    \n\n\n\n\n3.3 Mean Squared Logarithmic Error\nBased on the derivations from the LightGBM section (Equation 5 and Equation 6), we can implement the MSLE objective function for CatBoost:\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                MSLE Objective Function\nMean Squared Logarithmic Error objective function for CatBoost.\n\n            \n            \n                1import numpy as np\n2\n3\n4class MSLEObjective:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                 5    def calc_ders_range(\n 6        self,\n 7        approx: list,\n 8        target: list,\n 9        weights: list = None,\n10    ):\n11        result = []\n12        for i in range(len(target)):\n13            approx_i = max(approx[i], -1 + 1e-6)\n14            grad_i = (\n15                np.log1p(approx_i) - np.log1p(target[i])\n16            ) / (approx_i + 1)\n17            hess_i = (\n18                -np.log1p(approx_i)\n19                + np.log1p(target[i])\n20                + 1\n21            ) / ((approx_i + 1) ** 2)\n22\n23            if weights is not None:\n24                grad_i *= weights[i]\n25                hess_i *= weights[i]\n26\n27            result.append((-grad_i, -hess_i))\n28        return result\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                MSLE Evaluation Metric\nMean Squared Logarithmic Error metric for CatBoost.\n\n            \n            \n                29class MSLEMetric:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                30    def evaluate(\n31        self,\n32        approxes: list[list],\n33        target: list,\n34        weight: list = None,\n35    ) -&gt; tuple[float, float]:\n36        assert len(approxes) == 1\n37        approx = approxes[0]\n38        assert len(target) == len(approx)\n39\n40        error_sum = 0.0\n41        weight_sum = 0.0\n42\n43        for i in range(len(approx)):\n44            approx_i = max(approx[i], -1 + 1e-6)\n45            error_i = (\n46                np.log1p(approx_i) - np.log1p(target[i])\n47            ) ** 2\n48            weight_i = (\n49                1.0 if weight is None else weight[i]\n50            )\n51\n52            weight_sum += weight_i\n53            error_sum += weight_i * error_i\n54\n55        return error_sum, weight_sum\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the final metric value from the accumulated error and total weight\n\n            \n            \n                56    def get_final_error(\n57        self, error: float, weight: float\n58    ) -&gt; float:\n59        return error / weight\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Indicate whether a higher metric value is better. MSLE is an error metric, so lower is better.\n\n            \n            \n                60    def is_max_optimal(self) -&gt; bool:\n61        return False\n\n            \n        \n    \n        \n    \n\n\n\n\n3.4 Usage\nTo use the custom loss functions in CatBoost, we need to pass it as a parameter during model initialization.\nimport catboost as cb\n\nregressor = cb.CatBoostRegressor(\n    loss_function=MSLEObjective(),\n    eval_metric=MSLEMetric()\n)\nNote that since the functions are implemented as classes, we need to instantiate them before passing them to the model. I fell victim to this mistake a few times, so be careful!"
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#introduction",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#introduction",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "1 Introduction",
    "text": "1 Introduction\nWhen I started writing this post, my goal was to refresh my knowledge of LSTMs by implementing one from scratch. I was initially tempted to use PyTorch or Karpathy’s micrograd, but since I also wanted to implement the backpropagation part myself without relying on an autograd engine, I decided to go with NumPy. This choice meant that the optimizer and training loop would also have to be implemented in NumPy, turning the project into a comprehensive deep dive. So here we are 😅\nOn the bright side, it’s been a great learning experience. I’ve refreshed my understanding of computational graphs, gradient accumulation in recurrent models, and the inner workings of the Adam optimizer. In this post, I’ll walk you through the implementation which resembles a PyTorch-like API. The areas covered are:\n\nMulti-layer LSTM Model\n\nAdamW Optimizer\n\nDataset and Dataloader\n\nTraining on the Shakespeare dataset\n\n I’ll be using a similar presentation style to labml.ai since it’s much easier to follow the code when the explanation is right beside it. Be sure to check out their website for some cool implementations if you haven’t already.\nI hope you’ll find this post helpful. 😎\nP.S. You can toggle between light and dark mode through the button at the top right corner."
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#multi-layer-lstm",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#multi-layer-lstm",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "2 Multi-Layer LSTM",
    "text": "2 Multi-Layer LSTM\nLong Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture specifically designed to handle long-term dependencies in sequential data. It incorporates a memory state, a hidden state, and three gating mechanisms: the input gate, forget gate, and output gate. These gates control the flow of information into, out of, and within the memory and hidden states, allowing the LSTM to selectively remember or forget information at each time step.\nThe memory state in an LSTM acts as a long-term storage unit, allowing the network to retain information over long sequences. The input gate determines how much new information should be stored in the memory state, while the forget gate controls the amount of old information to be discarded. The output gate regulates the flow of information from the memory state and hidden state to the next time step.\n\n\n\nLSTM Architecture Olah (2015)\n\n\nThe LSTM cell consists of the following components: \\[\n\\begin{aligned}\nf_t &= \\sigma(W_{if}x_t + b_{if} \\;+\\; W_{hf}h_{t-1} + b_{hf}) \\\\\ni_t &= \\sigma(W_{ii}x_t + b_{ii} \\;+\\; W_{hi}h_{t-1} + b_{hi}) \\\\\no_t &= \\sigma(W_{io}x_t + b_{io} \\;+\\; W_{ho}h_{t-1} + b_{ho}) \\\\\n\\tilde{c}_t &= \\tanh(W_{ic}x_t + b_{ic} \\;+\\; W_{hc}h_{t-1} + b_{hc}) \\\\\nc_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\\\\nh_t &= o_t \\odot \\tanh(c_t)\n\\end{aligned}\n\\]\nwhere \\(f_t\\), \\(i_t\\), and \\(o_t\\) are the forget, input, and output gates, respectively. \\(\\tilde{c}_t\\) is the candidate memory state, \\(c_t\\) is the memory state, and \\(h_t\\) is the hidden state at time step \\(t\\). \\(x_t\\) is the input at time step \\(t\\), \\(h_{t-1}\\) is the hidden state at time step \\(t-1\\), and \\(W\\) and \\(b\\) are the weights and biases of each gate.\n\nCIFG LSTM\nIn this post, we’ll implement a special of type of LSTM called Coupled Input and Forget Gate (CIFG) Greff et al. (2017). In CIFG LSTM, the input gate is computed as: \\[i_t = 1 - f_t\\] This reduces the number of parameters in the model and has been shown to perform well in practice.\n\n\nMulti-layers\nA multi-layer LSTM is simply stacking multiple LSTM cells on top of each other. The output of the previous LSTM cell is fed as input to the next LSTM cell. The hidden state of the last LSTM cell is the input to the classification layer.\n\n\n\nMulti-layer LSTM Example CS231n (2023). Each row of the green rectangles represent an LSTM cell.\n\n\nNow let’s get into the implementation, step by step.\n\n\nlstm.py\n        \n            \n        \n            \n                \n                    #\n                \n                Import the dependencies.  \nThe activation functions are defined in a separate module\n\n            \n            \n                1from collections import defaultdict\n2from copy import deepcopy\n3\n4import numpy as np\n5from op import sigmoid, softmax, tanh\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                LSTM Classifier\nMulti-layer LSTM classifier for sequence classification tasks.\nIt consists of an embedding layer, multiple LSTM cells, and a classification head.\nThe model is used to process input sequences and generate output logits.\n\n            \n            \n                6class LSTMClassifier:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \nembed_size: Dimension of the word embeddings, or more generally, the input features.\nhidden_size: The size of the hidden state of the LSTM cells.\nvocab_size: The number of unique tokens in the vocabulary.\nn_cells: Number of stacked LSTM cells in the model.\n\n\n            \n            \n                 7    def __init__(\n 8        self,\n 9        embed_size: int,\n10        hidden_size: int,\n11        vocab_size: int,\n12        n_cells: int = 1,\n13    ) -&gt; None:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Define internal variables\n\n            \n            \n                14        self.embed_size = embed_size\n15        self.hidden_size = hidden_size\n16        self.vocab_size = vocab_size\n17        self.n_cells = n_cells\n18        self.layers = dict()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Create embedding layer to convert word indices to embeddings\n\n            \n            \n                19        self.layers[\"embedding\"] = np.empty((vocab_size, embed_size))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Create LSTM layers\n\n            \n            \n                20        for cell_index in range(n_cells):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                For every forget, output, and cell gates, create a linear layer\n\n            \n            \n                21            for layer_name in [\"f\", \"o\", \"c\"]:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                The input size of the first layer is embed_size + hidden_size, since the input is the concatenation of the input features and the previous hidden state. For subsequent layers, the input size is 2 x hidden_size.\n\n            \n            \n                22                linp_sz = hidden_size + (\n23                    embed_size if cell_index == 0 else hidden_size\n24                )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Weights and bias for the linear layer\n\n            \n            \n                25                self.layers[f\"W{layer_name}_{cell_index}\"] = np.empty(\n26                    (linp_sz, hidden_size)\n27                )\n28                self.layers[f\"b{layer_name}_{cell_index}\"] = np.empty(\n29                    (hidden_size)\n30                )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Classification head (projection layer) to generate logits\n\n            \n            \n                31        self.layers[\"W_head\"] = np.empty((hidden_size, vocab_size))\n32        self.layers[\"b_head\"] = np.empty((vocab_size))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Create the gradient arrays. These will be used to store the gradients during backpropagation.\n\n            \n            \n                33        self.grad = {k: np.empty_like(v) for k, v in self.layers.items()}\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize the weights\n\n            \n            \n                34        self.init_weights()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the total number of parameters in the model.\nThe size property of a numpy array returns the number of elements in the array.\n\n            \n            \n                35    @property\n36    def num_parameters(self):\n37        return sum(l.size for l in self.layers.values())\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Glorot/Xavier initialization\nThe weights are initialized from a uniform distribution in the range \\([-d, d]\\), where\n\\(d = \\sqrt{\\frac{6.0}{(r + c)}}\\), and \\(r\\) and \\(c\\) are the number of rows and columns\nin the weight matrix. This makes the variance of the weights inversely proportional to the\nnumber the units, and helps in preventing the gradients from vanishing or exploding during\ntraining. The biases are initialized to zero.\n\n            \n            \n                38    def init_weights(self):\n39        for name, layer in self.layers.items():\n40            if layer.ndim == 1:\n41                self.layers[name] = np.zeros((layer.shape[0]))\n42            elif layer.ndim == 2:\n43                r, c = layer.shape\n44                d = np.sqrt(6.0 / (r + c))\n45                self.layers[name] = np.random.uniform(-d, d, (r, c))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize the hidden and cell states for the LSTM layers.\n\n            \n            \n                46    def init_state(self, batch_size):\n47        state = dict()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                For every LSTM cell and every sample in the batch, initialize the hidden and cell states to zeros.\n\n            \n            \n                48        state[\"h\"] = np.zeros((self.n_cells, batch_size, self.hidden_size))\n49        state[\"c\"] = np.zeros((self.n_cells, batch_size, self.hidden_size))\n50        return state\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Forward pass through the LSTM model.\n\ninputs: Input sequences of shape (batch_size, seq_len, features)\nstate: Hidden and cell states of the LSTM layers. If None, initialize the states to zeros.\nteacher_forcing: If True, use inputs as the input at each timestep.\nIf False, inputs is used as the prefix.\ngeneration_length: Length of the generated sequence when teacher_forcing is False.\n\n\n            \n            \n                51    def forward(\n52        self, inputs, state=None, teacher_forcing=True, generation_length=0\n53    ):\n54        batch_sz, seq_len = inputs.shape[:2]\n55\n56        if teacher_forcing is True:\n57            assert generation_length == 0\n58\n59        n_timestamps = seq_len + generation_length\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Dictionary to store the activations at each timestep. This'll be used during backpropagation.\n\n            \n            \n                60        activations = defaultdict(lambda: defaultdict(list))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Output probabilities of every token in the vocabulary at each timestep\n\n            \n            \n                61        outputs = np.zeros((batch_sz, n_timestamps, self.vocab_size))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize the hidden and cell states\n\n            \n            \n                62        if state is None:\n63            state = self.init_state(batch_sz)\n64        else:\n65            state = state.copy()  # make a shallow copy\n66        for k in [\"h\", \"c\"]:\n67            activations[k][-1] = state[k]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Process the input sequences\n\n            \n            \n                68        for timestep in range(n_timestamps):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                If teacher forcing is False and the prefix is consumed, use the previous prediction as the input\nfor the next timestep\n\n            \n            \n                69            if teacher_forcing is False and timestep &gt;= 1:\n70                word_indices = np.argmax(outputs[:, timestep - 1], axis=1)\n71            else:\n72                word_indices = inputs[:, timestep]\n73            features = self.layers[\"embedding\"][word_indices]\n74            activations[\"input\"][timestep] = word_indices\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Forward pass through the LSTM cells\n\n            \n            \n                75            for cell_idx in range(self.n_cells):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Previous cell states\n\n            \n            \n                76                h_prev = state[\"h\"][cell_idx]\n77                c_prev = state[\"c\"][cell_idx]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                We can simplify the gate operation by concatenating the input features with the hidden state,\nand perform a single linear projection instead of two.\n\n            \n            \n                78                X = np.concatenate((features, h_prev), axis=-1)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Apply the gates, which are linear operations followed by activation functions\n\\(\n\\begin{aligned}\nf_t &= \\sigma(W_{if}{input}_t + b_{if} \\;+\\; W_{hf}h_{t-1} + b_{hf}) \\\\[1ex]\ni_t &= 1 - f_t \\qquad\\qquad \\text{Coupled forget and input gates} \\\\[1ex]\no_t &= \\sigma(W_{io}{input}_t + b_{io} \\;+\\; W_{ho}h_{t-1} + b_{ho}) \\\\[1ex]\n\\tilde{c}_t &= \\tanh(W_{ic}{input}_t + b_{ic} \\;+\\; W_{hc}h_{t-1} + b_{hc}) \\\\[1ex]\n\\end{aligned}\n\\)\n\n            \n            \n                79                f = sigmoid(\n80                    X @ self.layers[f\"Wf_{cell_idx}\"]\n81                    + self.layers[f\"bf_{cell_idx}\"]\n82                )\n83                i = 1 - f\n84                o = sigmoid(\n85                    X @ self.layers[f\"Wo_{cell_idx}\"]\n86                    + self.layers[f\"bo_{cell_idx}\"]\n87                )\n88                c_bar = tanh(\n89                    X @ self.layers[f\"Wc_{cell_idx}\"]\n90                    + self.layers[f\"bc_{cell_idx}\"]\n91                )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                New memory cell and hidden state\n\\(\n\\begin{aligned}\nc_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\\\[1ex]\nh_t &= o_t \\odot \\tanh(c_t)\n\\end{aligned}\n\\)\n\n            \n            \n                92                c = f * c_prev + i * c_bar\n93                h = o * tanh(c)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Classification head\n\n            \n            \n                94                if cell_idx == self.n_cells - 1:\n95                    logits = h @ self.layers[\"W_head\"] + self.layers[\"b_head\"]\n96                    probs = softmax(logits, axis=1)\n97                    outputs[:, timestep] = probs\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Update the state for the next timestep\n\n            \n            \n                 98                state[\"c\"][cell_idx] = c\n 99                state[\"h\"][cell_idx] = h\n100                features = h\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Save the activations for backpropagation\n\n            \n            \n                101                for k, v in zip(\n102                    [\"x\", \"f\", \"o\", \"c_bar\", \"c\", \"h\"], [X, f, o, c_bar, c, h]\n103                ):\n104                    activations[k][timestep].append(v)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                105        return outputs, state, activations\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Alias for the forward method, similar to PyTorch's nn.Module.\nThis enables model(inputs) \\(\\equiv\\) model.forward(inputs)\n\n            \n            \n                106    __call__ = forward\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Backward pass to compute the gradients.\n\ngrad: Gradient of the loss with respect to the output of the model, i.e. logits (pre-softmax scores)\nactivations: Activations from the forward pass.\n\n\n            \n            \n                107    def backward(self, grad, activations):\n108        batch_sz, seq_len = grad.shape[:2]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize the gradients of the next timestep to zeros. This will be updated as we move backward in time.\n\n            \n            \n                109        grad_next = {\n110            k: np.zeros((self.n_cells, batch_sz, self.hidden_size))\n111            for k in [\"h\", \"c\"]\n112        }\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Helper function to compute the gradients of the linear layer.\nThe gradients are computed with respect to the input, weights, and biases respectively.\n\nX: Input to the linear layer\nW: Weights of the linear layer\ndY: Gradient of the loss with respect to the output of the linear layer\n\n\n            \n            \n                113        def _lin_grad(X, W, dY):\n114            return (dY @ W.T, X.T @ dY, dY)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Backpropagation through time. Let's denote \\(t\\) as the current timestep\n\n            \n            \n                115        for timestep in reversed(range(seq_len)):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Classification head\n\n            \n            \n                116            dout_t = grad[:, timestep]\n117            h = activations[\"h\"][timestep][-1]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                The gradients for the classification head:\n\\(\\text{logits}_t = h_t \\mathbf{W}_{\\text{head}} + \\mathbf{b}_{\\text{head}}\\)\n\n            \n            \n                118            dh, dW_head, db_head = _lin_grad(\n119                X=h, W=self.layers[\"W_head\"], dY=dout_t\n120            )\n121            self.grad[f\"W_head\"] += dW_head\n122            self.grad[f\"b_head\"] += np.sum(db_head, axis=0)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Iterate over the LSTM cells in reverse order\n\n            \n            \n                123            for cell_idx in reversed(range(self.n_cells)):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Get the activations for the current timestep\n\n            \n            \n                124                x, f, o, c_bar, c = (\n125                    activations[key][timestep][cell_idx]\n126                    for key in [\"x\", \"f\", \"o\", \"c_bar\", \"c\"]\n127                )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Cell state from the previous timestep\n\n            \n            \n                128                c_prev = activations[\"c\"][timestep - 1][cell_idx]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Receive the gradients flowing from the next timestep.\nThe gradient of the hidden state \\(h_t\\) is the sum of the gradients from the next cell and the next timestep.\n\n            \n            \n                129                dh += grad_next[\"h\"][cell_idx]\n130                dc = grad_next[\"c\"][cell_idx]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \\(h_t = o * tanh(c_t)\\)\n\n            \n            \n                131                do = dh * tanh(c)\n132                dc += dh * o * tanh(c, grad=True)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \\(c_t = f_t \\odot c_{t-1} + (1 - f_t) \\odot \\tilde{c}_t\\)\n\n            \n            \n                133                df = dc * (c_prev - c_bar)\n134                dc_prev = dc * f\n135                dc_bar = dc * (1 - f)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Pre-activation gradients\n\n            \n            \n                136                dc_bar *= tanh(c_bar, grad=True)\n137                do *= sigmoid(o, grad=True)\n138                df *= sigmoid(f, grad=True)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                f, o, c gates, and the inputs X and \\(h_{t-1}\\)\nSince all the gates are linear operations, the calculation will be similar\n\n            \n            \n                139                dinp, dh_prev = 0, 0\n140                for gate, doutput in zip([\"f\", \"o\", \"c\"], [df, do, dc_bar]):\n141                    dX, dW, db = _lin_grad(\n142                        X=x, W=self.layers[f\"W{gate}_{cell_idx}\"], dY=doutput\n143                    )\n144                    self.grad[f\"W{gate}_{cell_idx}\"] += dW\n145                    self.grad[f\"b{gate}_{cell_idx}\"] += np.sum(db, axis=0)\n146                    dinp_gate, dh_prev_gate = (\n147                        dX[:, : -self.hidden_size],\n148                        dX[:, -self.hidden_size :],\n149                    )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Accumulate the gradients for the input and the hidden state,\nsince they are shared between the gates\n\n            \n            \n                150                    dinp += dinp_gate\n151                    dh_prev += dh_prev_gate\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Flow the gradients to the previous cell and previous timestep\n\n            \n            \n                152                dh = dinp\n153                grad_next[\"c\"][cell_idx] = dc_prev\n154                grad_next[\"h\"][cell_idx] = dh_prev\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Embedding layer\n\n            \n            \n                155            word_indices = activations[\"input\"][timestep]\n156            self.grad[\"embedding\"][word_indices] += dinp\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Helper method to serialize the model state, similar to PyTorch's state_dict.\nThe state dictionary contains the model configuration, weights, and gradients.\nIt can be used to save and load the model.\n\n            \n            \n                157    @property\n158    def state_dict(self):\n159        return dict(\n160            config=dict(\n161                embed_size=self.embed_size,\n162                hidden_size=self.hidden_size,\n163                vocab_size=self.vocab_size,\n164                n_cells=self.n_cells,\n165            ),\n166            weights=deepcopy(self.layers),\n167            grad=deepcopy(self.grad),\n168        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                169    @classmethod\n170    def from_state_dict(cls, state_dict):\n171        obj = cls(**state_dict[\"config\"])\n172        for src, tgt in zip(\n173            [state_dict[\"weights\"], state_dict[\"grad\"]],\n174            [obj.layers, obj.grad],\n175        ):\n176            for k, v in src.items():\n177                tgt[k][:] = v\n178        return obj"
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#activation-and-loss-functions",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#activation-and-loss-functions",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "3 Activation and Loss Functions",
    "text": "3 Activation and Loss Functions\nThe activation functions used in LSTM are the sigmoid, tanh (hyperbolic tangent), and softmax functions.\n\nSigmoid is used to compute the gates, which are values between 0 and 1 that control the flow of information.\n\ntanh function is used to compute the candidate memory state.\n\nSoftmax is used to compute the output probabilities.\n\nThe loss function used is the cross-entropy loss, which is suitable for classification tasks. Next token prediction is indeed a classification task where the model predicts the probability distribution over the vocabulary for the next token in the sequence.\n\n\nop.py\n        \n            \n        \n            \n                \n                    #\n                \n                \n            \n            \n                1import numpy as np\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Sigmoid function\nThe sigmoid squashes the input to the range [0, 1].\n\nIf the flag grad is False, returns the sigmoid of x: $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\nOtherwise, \\(x = \\sigma(z)\\) and the derivate \\(\\frac{\\partial \\sigma(z)}{\\partial z}\\) is returned:\n$$\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z) * (1 - \\sigma(z))= x(1-x)$$\n\n\n            \n            \n                2def sigmoid(x, grad=False):\n3    if not grad:\n4        return 1 / (1 + np.exp(-x))\n5    return x * (1 - x)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Hyperbolic tangent function\nThe tanh function squashes the input to the range [-1, 1]. It's defined as:\n$$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n\n            \n            \n                6def tanh(x, grad=False):\n7    if not grad:\n8        return np.tanh(x)\n9    return 1 - x**2\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Softmax function\nApplies the softmax function to the input array along the specified axis. Softmax converts\na vector of real numbers into a probability distribution. The logits are first exponentiated\nto make them positive and increase their separation. It's defined as:\n$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n\n            \n            \n                10def softmax(x, axis):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Subtracting the maximum value for numerical stability. Softmax is invariant to to a constant shift\n\n            \n            \n                11    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n12    return exps / np.sum(exps, axis=axis, keepdims=True)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Cross-entropy loss function\nComputes the cross-entropy loss between the predicted and target distributions. The cross-entropy loss is defined as:\n$$H(y, p) = -\\sum_{i} y_i \\log(p_i)$$\n\nprediction: The predicted array of probabilities of shape (batch_size, num_classes).\ntarget: The target array of shape (batch_size,) containing the class indices.\n\n\n            \n            \n                13def cross_entropy(prediction, target, reduction=\"mean\"):\n14    eps = np.finfo(prediction.dtype).eps\n15    prediction = np.clip(prediction, eps, 1 - eps)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Take the negative log of the predicted probability of the target class\n\n            \n            \n                16    loss = -np.take_along_axis(\n17        np.log(prediction), target[..., np.newaxis], axis=-1\n18    )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Aggregate the loss\n\n            \n            \n                19    if reduction == \"mean\":\n20        loss = loss.mean()\n21    elif reduction == \"sum\":\n22        loss = loss.sum()\n23    return loss"
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#adamw",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#adamw",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "4 AdamW",
    "text": "4 AdamW\nAdamW is a variant of the Adam optimizer that decouples weight penalty from the optimization steps, where the weight penalty is applied directly to the gradients. Adam optimizer uses both the first and second moments of the gradients to adapt the learning rate tailored to each parameter. The benefit of Adam/AdamW is that it requires little tuning of hyperparameters compared to RMSprop and SGD. We’ll go over each step of the optimization in the implementation.\n\n\noptim.py\n        \n            \n        \n            \n                \n                    #\n                \n                Import NumPy\n\n            \n            \n                1import numpy as np\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                AdamW Optimizer\nParameters:\n\nparams (dict): Dictionary referencing the model parameters\ngrads (dict): Dictionary referencing the gradients of the model parameters\nlr (float): Learning rate\nbetas (Tuple[float, float]): Coefficients used for computing running averages of gradient and its square\neps (float): Term added to the denominator to improve numerical stability\nweight_decay (float): Weight decay (L2 penalty) coefficient\namsgrad (bool): Whether to use the AMSGrad variant of the algorithm\n\n\n            \n            \n                 2class AdamW:\n 3    def __init__(\n 4        self,\n 5        params: dict,\n 6        grads: dict,\n 7        lr=0.001,\n 8        betas: tuple[float, float] = (0.9, 0.999),\n 9        eps: float = 1e-8,\n10        weight_decay: float = 1e-2,\n11        amsgrad: bool = False,\n12    ):\n13        self.params = params\n14        self.grads = grads\n15        self.lr = lr\n16        self.betas = betas\n17        self.eps = eps\n18        self.weight_decay = weight_decay\n19        self.amsgrad = amsgrad\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Counter for the number of iterations\n\n            \n            \n                20        self.n_iters = 0\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize first moment vector (mean of gradients) for each parameter\n\n            \n            \n                21        self.m = {k: np.zeros_like(v) for k, v in params.items()}\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize second moment vector (uncentered variance of gradients) for each parameter\n\n            \n            \n                22        self.v = {k: np.zeros_like(v) for k, v in params.items()}\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize maximum of second moment vector for AMSGrad if needed\n\n            \n            \n                23        self.v_m = (\n24            {k: np.zeros_like(v) for k, v in params.items()}\n25            if amsgrad\n26            else None\n27        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Resets all gradients to zero. This is typically used before computing new\ngradients in the training loop.\n\n            \n            \n                28    def zero_grad(self):\n29        for v in self.grads.values():\n30            v[:] = 0\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Perform a single optimization step.\nUpdates the parameters of the model using the AdamW update rule, which\nincludes bias correction, optional AMSGrad, and weight decay.\n\n            \n            \n                31    def step(self):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Increment the iteration counter\n\n            \n            \n                32        self.n_iters += 1\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Unpack the beta values\n\n            \n            \n                33        beta1, beta2 = self.betas\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Iterate over the parameters and their gradients\n\n            \n            \n                34        for (name, param), grad in zip(\n35            self.params.items(), self.grads.values()\n36        ):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Update the first moment estimate:\n$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\nwhere \\(\\beta_1\\) is the exponential decay rate for the first moment estimates,\nand \\(g_t\\) is the gradient at time step \\(t\\).  \n\\(m_{t}\\) is simply an exponential moving average (EMA) of the past gradients.\n\n            \n            \n                37            m_t = self.m[name] = beta1 * self.m[name] + (1 - beta1) * grad\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Update the second moment estimate:\n$$v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$$\nwhere \\(\\beta_2\\) is the exponential decay rate for the second moment estimates.\n\n            \n            \n                38            v_t = self.v[name] = beta2 * self.v[name] + (1 - beta2) * (\n39                grad**2\n40            )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute bias-corrected first moment estimate:\n$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\nWithout correction, the bias causes the algorithm to move very slowly at the beginning of training,\nas the moment estimates are underestimated. In the early iterations, \\(t\\) is small, so \\(\\beta_1^t\\) is close to 1,\nmaking \\(1 - \\beta_1^t\\) a small number. Dividing by this small number effectively increases the estimate.\n\n            \n            \n                41            m_t_hat = m_t / (1 - beta1**self.n_iters)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute bias-corrected second moment estimate:\n$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n\n            \n            \n                42            v_t_hat = v_t / (1 - beta2**self.n_iters)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                AMSGrad update:\n$$\\hat{v}_t = \\max(\\hat{v}_t, v_{t-1})$$\nwhere \\(v_{t-1}\\) is the previous second moment estimate.\nThis ensures \\(v_t\\) is always non-decreasing, preventing the learning rate from growing too large.\n\n            \n            \n                43            if self.amsgrad:\n44                v_t_hat = self.v_m[name] = np.maximum(self.v_m[name], v_t_hat)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Adjusted gradient:\n$$\\hat{g} = \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\nwhere \\(\\epsilon\\) is a small constant to avoid division by zero.  \n\\(\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t}}\\) can be thought of as the signal-to-noise ratio of the gradient.\nI'll leave the intuition behind this to another blog post.\n\n            \n            \n                45            g_hat = m_t_hat / (np.sqrt(v_t_hat) + self.eps)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Add weight penalty to the update:\n$$\\text{update} = \\hat{g} + \\lambda \\cdot p$$\nwhere \\(\\lambda\\) is the weight_decay coefficient.\nThis is equivalent to adding the L2 penalty to the loss function, which penalizes large weights.\n\n            \n            \n                46            update = g_hat + self.weight_decay * param\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Update the parameters in the direction of the negative gradient, scaled by the learning rate:\n$$ p_t = p_{t-1} - \\eta \\cdot \\text{update}$$\nwhere \\(p_{t-1}\\) is the previous parameter value.\n\n            \n            \n                47            self.params[name] -= self.lr * update"
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#data-utilities",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#data-utilities",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "5 Data Utilities",
    "text": "5 Data Utilities\nIn this section we’ll implement the Dataset and Dataloader classes to handle the Shakespeare dataset. We follow the best practices of PyTorch’s Dataset and DataLoader classes to make the implementation more modular and reusable.\n\nThe Dataset class implements the __getitem__ method, which returns a single sample from the dataset.\nThe DataLoader class will be used to sample mini-batches from the dataset, by calling the __getitem__ method of the Dataset.\n\n\n\ndata.py\n        \n            \n        \n            \n                \n                    #\n                \n                Import NumPy\n\n            \n            \n                1import numpy as np\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Dataset\nA dataset for next character prediction tasks.\nFor a sequence of characters \\([c_1, c_2, ..., c_n]\\) and a given sequence length \\(l\\),\nthis dataset creates input/target pairs of the form:\n\nInput \\(x_i\\):  \\([c_i, c_{i+1}, ..., c_{i+l-1}]\\)\nTarget \\(y_i\\): \\([c_{i+1}, c_{i+2}, ..., c_{i+l}]\\)\n\nwhere \\(i\\) ranges from 1 to \\(n-l\\).\nEach item in the dataset is a tuple \\((x_i, y_i)\\) where both \\(x_i\\) and \\(y_i\\) have length \\(l\\).\nThe task is to predict each character in \\(y_i\\) given the corresponding prefix in \\(x_i\\).\nFor example, given \\(x_i = [c_i, c_{i+1}, c_{i+2}]\\), the model would aim to predict:\n\n\\(c_{i+1}\\) given \\([c_i]\\)\n\\(c_{i+2}\\) given \\([c_i, c_{i+1}]\\)\n\\(c_{i+3}\\) given \\([c_i, c_{i+1}, c_{i+2}]\\)\n\n\n            \n            \n                2class NextCharDataset:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                3    def __init__(self, data, seq_length):\n4        self.data = data.copy()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Create a sliding window view of the data\n\n            \n            \n                5        self.window_view = np.lib.stride_tricks.sliding_window_view(\n6            self.data, window_shape=seq_length + 1\n7        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                8    def __len__(self):\n9        return len(self.window_view)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \\(\\text{Input}_i\\):  \\([c_i, c_{i+1}, ..., c_{i+l-1}]\\)\n\\(\\text{Target}_i\\): \\([c_{i+1}, c_{i+2}, ..., c_{i+l}]\\)\n\n            \n            \n                10    def __getitem__(self, idx):\n11        x, y = self.window_view[idx, :-1], self.window_view[idx, 1:]\n12        return x, y\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                DataLoader\nA simple DataLoader to iterate over a dataset in batches.\n\n            \n            \n                13class DataLoader:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                14    def __init__(self, dataset, batch_size, shuffle=False, drop_last=False):\n15        self.dataset = dataset\n16        self.batch_size = batch_size\n17        self.shuffle = shuffle\n18        self.drop_last = drop_last\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                The __iter__ method returns an iterator that yields batches of data. It's mainly\nused in a for loop to iterate over the dataset. e.g.:\nfor inputs, targets in dataloader:\n    ...\n\n\n\n            \n            \n                19    def __iter__(self):\n20        indices = np.arange(len(self.dataset))\n21\n22        if self.shuffle:\n23            np.random.shuffle(indices)\n24\n25        if self.drop_last:\n26            remainder = len(self.dataset) % self.batch_size\n27            if remainder:\n28                indices = indices[:-remainder]\n29\n30        for i in range(0, len(indices), self.batch_size):\n31            batch_indices = indices[i : i + self.batch_size]\n32            batch = [self.dataset[j] for j in batch_indices]\n33            yield self.collate_fn(batch)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                34    def __len__(self):\n35        if self.drop_last:\n36            return len(self.dataset) // self.batch_size\n37        else:\n38            return np.ceil(len(self.dataset) / self.batch_size).astype(int)\n39\n40    def collate_fn(self, batch):\n41        if isinstance(batch[0], (tuple, list)):\n42            return [np.array(samples) for samples in zip(*batch)]\n43        elif isinstance(batch[0], dict):\n44            return {\n45                key: np.array([d[key] for d in batch]) for key in batch[0]\n46            }\n47        else:\n48            return np.array(batch)"
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#training-on-shakespeare-dataset",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#training-on-shakespeare-dataset",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "6 Training on Shakespeare dataset",
    "text": "6 Training on Shakespeare dataset\nNow it’s time to put everything together and train the model on the a dataset. We’ll use the Shakespeare dataset, which consists of a collection of Shakespeare’s plays. The model will be trained to predict the next character in the sequence given a sequence of characters.\nAn important distinction to make between the text generation at training time and inference time is that at training time, we feed the ground truth characters to the model to predict the next character; This is called teacher forcing. At inference time, we feed the model’s prediction at time step \\(t\\) as the input at time step \\(t+1\\) to predict the next character.\n\n6.1 Load\n\nimport numpy as np\nimport matplotlib.pylab as plt\n\nDownload the Shakespeare dataset which is a single text file from the following link: Shakespeare dataset\n\nwith open(\"shakespeare.txt\") as file:\n    data = file.read()\n\n\nprint(data[:200])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you\n\n\n\n\n6.2 Preprocess\nWe need to convert the text data into numerical data. Using scikit-learn’s LabelEncoder we can map each character to a unique integer. The same encoder will be used to inverse transform the predictions back to characters.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nchar_data = np.array(list(data))\nencoder = LabelEncoder()\nindices_data = encoder.fit_transform(char_data)\n\n\nvocabulary = encoder.classes_\nvocabulary\n\narray(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n      dtype='&lt;U1')\n\n\nAn example of the mapped data:\n\nindices_data[:200]\n\narray([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n       44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n       52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n       51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31,\n       54, 43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56,\n       57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39,\n       56, 43,  1, 39, 50, 50,  1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56,\n       39, 58, 46, 43, 56,  1, 58, 53,  1, 42, 47, 43,  1, 58, 46, 39, 52,\n        1, 58, 53,  1, 44, 39, 51, 47, 57, 46, 12,  0,  0, 13, 50, 50, 10,\n        0, 30, 43, 57, 53, 50, 60, 43, 42,  8,  1, 56, 43, 57, 53, 50, 60,\n       43, 42,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43,\n       52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63, 53, 59])\n\n\n\n\n6.3 Initialize\nNow let’s define the dataloader, the model and the optimizer. I used the following hyperparameters below, but feel free to experiment with different values.\n\nSEQUENCE_LENGTH = 128\nBATCH_SIZE = 32\nVOCAB_SIZE = len(vocabulary)\nTRAIN_SPLIT = 0.8\nLEARNING_RATE = 0.001\nSHUFFLE_TRAIN = True\n\nEMBED_SIZE = 256\nHIDDEN_SIZE = 512\nNUM_LAYERS = 2\nNUM_EPOCHS = 5\n\nDefine the train and test data loaders\n\nfrom data import NextCharDataset, DataLoader\n\ntrainset_size = int(len(indices_data) * TRAIN_SPLIT)\ntrain_data = indices_data[:trainset_size]\ntest_data = indices_data[trainset_size:]\n\ntrainset = NextCharDataset(train_data, SEQUENCE_LENGTH)\ntestset = NextCharDataset(test_data, SEQUENCE_LENGTH)\n\ntrainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=SHUFFLE_TRAIN)\ntestloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n\nDefine the model and optimizer\n\nfrom lstm import LSTMClassifier\nfrom optim import AdamW\n\nmodel = LSTMClassifier(EMBED_SIZE, HIDDEN_SIZE, VOCAB_SIZE, NUM_LAYERS)\noptimizer = AdamW(params=model.layers, grads=model.grad, lr=LEARNING_RATE)\n\n\n\n6.4 Training loop\nThe training loop follows this standard structure:\nfor epoch = 1 to TOTAL_EPOCHS:    \n    // Training Phase\n    for each batch in train_data:\n        predictions = forward_pass(model, batch)\n        loss = compute_loss(predictions, true_labels)\n        gradients = compute_gradients(loss)\n        update_model_parameters(model, gradients)\n        record_metrics(loss, accuracy, ...)\n    \n    // Testing Phase\n    for each batch in test_data:\n        predictions = forward_pass(model, batch)\n        loss = compute_loss(predictions, true_labels)\n        record_metrics(loss, accuracy, ...)\nMost of the implementation such as the forward and backward passes, optimization and data loading is already done. The remaining part is loss computation and gradient of loss w.r.t the predictions. Since next-token prediction is a classification task, we’ll use the cross-entropy loss function.\n\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\nfrom op import cross_entropy\n\nstate = None\ntrain_losses = defaultdict(list)\ntest_losses = defaultdict(list)\n\nfor epoch in tqdm(range(NUM_EPOCHS), desc=\"Epoch\"):\n    # training loop\n    for inputs, targets in (pbar := tqdm(trainloader, leave=False)):\n        if SHUFFLE_TRAIN:\n            state = None\n        probabilities, state, activations = model.forward(inputs, state)\n\n        # cross entropy loss\n        loss = cross_entropy(probabilities, targets)\n        # accuracy\n        accuracy = np.mean(np.argmax(probabilities, axis=-1) == targets)\n\n        # loss gradient w.r.t logits (before softmax)\n        gradient = np.copy(probabilities)\n        # Subtract 1 from the probabilities of the true classes\n        # Since the gradient is p_i - y_i\n        gradient[np.arange(targets.shape[0])[:, None], \n                 np.arange(targets.shape[1]), targets] -= 1\n        # Subtract 1 from the probabilities of the true classes\n        gradient /= gradient.shape[0]\n\n        # backpropagate and update\n        optimizer.zero_grad()\n        model.backward(gradient, activations)\n        optimizer.step()\n\n        # log\n        pbar.set_postfix({\"loss\": f\"{loss:.5f}\", \n                          \"accuracy\": f\"{accuracy*100:.2f}\"})\n        train_losses[epoch].append(loss)\n\n    # testing loop\n    loss_sum = 0\n    accuracy_sum = 0\n    for iter, (inputs, targets) in (pbar := tqdm(enumerate(testloader),\n                                                 leave=False)):\n        probabilities, state, _ = model.forward(\n            inputs, state=None, teacher_forcing=False\n        )\n        loss = cross_entropy(probabilities, targets)\n        accuracy = np.mean(np.argmax(probabilities, axis=-1) == targets)\n\n        loss_sum += loss\n        accuracy_sum += accuracy\n        pbar.set_postfix(\n            {\n                \"loss\": f\"{loss_sum / (iter + 1):.5f}\",\n                \"accuracy\": f\"{accuracy_sum / (iter + 1)*100:.2f}\",\n            }\n        )\n        test_losses[epoch].append(loss)\n\nNow that it’s time for training, the bad news is that the process will be slow! Understandably so, since we’re using NumPy over CPU. Still, I trained the model for ~6000 iterations (batches) to make sure the implementation is correct and that the model is learning. The figure below shows the loss curve decreasing consistently over the iterations.\n\n\n\nTraining Loss Curve\n\n\nFor checkpointing, we can save the model to disk:\n\nnp.save(\"checkpoint.npy\", model.state_dict)\n\nTo reload from the checkpoint, use the from_state_dict method:\n\nstate_dict = np.load(\"checkpoint.npy\", allow_pickle=True).item()\nmodel = LSTMClassifier.from_state_dict(state_dict)\n\nstate_dict.keys()\n\ndict_keys(['config', 'weights', 'grad'])\n\n\n\n\n6.5 Generating text\nAt inference time, we feed the model a prefix text and let it generate the next characters. We can control the number of characters to generate by setting the generate_length parameter in forward. I used greedy decoding to generate the text which works by selecting the character with the highest probability at each time step.\n\ndef generate(model, prefix: str, length: int):\n    inputs = np.array(list(prefix))\n    inputs = encoder.transform(inputs)\n    inputs = inputs[np.newaxis]\n    state = None\n\n    probabilities, state, _ = model.forward(\n        inputs, state, teacher_forcing=False, generation_length=length\n    )\n    tokens = np.argmax(probabilities[0, len(prefix) - 1 :], axis=-1)\n\n    output = prefix + \"\".join(encoder.inverse_transform(tokens))\n    return output\n\n\nprint(generate(model, prefix=\"I will\", length=400))\n\nI will rest blood that bear blood at all,\nAnd stay the king to the consulships?\n\nMENENIUS:\nNay, then he will stay the king to the cause of my son's exile is banished.\n\nROMEO:\nAnd stay the common people: there is no need, that I may call thee back.\n\nNORTHUMBERLAND:\nHere comes the county strict ready to give me leave to see him as he fall be thine, my lord.\n\nKING RICHARD II:\nNorfolk, throw down the coronat\n\n\nLooks like the model was able to learn something! As an alternative to basic sampling, more advanced techniques like beam search, Top-K sampling, and nucleus sampling can significantly enhance the text generation quality but that’d be beyond the scope of this post.\n\nI hope you found this post helpful. If you have any questions or suggestions, feel free to leave a comment. Thanks for reading!"
  },
  {
    "objectID": "about/index.html#about-me",
    "href": "about/index.html#about-me",
    "title": "About Me",
    "section": "About Me",
    "text": "About Me\nPrior to working at Expedia Group, I was a Machine Learning Engineer at Divar, where I worked on pricing and computer vision models for the largest e-commerce platform in Iran. I had the joy and fortune of working with a lot of talented people, and learned how to build and deploy machine learning models at scale.\nI completed my master’s degree in Computer Science at Concordia University, where I was a member of the Immersive & Creative Technologies Lab. I worked on applying deep reinforement learning to satellite imagery data for extracting road networks using controllable agents.\nI’m a passionate about solving problems. In my free time, I often participate in Kaggle-like competitions to learn about applying ML techniques to different problems. I try to keep up with the latest research in the field, and am always looking for ways to improve my skills. One of the reason I started this website is to write about things I discover and share cool stuff.\n\nLet’s do something awesome together!\nIf you’re interested in working with me, please drop me a line at contact@nimasarang.com!\n\n\nA little bit more about me…\n\nHere is a timeline of my professional experience and the projects I’ve worked on: \n\n    \n        2022 - Present\n        \n          \n          Expedia Group\n          Machine Learning Scientist\n        \n        Capital Allocation\n            Capital optimization for search ads bidding on Expedia Group's global brands. My team manages a $100M+ annual budget.\n        \n        Sparsity-Aware ML Models\n            Novel ML, bayesian and tree-based methods for training on large-scale and highly sparse data.\n        \n        Real-time Controllers\n            Developed control-loop systems for real-time bidding on search engine marketing platforms.\n        \n    \n\n    \n        Sep 2021 - Mar 2022\n        \n          \n          Concordia University\n          Instructor\n        \n        Ericsson ML/AI Upskill Training Program\n            I taught PyTorch and Computer Vision tutorials to Ericsson employees.\n            I also mentored three teams throughout the program,\n             guiding them through the implementation and debugging of their projects.\n        \n    \n\n    \n        2020 - 2022\n        \n          \n          Immersive & Creative Technologies Lab\n          ML Researcher\n        \n        Tractable Large-scale Deep Reinforcement Learning\n            Leveraged deep reinforcement learning to solve massive-scale environments and developed an automatic extraction system for urban road networks from high-resolution aerial imagery, in collaboration with CAE.\n        \n    \n\n    \n        2021\n        \n          Stock Trading Agent\n          I spent a good chunk of my free time developing a stock trading agent that uses reinforcement learning and forecasting models to make trading decisions. Even though it wasn't able to beat the B&H strategy, I learned a lot about trading, forecasting, scalability, and offline policy evaluation.\n        \n    \n\n    \n        2019 - 2020\n        \n          \n          Divar\n          Machine Learning Engineer\n        \n        \n          License Plate Detection and Anonymization\n          Developed a real-time pose estimation model for automatically hiding vehicle license plates in images, and published an educational technical blog on the implementation details. \n          This is an example in action.\n        \n        \n          Used Vehicle Price Estimation Model\n          Developed a used-car price valuation model that was deployed as a free \n          SaaS to all users.\n        \n        \n          Client-Side ML for Merchandise Valuation\n          Developed a client-side multi-task AI model for image classification and price estimation of merchandise and commodities in real-time. Deployed on Android using Java and TensorFlow Lite. Used fastText and TF-IDF to automatically tag unlabeled data.\n        \n    \n\n    \n        2019\n        \n          Augmented Reality Soccer Using Deep Learning\n          As my Bachelor's thesis, I developed an two-player augmented reality soccer game played witha virtual ball and field.\n          Built using Unity, an optimized semantic segmentation model, and an object tracking algorithm.\n        \n    \n\n    \n        2017 - 2018\n        Computational Biology Research Center\n        Research Assistant\n        \n          Protein Design\n          Worked on designing protein sequences that can fold into a given tertiary structure using AI and evolutionary profiles.\n        \n    \n\n    \n        2016 - 2017\n        \n          \n          Amirkabir University\n          Competitive Programmer\n        \n        ICPC - ACM International Collegiate Programming Contest\n            I was a member of the university's competitive programming team, and participated in the ACM ICPC regional contest. The algorithmic problems we solved varied from graph theory and dynamic programming, to computational\n            geometry and greedy."
  },
  {
    "objectID": "blog/2024-08-24-information-theory/index.html#introduction",
    "href": "blog/2024-08-24-information-theory/index.html#introduction",
    "title": "Information Theory Fundamentals: Entropy, Cross-Entropy, and KL Divergence",
    "section": "1 Introduction",
    "text": "1 Introduction\nInformation theory concepts appear throughout machine learning and data science, from loss functions in neural networks to variational inference in generative models. Understanding these fundamentals helps make sense of why certain algorithms work the way they do.\nThis guide covers the key concepts with mathematical foundations and interactive examples. We’ll explore entropy as a measure of uncertainty, cross-entropy’s role in model performance, and the important differences between forward and reverse KL divergence. By the end, understanding JS divergence becomes straightforward."
  },
  {
    "objectID": "blog/2024-08-24-information-theory/index.html#entropy",
    "href": "blog/2024-08-24-information-theory/index.html#entropy",
    "title": "Information Theory Fundamentals: Entropy, Cross-Entropy, and KL Divergence",
    "section": "2 Entropy",
    "text": "2 Entropy\nEntropy is a fundamental concept in information theory that quantifies the uncertainty or randomness in a system. Let’s consider a couple of examples:\nImagine you’re trying to guess the next card drawn from a deck. With a well-shuffled deck, each draw is unpredictable – this scenario has high entropy. On the other hand, if the cards were arranged in a known order, each draw would be predictable – this has low entropy.\nNow, let’s think about weather prediction. In a climate where it rains 90% of the days, guessing that it will rain tomorrow is usually correct but not very informative. This scenario has lower entropy than a climate where rain and sun are equally likely (50% each), making predictions more challenging and uncertain.\n\n2.1 Information Content\nWe quantify the uncertainty or information content of an event \\(x\\) with probability \\(p(x)\\) as:\n\\[I(x) = -\\log_2{p(x)}\\]\nIt measures the surprise or “news value” of observing event \\(x\\). \\(I(x)\\) is also referred to as the number of bits needed to encode the event, but we’ll get to that later.\nWhy use \\(-\\log p\\)? Let’s visualize it:\n\n\n\n\n\n\n\n\nFigure 1: Information content as a function of probability\n\n\n\n\n\nThis function captures several intuitive properties:\n\nRare events (low \\(p\\)) have higher information content, as \\(\\displaystyle{\\lim_{p \\to 0^+} (-\\log{p}) = +\\infty}.\\)\nCertain events (\\(p = 1\\)) have zero information content.\nThe negative sign ensures positive values.\n\n\n\n\n\n\n\nTipBoltzmann’s Entropy\n\n\n\n\n\nIt seems that the idea of using natural logarithm in entropy was first introduced by Boltzmann in the context of thermodynamics [1]. I’m interested to read on this later.\n\n\n\n\n\n2.2 Shannon Entropy\nShannon entropy is the expected value (weighted average) of information content across all possible events in a probability distribution:\n\\[H(X) = \\mathbb{E}[-\\log p(X)] = -\\sum_x p(x) \\log{p(x)} \\tag{1}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf \\(p(x) = 0\\), then \\(p(x)\\log{p(x)} = 0\\times\\log{0} \\boldsymbol{= 0}\\). This is because \\(\\displaystyle{\\lim_{x \\to 0^+} x\\log{x} = 0}\\) and can be proven using L’Hôpital’s rule [2]: \\(\\displaystyle{\\lim_{x \\to 0^+} x\\log{x} = 0 = \\lim_{x \\to 0^+} \\frac{\\log{x}}{1/x} \\;\\xlongequal{\\text{L'Hôpital}}\\; \\lim_{x \\to 0^+} \\frac{1/x}{-1/x^2} = \\lim_{x \\to 0^+} -x = 0}\\)\n\n\n\nTo put this into perspective, English text typically has an entropy of about 1-1.5 bits per character. In comparison, a random string of characters over the same alphabet has an entropy of \\[\\mathbb{E}[-\\log_2{p(X)}] = -\\sum\\limits_{i=1}^{26} \\frac{1}{26} \\log_2(\\frac{1}{26}) = \\log_2(26) \\approx 4.7\\] bits per character.\nThis concept has practical applications in various fields. For instance, in cybersecurity, a truly random string of characters (like a strong password) approaches the maximum entropy of \\(\\log_2(n)\\) bits per character, where \\(n\\) is the number of possible characters. The password “password123” has low entropy and is easily guessable, while “Tr0ub4dor&Co” has higher entropy and is more secure. This difference in entropy reflects the varying degrees of unpredictability and, consequently, the strength of these passwords.\n\n\n2.3 A bit of history\nThe terms “coding” and “encoded” in information theory stem from Claude Shannon’s work on efficient information transmission over noisy channels. Entropy \\(H(X)\\) represents the theoretical lower bound on the average number of bits needed to encode symbols from a source, which is why we measure it in bits (using log base 2) - it directly relates to optimal coding length.\nShannon’s Source Coding Theorem formalizes this: no lossless compression can exceed this efficiency on average. This explains why English text (~1.5 bits of entropy per character) compresses far more than random text (~4.7 bits per character), and why compression algorithms excel with natural language but struggle with random data. Understanding entropy as a limit on coding efficiency makes it a powerful tool across various domains like data compression, cryptography, and machine learning."
  },
  {
    "objectID": "blog/2024-08-24-information-theory/index.html#cross-entropy",
    "href": "blog/2024-08-24-information-theory/index.html#cross-entropy",
    "title": "Information Theory Fundamentals: Entropy, Cross-Entropy, and KL Divergence",
    "section": "3 Cross Entropy",
    "text": "3 Cross Entropy\nImagine you have a source that produces messages according to a true distribution \\(P\\), but you’re using a different distribution \\(Q\\) to encode these messages. Cross-entropy measures the average number of bits you’ll need using this possibly inefficient encoding.\nFor example, consider describing a language you’ve never seen before. You start by guessing how often each letter appears.\n\nThe true frequency (\\(P\\)) of letters in this language is: \\(A: 40\\%, \\; B: 40\\%, \\; C: 20\\%\\)\nYour initial guess (\\(Q\\)) is: \\(A: 60\\%, \\; B: 30\\%, \\; C: 10\\%\\)\n\nNow, let’s use your guess to “encode” messages in this language, i.e., calculate its uncertainty:\n\nFor each ‘A’, we use \\(-\\log_2{0.6} \\approx 0.74\\) bits\nFor each ‘B’, we use \\(-\\log_2{0.3} \\approx 1.74\\) bits\nFor each ‘C’, we use \\(-\\log_2{0.1} \\approx 3.32\\) bits\n\nTo calculate the average number of bits per letter, we use \\(P\\)’s frequencies because that’s how often each letter actually appears, while \\(Q\\) determines the cost for each letter. This combination tells us the real-world performance of our encoding scheme.\n\\[ \\left(0.4 \\times 0.74\\right) + \\left(0.4 \\times 1.74\\right) + \\left(0.2 \\times 3.32\\right) \\approx 1.57 \\text{ bits per letter}\\]\nThe resulting average, \\(H(P,Q) \\approx 1.57\\) bits, represents the typical encoding cost per letter when our guess (\\(Q\\)) is used to encode messages that actually follow the true distribution (\\(P\\)).\nThis approach mirrors how we evaluate predictions in ML: Our model makes guesses (Q), but the world behaves according to the true probabilities (P). Cross-entropy quantifies the average encoding length under this scenario.\nFormal Definition:\nThe cross-entropy between two probability distributions \\(p\\) and \\(q\\) over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme used for the set is optimized for an estimated probability distribution \\(q\\), rather than the true distribution \\(p\\).\n\\[H(P, Q) = -\\sum\\limits_{x \\in X} p(x)\\log{q(x)} = \\mathbb{E}_{x \\sim P}[-\\log{q(x)}] \\tag{2}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf \\(p(x) &gt; 0\\) and \\(q(x) = 0\\) for some \\(x\\), then \\(p(x)\\log{q(x)}\\) is undefined because of \\(\\log{0}\\). To address this we can add a small epsilon to \\(q(x)\\) to avoid \\(\\log{0}\\). In fact, this is a common practice in ML libraries.\n\n\n\nBinary Cross-Entropy:\n\nIn binary classification, we deal with two classes, and the predicted probability for one class inherently determines the probability for the other. If \\(q\\) is the predicted probability of the positive class (class 1), then \\(1-q\\) is the probability of the negative class (class 0). The binary cross-entropy loss combines the log-likelihood terms for both possible classes (positive and negative) into a single expression. For a single data point, it is defined as:\n\\[-\\left[y\\,log(q)\\;+\\;(1−y)\\,log(1−q)\\right]\\]"
  },
  {
    "objectID": "blog/2024-08-24-information-theory/index.html#kullbackleibler-divergence",
    "href": "blog/2024-08-24-information-theory/index.html#kullbackleibler-divergence",
    "title": "Information Theory Fundamentals: Entropy, Cross-Entropy, and KL Divergence",
    "section": "4 Kullback–Leibler Divergence",
    "text": "4 Kullback–Leibler Divergence\nTo measure the expected number of extra bits required to code samples from \\(P\\) using a code optimized for \\(Q\\) rather than the code optimized for \\(P\\), we use the Kullback-Leibler divergence. It is defined as:\n\\[\n\\begin{aligned}\nD_{KL}(P \\;\\Vert\\; Q) &= H(P, Q) - H(P) && \\hspace{-2em}\\text{\\small{(extra bits to code $P$ using $Q$)}} \\\\[3ex]\n&= \\mathbb{E}_{x \\sim P}[-\\log q(x)] - \\mathbb{E}_{x \\sim P}[-\\log p(x)] \\\\[3ex]\n&= -\\sum\\limits_{x \\in X} p(x)\\log{q(x)} + \\sum\\limits_{x \\in X} p(x)\\log{p(x)} \\\\[1ex]\n&= \\sum\\limits_{x \\in X} p(x)\\log{\\frac{p(x)}{q(x)}} && \\hspace{-2em}\\text{\\small{(as $\\log{a} - \\log{b} = \\log{\\frac{a}{b}}$)}}\n\\end{aligned}\n\\tag{3}\\]\nThe relative entropy \\({D_{\\text{KL}}(P\\parallel Q)}\\) quantifies how far the distribution \\(Q\\) is from the distribution \\(P\\). The cross-entropy alone cannot be thought of as a distance, since \\({H(P,P)=:H(P)}\\) isn’t zero. This can be fixed by subtracting \\({H(P)}\\) to make \\({D_{\\text{KL}}(P\\parallel Q)}\\) agree more closely with our notion of distance, as the excess loss.\n\n\n\n\n\n\nNote\n\n\n\n\n\nKL divergence is asymmetric, i.e. \\({D_{\\text{KL}}(P\\parallel Q)\\neq D_{\\text{KL}}(Q\\parallel P)}\\) except when \\(P = Q\\).\n\n\n\n\n4.1 Non-Negativity\nIf \\(Q = P\\), then \\(D_{KL}(P \\;\\Vert\\; P) = H(P, P) - H(P) = 0\\). But is zero the lower bound for \\(D_{KL}\\)? Basically the question is whether: \\[H(P, Q) \\stackrel{?}{\\geq} H(P) \\quad \\forall P, Q\\]\nThis is known as Gibbs’ inequality, and it’s a fundamental property of the Kullback-Leibler divergence. It tells us that if you try to use a probability distribution \\(Q\\) to encode data that actually follows a different distribution \\(P\\), you will always need more bits on average, compared to using \\(P\\) itself.\nTo prove \\(D_{KL}(P \\;\\Vert\\; Q) \\stackrel{?}{\\ge} 0\\), we can use the fact that \\(-\\log\\) is a convex function (see Figure 1). By Jensen’s inequality, we have:\n\\[\n\\begin{aligned}\n\\sum\\limits_{x} p(x)\\log{\\frac{p(x)}{q(x)}} &= \\sum\\limits_{x} p(x)\\times -\\log{\\frac{q(x)}{p(x)}} \\\\[2ex]\n&\\ge -\\log\\left(\\sum_{x} p(x) {\\frac{q(x)}{p(x)}}\\right) && \\text{\\small{(Jensen's inequality)}} \\\\[2ex]\n&= -\\log\\left(\\sum_{x} q(x)\\right) = -\\log {1} && \\text{\\small{(sum of probabilities = 1)}} \\\\[2ex]\n& = 0\n\\end{aligned}\n\\] \\[ \\therefore \\quad D_{KL}(P \\;\\Vert\\; Q) \\ge 0\\]\n\n\n\n\n\n\nTipJensen’s Inequality\n\n\n\n\n\nJensen’s inequality states that for a convex function \\(f\\) and a random variable \\(X\\):\n\\[f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)]\\]\nSimilarly, for a concave function \\(f\\): \\[f(\\mathbb{E}[X]) \\geq \\mathbb{E}[f(X)]\\]\n\n\n\n\n\n4.2 At Zero Probabilities\nLet’s consider the cases where \\(p(x) = 0\\) or \\(q(x) = 0\\), because it’ll be useful later on:\n\n\\(p(𝑋)=0 \\text{ and } q(𝑋)\\gt 0\\): As a result, \\(\\displaystyle\\lim_{p \\to 0} p\\log{p} = 0\\). This makes sense since we sample according to distribution \\(p\\), we’ll never sample event \\(x\\). Hence, it does not weight in \\({D_{KL}(P \\;\\Vert\\; Q)}\\), meaning \\(Q\\) can make mistakes about events that \\(P\\) considers impossible without penalty.\n\\(p(𝑋)\\gt 0 \\text{ and } q(𝑋)= 0\\): Then \\(\\displaystyle\\lim_{q \\to 0^+} \\log{\\frac{1}{q}} = +\\infty\\). An infinite divergence effectively signals that the model \\(Q\\) fails to account for an event \\(x\\) that is observed under \\(P\\).\n\\(p(𝑋)=0 \\text{ and } q(𝑋)= 0\\): Then it’s really undefined :)\n\n\n\n4.3 Reverse KL vs Forward KL\nThis is something that I had never thought about before, because I had only seen KL divergence in the context of variational inference. But it turns out that there are two ways to use KL divergence in an optimization problem. We had previously established that \\(D_{KL}\\) is asymmetric. This means that minimizing \\(D_{KL}(P \\;\\Vert\\; Q)\\) is not the same as minimizing \\(D_{KL}(Q \\;\\Vert\\; P)\\). The former is known as the Forward KL and the latter is known as the Reverse KL. The practical difference between the two arises when the model cannot perfectly fit the true distribution, which is most often the case. This is where the two divergences diverge (pun intended).\nLet’s say we have a model \\(Q\\) that we want to optimize to approximate the true distribution \\(P\\). It’d be a fair assumption that \\(P\\) would be a more complex distribution than \\(Q\\), with more modes and a more complex structure, since \\(Q\\) is a simpler model. For example, \\(P\\) could be a Gaussian mixture model with 10 components (multimodal), while \\(Q\\) is a single Gaussian (unimodal).\n\nForward KL\n\\(D_{KL}(P \\;\\Vert\\; Q)\\) measures the extra bits needed to encode samples from \\(P\\) using a code optimized for \\(Q\\). By minimizing this, we’re trying to make \\(Q\\) as close as possible to \\(P\\):\n\\[ \\begin{aligned}\n\\mathop{\\mathrm{arg\\,min}}\\limits_Q\\; D_{KL}(P \\;\\Vert\\; Q) &= \\mathop{\\mathrm{arg\\,min}}\\limits_Q\\; H(P, Q) - H(P) \\\\[1ex]\n&= \\mathop{\\mathrm{arg\\,min}}\\limits_Q\\; H(P, Q) && \\text{\\small{(since $H(P)$ is constant w.r.t. $Q$)}} \\\\[1ex]\n&= \\mathop{\\mathrm{arg\\,min}}\\limits_Q\\; -\\sum\\limits_{x \\in X} p(x)\\log{q(x)}\n\\end{aligned} \\]\nSo basically minimizing the forward KL is equivalent to minimizing the cross-entropy between \\(P\\) and \\(Q\\), a common objective in supervised classification problems. As we saw in Section 4.2, this loss landscape is interesting around zero probabilities:\n\nIf \\(p(x) = 0\\) then \\(q(x)\\) can be anything, because the loss is zero. The con of this is that it can lead to overconfident predictions in regions with little data.\nIf \\(P(x) \\gt 0\\) and \\(Q(x) \\le \\epsilon\\), then the loss becomes significantly large. As a consequence, \\(Q\\) would rather assign a low probability to events that are very unlikely under \\(P\\) than missing them entirely.\n\nIn the interactive plot below you can see how the forward KL divergence changes as you adjust the mean and variance of the Gaussian distribution \\(Q\\) (inspired by [3]).\n\n\ndomain = [-5, 5];\nmeanRange = [-3, 3];\nmeanStep = 0.1;\nvarianceRange = [0.5, 3];\nvarianceStep = 0.1;\n\nfont_styles = `\n    svg text {\n        fill: currentColor;\n    }\n    .tick text {\n        font-size: 12px;\n    }\n    .plot-title {\n        font-size: 18px; font-weight: bold;\n    }\n    .axis-label {\n        font-size: 16px;\n    }\n    .tooltip-text {\n        font-size: 16px; font-weight: bold;\n        color: white;\n    }\n`;\n\n\n// Cell 1: Import libraries\nd3 = require(\"d3\")\nPlot = require(\"@observablehq/plot\")\nimport {slider} from '@jashkenas/inputs'\n\n// Cell 2: Input sliders\nviewof mean = slider({\n  min: meanRange[0],\n  max: meanRange[1],\n  step: meanStep,\n  value: -1.4, \n  title: \"Mean\", \n})\nviewof variance = slider({\n  min: varianceRange[0],\n  max: varianceRange[1],\n  step: varianceStep,\n  value: 1.5, \n  title: \"Variance\",\n})\n\n\n// Static P distribution: Gaussian Mixture Model\nP = generateDataFromGMM([\n  {mean: -2.5, variance: 0.3, weight: 0.4},\n  {mean: 2, variance: 0.5, weight: 0.6}\n]);\n\n// Cell 3: Gaussian mixture function, data generation, and KL divergence calculation\nfunction gaussian(x, mean, variance) {\n  return (1 / Math.sqrt(2 * Math.PI * variance)) * Math.exp(-Math.pow(x - mean, 2) / (2 * variance));\n}\n\n// Define Gaussian Mixture Model for P\nfunction gaussianMixture(x, components) {\n  // components is an array of objects with {mean, variance, weight}\n  return components.reduce((sum, {mean, variance, weight}) =&gt; {\n    return sum + weight * gaussian(x, mean, variance);\n  }, 0);\n}\n\nfunction generateDataFromGMM(components) {\n  return d3.range(domain[0], domain[1], 0.1).map(x =&gt; ({\n    x: x,\n    y: gaussianMixture(x, components)\n  }));\n}\n\n// Generate data for Q (simple Gaussian)\nfunction generateData(mean, variance) {\n  return d3.range(domain[0], domain[1], 0.1).map(x =&gt; ({\n    x: x,\n    y: gaussian(x, mean, variance)\n  }));\n}\n\n// Calculate KL divergence\nfunction calculateKLDivergence(P, Q) {\n  return P.reduce((sum, p, i) =&gt; {\n    const q = Q[i];\n    if (p.y &gt; 0 && q.y &gt; 0) {\n      return sum + p.y * Math.log(p.y / q.y);\n    }\n    return sum;\n  }, 0);\n}\n\n// Cell 4: Calculate KL divergence for contour plot\nfunction calculateKLDivergenceGrid(dist1, meanRange, varianceRange, forward=true) {\n   const grid = [];\n   let kl;\n   for (let m of meanRange) {\n      for (let v of varianceRange) {\n        const dist2 = generateData(m, v);\n        if (forward) {\n            kl = calculateKLDivergence(dist1, dist2);\n        } else {\n            kl = calculateKLDivergence(dist2, dist1);\n        }\n        grid.push({mean: m, variance: v, kl: kl});\n      }\n   }\n   return grid;\n}\n\n// Cell 5: Create and update the Gaussian mixture model plot\nfunction createGuassianPlot(mean, variance, P) {\n  const width = 600;\n  const height = 400;\n  const margin = {top: 20, right: 20, bottom: 40, left: 60};\n\n  const dataQ = generateData(mean, variance);\n  \n  const x = d3.scaleLinear()\n    .domain(domain)\n    .range([margin.left, width - margin.right]);\n\n  const y = d3.scaleLinear()\n    .domain([0, d3.max([...P, ...dataQ], d =&gt; d.y)])\n    .range([height - margin.bottom, margin.top]);\n\n  const line = d3.line()\n    .x(d =&gt; x(d.x))\n    .y(d =&gt; y(d.y));\n\n  const svg = d3.create(\"svg\")\n    .attr(\"viewBox\", [0, 0, width, height]);\n\n  // Add font\n  svg.append(\"style\").text(font_styles);\n\n  // X-axis\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n    .call(d3.axisBottom(x));\n  \n  // Y-axis\n  const yAxis = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},0)`)\n    .call(d3.axisLeft(y));\n  \n  // X-axis label\n  svg.append(\"text\")\n    .attr(\"class\", \"axis-label\")\n    .attr(\"x\", width / 2)\n    .attr(\"y\", height)\n    .attr(\"text-anchor\", \"middle\")\n    .text(\"X\");\n\n  // Y-axis label (centered and rotated)\n  svg.append(\"text\")\n    .attr(\"class\", \"axis-label\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"x\", -height / 2)\n    .attr(\"y\", 17.5)\n    .attr(\"text-anchor\", \"middle\")\n    .text(\"Probability Density\");\n\n  // Line for Q (interactive)\n  const pathQ = svg.append(\"path\")\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"steelblue\")\n    .attr(\"stroke-width\", 2);\n  \n  // Line for P (static GMM)\n  svg.append(\"path\")\n    .datum(P)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"orange\")\n    .attr(\"stroke-width\", 2)\n    .attr(\"d\", line);\n  \n  // Legend\n  const legend = svg.append(\"g\")\n    .attr(\"transform\", `translate(${width - margin.right - 25},${margin.top})`);\n  \n  legend.append(\"rect\")\n    .attr(\"width\", 20)\n    .attr(\"height\", 20)\n    .attr(\"fill\", \"steelblue\");\n  \n  legend.append(\"text\")\n    .attr(\"x\", 25)\n    .attr(\"y\", 15)\n    .text(\"Q\");\n  \n  legend.append(\"rect\")\n    .attr(\"y\", 25)\n    .attr(\"width\", 20)\n    .attr(\"height\", 20)\n    .attr(\"fill\", \"orange\");\n  \n  legend.append(\"text\")\n    .attr(\"x\", 25)\n    .attr(\"y\", 40)\n    .text(\"P\");\n\n  // Update Q distribution\n  y.domain([0, d3.max([...P, ...dataQ], d =&gt; d.y)]);\n  yAxis.call(d3.axisLeft(y));\n  pathQ.datum(dataQ).attr(\"d\", line);\n  \n  return svg.node();\n}\n\n// Cell 6: Refactored KL divergence contour plot\nfunction createklContourPlot(mean, variance, klGrid, cntTitle) {\n  const width = 600;\n  const height = 400;\n  const margin = {top: 50, right: 30, bottom: 45, left: 50};\n  const plotWidth = width - margin.left - margin.right;\n  const plotHeight = height - margin.top - margin.bottom;\n\n  const uniqueMeans = Array.from(new Set(klGrid.map(d =&gt; d.mean)));\n  const uniqueVariances = Array.from(new Set(klGrid.map(d =&gt; d.variance)));\n\n  // Create 2D array of KL values\n  const klValues = uniqueVariances.map(v =&gt; \n    uniqueMeans.map(m =&gt; {\n      const point = klGrid.find(d =&gt; d.mean === m && d.variance === v);\n      return point ? point.kl : null;\n    })\n  );\n\n  // Scales\n  const xScale = d3.scaleLinear()\n    .domain(d3.extent(uniqueMeans))\n    .range([0, plotWidth]);\n\n  const yScale = d3.scaleLinear()\n    .domain(d3.extent(uniqueVariances))\n    .range([plotHeight, 0]);\n\n  const colorScale = d3.scaleSequential(d3.interpolateViridis)\n    .domain(d3.extent(klGrid, d =&gt; d.kl));\n  \n  // Generate contours\n  const contours = d3.contours()\n    .size([uniqueMeans.length, uniqueVariances.length])\n    .thresholds(50)\n    (klValues.flat());\n\n  // Create SVG element\n  const svg = d3.create(\"svg\")\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"style\", \"max-width: 100%; height: auto;\");\n\n  // Add font\n  svg.append(\"style\").text(font_styles);\n\n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`);\n\n  // Draw contour paths\n  g.append(\"g\")\n    .selectAll(\"path\")\n    .data(contours)\n    .join(\"path\")\n    .attr(\"fill\", d =&gt; colorScale(d.value))\n    .attr(\"d\", d3.geoPath()\n      .projection(d3.geoTransform({\n        point: function(x, y) {\n          // Projection mapping based on exact data grid size\n          const scaleX = plotWidth / (uniqueMeans.length - 1);\n          const scaleY = plotHeight / (uniqueVariances.length - 1);\n          this.stream.point(x * scaleX, plotHeight - y * scaleY);  // Corrected projection\n        }\n      }))\n    );\n\n  // X-axis\n  g.append(\"g\")\n    .attr(\"transform\", `translate(0,${plotHeight})`)\n    .call(d3.axisBottom(xScale).ticks(10))\n    .append(\"text\")\n    .attr(\"class\", \"axis-label\")\n    .attr(\"x\", plotWidth / 2)\n    .attr(\"y\", 40)\n    .attr(\"fill\", \"currentColor\")\n    .attr(\"text-anchor\", \"middle\")\n    .text(\"Mean\");\n\n  // Y-axis (centered label and rotated)\n  g.append(\"g\")\n    .call(d3.axisLeft(yScale).ticks(10))\n    .append(\"text\")\n    .attr(\"class\", \"axis-label\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"x\", -plotHeight / 2)\n    .attr(\"y\", -35)\n    .attr(\"text-anchor\", \"middle\")\n    .attr(\"fill\", \"currentColor\")\n    .text(\"Variance\");\n\n  svg.append(\"text\")\n   .attr(\"class\", \"plot-title\")\n   .attr(\"x\", width / 2)\n   .attr(\"y\", margin.top - 32.5)\n   .attr(\"text-anchor\", \"middle\")\n   .html(cntTitle);\n\n  // Function to update point dynamically\n  function updatePoint(mean, variance) {\n    const nearestPoint = klGrid.reduce((a, b) =&gt; \n      (Math.abs(b.mean - mean) + Math.abs(b.variance - variance) &lt; \n       Math.abs(a.mean - mean) + Math.abs(a.variance - variance)) ? b : a\n    );\n\n    g.selectAll(\".current-point, .kl-value\").remove();\n\n    g.append(\"circle\")\n      .attr(\"class\", \"current-point\")\n      .attr(\"cx\", xScale(mean))\n      .attr(\"cy\", yScale(variance))\n      .attr(\"r\", 5)\n      .attr(\"fill\", \"red\")\n      .attr(\"stroke\", \"white\");\n\n    g.append(\"text\")\n      .attr(\"class\", \"tooltip-text\")\n      .attr(\"x\", xScale(mean))\n      .attr(\"y\", yScale(variance) - 10)\n      .attr(\"text-anchor\", \"middle\")\n      .attr(\"fill\", \"white\")\n      .attr(\"stroke\", \"black\")\n      .attr(\"stroke-width\", 2)\n      .attr(\"paint-order\", \"stroke\")\n      .text(nearestPoint.kl.toFixed(2));\n  }\n\n  updatePoint(mean, variance);\n\n  return Object.assign(svg.node(), {updatePoint});\n}\n\nklGrid = calculateKLDivergenceGrid(\n    P,\n    d3.range(meanRange[0], meanRange[1] + meanStep, meanStep),\n    d3.range(varianceRange[0], varianceRange[1] + varianceStep, varianceStep),\n    true\n);\ngaussianPlot = createGuassianPlot(mean, variance, P);\nklContourPlot = createklContourPlot(\n    mean, variance, klGrid,\n    `KL Divergence (&lt;tspan fill=\"orange\"&gt;P&lt;/tspan&gt;, &lt;tspan fill=\"steelblue\"&gt;Q&lt;/tspan&gt;) Contour Plot`\n);\n\nhtml`&lt;div style=\"display: flex; align-items: center; column-gap: 1em;\"&gt;\n &lt;div style=\"flex-basis:50%\"&gt;${gaussianPlot}&lt;/div&gt;\n &lt;div style=\"flex-basis:50%\"&gt;${klContourPlot} &lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, the minimum is achieved when \\(Q\\) is spread out enough to capture the full support of \\(P\\). This is the mean-seeking behavior of the forward KL divergence.\n\n\nReverse KL\n\\(D_{KL}(Q \\;\\Vert\\; P)\\) measures the extra bits needed to encode samples from \\(Q\\) using a code optimized for \\(P\\). Similar to the Section 4.2 analysis:\n\nIt penalizes distributions \\(Q\\) that assign non-zero probability to regions where \\(P\\) is zero, since we’re minimizing \\(q(x)\\log{\\frac{q(x)}{p(x)}}\\). Conversely, it doesn’t penalize \\(Q\\) for missing regions where \\(P\\) has non-zero probability.\nIt tends to capturing the dominant mode of \\(P\\) and ignoring the rest. This can be demonstrated by the following example.\n\nThe plot below is same as the previous one, but this time the contour plot shows the reverse KL divergence.\n\n\nviewof mean2 = slider({\n  min: meanRange[0],\n  max: meanRange[1],\n  step: meanStep,\n  value: -1.4, \n  title: \"Mean\", \n})\nviewof variance2 = slider({\n  min: varianceRange[0],\n  max: varianceRange[1],\n  step: varianceStep,\n  value: 1.5, \n  title: \"Variance\",\n})\n\nklGrid2 = calculateKLDivergenceGrid(\n    P,\n    d3.range(meanRange[0], meanRange[1] + meanStep, meanStep),\n    d3.range(varianceRange[0], varianceRange[1] + varianceStep, varianceStep),\n    false\n);\ngaussianPlot2 = createGuassianPlot(mean2, variance2, P);\nklContourPlot2 = createklContourPlot(\n    mean2, variance2, klGrid2,\n    `KL Divergence (&lt;tspan fill=\"steelblue\"&gt;Q&lt;/tspan&gt;, &lt;tspan fill=\"orange\"&gt;P&lt;/tspan&gt;) Contour Plot`\n);\n\n\nhtml`&lt;div style=\"display: flex; align-items: center; column-gap: 1em;\"&gt;\n &lt;div style=\"flex-basis:50%\"&gt;${gaussianPlot2}&lt;/div&gt;\n &lt;div style=\"flex-basis:50%\"&gt;${klContourPlot2} &lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can see how the minimum is achieved when \\(Q\\) is concentrated around the dominant mode of \\(P\\). This is the mode-seeking/zero-forcing behavior of the reverse KL divergence [4], and makes it desirable in the context of density estimation and variational inference. My own intuition is that in these tasks we would rather find a tight, concentrated approximation and miss some modes, than spread out and place density in low-probability regions.\n\n\n\n4.4 Relations to Mutual Information\nLet \\((X,Y)\\) be a pair of random variables with values over the space \\({\\mathcal {X}}\\times {\\mathcal {Y}}\\). If their joint distribution is \\({P_{(X,Y)}}\\) and the marginal distributions are \\({P_{Y}}\\) and \\({P_{Y}}\\), the mutual information is defined as:\n\\[{I(X;Y)=D_{\\mathrm {KL} }(P_{(X,Y)}\\|P_{X}P_{Y})} \\tag{4}\\]\nNotice, as per property of the KL divergence that \\(I(X;Y)\\) is equal to zero precisely when the joint distribution coincides with the product of the marginals, i.e. when \\(X\\) and \\(Y\\) are independent (and hence observing \\(Y\\) tells you nothing about \\(X\\)). Intuitively, \\(I(X;Y)\\) is a measure of the price for encoding \\((X,Y)\\) as a pair of independent random variables when in reality they may not be.\nMutual information can also be expressed using conditional entropy:\n\\[{I(X;Y) \\equiv H(X) - H(X \\vert Y)}\\]\n Check out this video for a more detailed explanation: [5]"
  },
  {
    "objectID": "blog/2024-08-24-information-theory/index.html#jensen-shannon-divergence",
    "href": "blog/2024-08-24-information-theory/index.html#jensen-shannon-divergence",
    "title": "Information Theory Fundamentals: Entropy, Cross-Entropy, and KL Divergence",
    "section": "5 Jensen-Shannon Divergence",
    "text": "5 Jensen-Shannon Divergence\nJS divergence is a symmetric measure, derived from KL divergence, and it does satisfy the properties of a metric. It is defined as the average of the KL divergence from each distribution to the average of both distributions:\n\\[{JS(P \\;\\Vert\\; Q) = \\frac{1}{2}KL(P \\;\\Vert\\; M) + \\frac{1}{2}KL(Q \\;\\Vert\\; M)} \\tag{5}\\]\nwhere \\(M=\\dfrac{1}{2} (P+Q)\\) is a mixture of \\(P\\) and \\(Q\\).\nThe averaging process and the introduction of \\(M\\) ensure that the divergence remains finite, even if one distribution assigns zero probability to an event that the other distribution assigns a positive probability.\nThis can be a desirable property in scenarios where you want a more stable and bounded measure of divergence. However, it also means that JS divergence is less sensitive to cases where one distribution completely ignores an event that the other considers possible."
  },
  {
    "objectID": "blog/2025-01-30-post-mortem/index.html#frame-inspection",
    "href": "blog/2025-01-30-post-mortem/index.html#frame-inspection",
    "title": "Saving Time: Post-mortem Debugging in Python",
    "section": "4.1 Frame Inspection",
    "text": "4.1 Frame Inspection\nLet’s try a more complex example with multiple exceptions. Using our earlier example from Section 3, let’s run train_model with a length of 5:\n\nfrom src.run import train_model\n\ntrain_model(length=5)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~/Nimas/nsarang.github.io/blog/2025-01-30-post-mortem/src/model.py:5, in ModelClass.train(self, data)\n      4     data_norm = data / sum(data)\n----&gt; 5     self._validate_data(data_norm)\n      6 except:\n\nFile ~/Nimas/nsarang.github.io/blog/2025-01-30-post-mortem/src/model.py:11, in ModelClass._validate_data(self, data)\n     10 if len(data) &lt; 10:\n---&gt; 11     raise ValueError(\"First exception: Data is too short.\")\n\nValueError: First exception: Data is too short.\n\nDuring handling of the above exception, another exception occurred:\n\nRuntimeError                              Traceback (most recent call last)\nCell In[15], line 3\n      1 from src.run import train_model\n----&gt; 3 train_model(length=5)\n\nFile ~/Nimas/nsarang.github.io/blog/2025-01-30-post-mortem/src/run.py:9, in train_model(length)\n      7 model = ModelClass()\n      8 data = np.arange(length)\n----&gt; 9 model.train(data)\n\nFile ~/Nimas/nsarang.github.io/blog/2025-01-30-post-mortem/src/model.py:7, in ModelClass.train(self, data)\n      5     self._validate_data(data_norm)\n      6 except:\n----&gt; 7     raise RuntimeError(\"Second exception: This is a dummy exception.\")\n\nRuntimeError: Second exception: This is a dummy exception.\n\n\n\nThe above is Jupyter’s error message. Let’s see how ours compares:\n\ntraceback_message, frame_info = extract_from_exception(context_lines=5, max_indent=4)\nprint(traceback_message)\n\n┌─── Frame 0 ────────────────────────────────────────\nFunction train, in file \"/Users/nsarang/Nimas/nsarang.github.io/blog/2025-01-30-post-mortem/src/model.py\"\n     3:      try:\n     4:          data_norm = data / sum(data)\n ➤➤➤ 5:          self._validate_data(data_norm)\n     6:      except:\n     7:          raise RuntimeError(\"Second exception: This is a dummy exception.\")\n \n┌─── Frame 1 ────────────────────────────────────────\nFunction _validate_data, in file \"/Users/nsarang/Nimas/nsarang.github.io/blog/2025-01-30-post-mortem/src/model.py\"\n      7:              raise RuntimeError(\"Second exception: This is a dummy exception.\")\n      8:  \n      9:      def _validate_data(self, data):\n     10:          if len(data) &lt; 10:\n ➤➤➤ 11:              raise ValueError(\"First exception: Data is too short.\")\n \nValueError: First exception: Data is too short.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\n┌─── Frame 2 ────────────────────────────────────────\nFunction run_code, in file \"/Users/nsarang/conda/envs/arsem-bidding/lib/python3.11/site-packages/IPython/core/interactiveshell.py\"\n     3547:              await eval(code_obj, self.user_global_ns, self.user_ns)\n     3548:          else:\n ➤➤➤ 3549:              exec(code_obj, self.user_global_ns, self.user_ns)\n     3550:      finally:\n     3551:          # Reset our crash handler in place\n \n┌─── Frame 3 ────────────────────────────────────────\nFunction &lt;module&gt;, in file \"/var/folders/29/fh16rbz95b99yz5df3c6yt2h0000gn/T/ipykernel_96683/4277452375.py\"\n     1:  from src.run import train_model\n     2:  \n ➤➤➤ 3:  train_model(length=5)\n \n┌─── Frame 4 ────────────────────────────────────────\nFunction train_model, in file \"/Users/nsarang/Nimas/nsarang.github.io/blog/2025-01-30-post-mortem/src/run.py\"\n      5:  \n      6:  def train_model(length: int):\n      7:      model = ModelClass()\n      8:      data = np.arange(length)\n ➤➤➤  9:      model.train(data)\n \n┌─── Frame 5 ────────────────────────────────────────\nFunction train, in file \"/Users/nsarang/Nimas/nsarang.github.io/blog/2025-01-30-post-mortem/src/model.py\"\n      5:              self._validate_data(data_norm)\n      6:          except:\n ➤➤➤  7:              raise RuntimeError(\"Second exception: This is a dummy exception.\")\n      8:  \n      9:      def _validate_data(self, data):\n \nRuntimeError: Second exception: This is a dummy exception.\n\n\nLook at that! The full chain of exceptions is displayed with the relevant code context. We can now inspect the variables at each step and even rerun the code to see what happens.\n\nframe_info[0][\"locals\"]\n\n{'self': &lt;src.model.ModelClass at 0x105e1ee10&gt;,\n 'data': array([0, 1, 2, 3, 4]),\n 'data_norm': array([0. , 0.1, 0.2, 0.3, 0.4])}\n\n\n\nprint(frame_info[1][\"message\"])\n\n# Check if the length of 'data' is actually less than 10\nframe_info[1][\"locals\"]\n\n┌─── Frame 1 ────────────────────────────────────────\nFunction _validate_data, in file \"/Users/nsarang/Nimas/nsarang.github.io/blog/2025-01-30-post-mortem/src/model.py\"\n      7:              raise RuntimeError(\"Second exception: This is a dummy exception.\")\n      8:  \n      9:      def _validate_data(self, data):\n     10:          if len(data) &lt; 10:\n ➤➤➤ 11:              raise ValueError(\"First exception: Data is too short.\")\n\n\n\n{'self': &lt;src.model.ModelClass at 0x105e1ee10&gt;,\n 'data': array([0. , 0.1, 0.2, 0.3, 0.4])}\n\n\n\nprint(frame_info[4][\"message\"])\n\n# Going back to the 'train_model' call\nframe_info[4][\"locals\"]\n\n┌─── Frame 4 ────────────────────────────────────────\nFunction train_model, in file \"/Users/nsarang/Nimas/nsarang.github.io/blog/2025-01-30-post-mortem/src/run.py\"\n      5:  \n      6:  def train_model(length: int):\n      7:      model = ModelClass()\n      8:      data = np.arange(length)\n ➤➤➤  9:      model.train(data)\n\n\n\n{'length': 5,\n 'model': &lt;src.model.ModelClass at 0x105e1ee10&gt;,\n 'data': array([0, 1, 2, 3, 4])}"
  },
  {
    "objectID": "blog/2025-01-30-post-mortem/index.html#code-execution",
    "href": "blog/2025-01-30-post-mortem/index.html#code-execution",
    "title": "Saving Time: Post-mortem Debugging in Python",
    "section": "4.2 Code Execution",
    "text": "4.2 Code Execution\nWhat if we want to execute some code within a frame’s context, just like in pdb?\n\ndef execute(source: str, context: dict):\n    \"\"\"\n    Execute the given source code in the given context.\n    \"\"\"\n    source = textwrap.dedent(source)\n    # compile for better performance\n    code = compile(source, \"&lt;string&gt;\", \"exec\")\n    exec(code, context[\"globals\"], context[\"locals\"])\n\n\nexecute(r\"\"\"\n        data = data + 10\n        print(\"data:\", data)\n        print(\"locals:\", locals().keys())\n        \"\"\",\n        context=frame_info[4])\n\ndata: [10 11 12 13 14]\nlocals: dict_keys(['length', 'model', 'data'])\n\n\nSee that? The data variable got updated in place, and the new value shows up in the locals dictionary.\nWe could also bring the frame’s locals into IPython’s namespace like in Andy’s approach, but there’s a risk of overwriting existing variables. Best to be selective about what we bring in."
  },
  {
    "objectID": "blog/2025-08-07-github-workflow-tips/index.html",
    "href": "blog/2025-08-07-github-workflow-tips/index.html",
    "title": "Github Workflow Tips and Tricks",
    "section": "",
    "text": "I have been using Github workflows for a while now, and there are just so many things to learn about. There were a couple of things I learned the hard way, either due to lack of documentation or being unobvious. So I decided to compile a few of them into this post and attempted to present them in a way that is easy to understand. My style of writing is that I defer some of the details to the inline comments within the code, so be sure to go through them as well. You can find the full code examples here ."
  },
  {
    "objectID": "blog/2025-08-07-github-workflow-tips/index.html#workflow-dispatch-with-inputs",
    "href": "blog/2025-08-07-github-workflow-tips/index.html#workflow-dispatch-with-inputs",
    "title": "Github Workflow Tips and Tricks",
    "section": "1 Workflow Dispatch with Inputs",
    "text": "1 Workflow Dispatch with Inputs\nThe workflow_dispatch syntax is to manually trigger a workflow. It has applications like running a deployment or release workflow on demand. It’s also a great way to test the workflows while developing them. What perhaps is not well-known is that you can specify all types of inputs such as dropdowns, checkboxes, text inputs, etc. I found this very useful when I wanted to deploy to different environments. The example below demonstrates using several input types:\nname: Workflow Dispatch with Inputs\non:\n  push:\n    branches: [main]\n\n  workflow_dispatch:\n    inputs:\n      # Arguments\n      artifact_name:\n        type: string # Text field\n        description: 'Artifact identifier'\n        required: true\n      data_source:\n        type: choice # Dropdown selection\n        description: 'Select data source'\n        default: 'sample'\n        options:\n          - sample\n          - test\n          - production\n      send_notification:\n        type: boolean # Checkbox\n        description: 'Send email notification'\n        default: true\nIn this example, the workflow is triggered on every push to the main branch, but it can also be manually triggered with the workflow_dispatch event. To distinguish between the two within the code logic, we can add a configuration step to use the github.event_name context variable in a conditional expression like:\njobs:\n  configure:\n    id: configure\n    runs-on: ubuntu-latest\n    steps:\n      - name: Resolve Inputs\n        run: |\n          data_source=\"${{ github.event_name == 'push' && 'production' || inputs.data_source }}\"\n\n          ... other logic ...\nHere, if the workflow is triggered by a push event, it’ll use production as the data source. But if it’s triggered manually, it’ll use the value provided in the data_source input field.\nWe can also write this more concisely as:\ndata_source=\"${{ inputs.data_source || 'production' }}\"\nThis works because if inputs.data_source is not provided, it will be null and is evaluated to false. The logical OR operator (||) will then return the right-hand side value, which is production in this case. Some may argue this is less readable, so I suppose it’s a matter of preference. I personally prefer the second version.\n\n\n\n\n\n\nTipRun Name Customization\n\n\n\n\n\nManual workflow runs all share the same generic title (the workflow name) which can be confusing when multiple users trigger the same workflow but from different branches or with different inputs. You can customize the title using the run-name keyword for better visibility. I think the following line should be added to every workflow by default:\nrun-name: &gt;\n  ${{ github.event_name == 'workflow_dispatch'\n      && format('{0} from \"{1}\"', github.workflow, github.ref_name)\n      || '' }}\nThis will set the title to include the branch name, but only if the workflow is triggered manually. For any other trigger events (e.g. push), it’ll use Github’s default run name. You can customize the format string to include workflow inputs or other context variables as needed."
  },
  {
    "objectID": "blog/2025-08-07-github-workflow-tips/index.html#reusable-workflows",
    "href": "blog/2025-08-07-github-workflow-tips/index.html#reusable-workflows",
    "title": "Github Workflow Tips and Tricks",
    "section": "2 Reusable Workflows",
    "text": "2 Reusable Workflows\nReusable workflows are workflows that can be called from other workflows. They can accept inputs similar to the workflow_dispatch inputs, and can provide outputs that can be used in the calling workflow. This is useful for sharing common logic across multiple workflows, such as a release process that does testing, building and creating artifacts. With a good configuration setup, this workflow can be reused for PR testing, deployment, and even manual releases.\nThis is a minimal example with a single input and output:\nname: Reusable Workflow Example\non:\n  # `workflow_call` makes the workflow callable from other workflows\n  workflow_call:\n    inputs:\n      run_tests:\n        type: boolean\n        description: 'Run tests before building'\n        required: true\n    # Need to explicitly define the outputs here\n    outputs:\n      example_output:\n        description: 'An example output from the reusable workflow'\n        value: ${{ jobs.job1-example.outputs.example_output }} # Reference the output from the job\n  \n  # In most cases, it's also useful to manually trigger these type of workflows\n  # Unfortunately, we need to duplicate the input definitions here for the dispatch trigger\n  workflow_dispatch:\n    inputs:\n      run_tests:\n        type: boolean\n        description: 'Run tests before building'\n        required: true\njobs:\n  job1-example:\n    runs-on: ubuntu-latest\n    # Output of the job is defined here, so it can be referenced as an output of the workflow\n    outputs:\n      # The job output itself is defined in one of the steps (so many references, I know)\n      example_output: ${{ steps.example_step.outputs.hello_message }}\n    steps:\n      - name: Example Step\n        id: example_step\n        run: |\n          echo \"This is an example step\"\n          echo \"hello_message=Hello from reusable workflow!\" &gt;&gt; $GITHUB_OUTPUT\nTo call this workflow from another workflow, we can use the uses keyword and pass the inputs:\nname: Main Workflow\non:\n  push:\n    branches: [main]\n\njobs:\n  call-reusable-workflow:\n    uses: ./.github/workflows/example2_reusable_workflow.yaml # Path to the reusable workflow\n    with: # Specify the inputs (if any) (also, why is this not called `inputs`?!)\n      run_tests: true\n    secrets: inherit # Inherit secrets from the calling workflow if needed\n  \n  print-outputs:\n    needs: call-reusable-workflow\n    runs-on: ubuntu-latest\n    steps:\n      - name: Print the outputs\n        run: |\n          echo \"Output from reusable workflow: ${{ needs.call-reusable-workflow.outputs.example_output }}\""
  },
  {
    "objectID": "blog/2025-08-07-github-workflow-tips/index.html#github-environments",
    "href": "blog/2025-08-07-github-workflow-tips/index.html#github-environments",
    "title": "Github Workflow Tips and Tricks",
    "section": "3 Github Environments",
    "text": "3 Github Environments\nGithub Environments are a way to define different deployment environments within a repository and can be used to set environment-specific secrets, protection rules, etc.\n\nInstead of having repository-wide secrets, we can have environment-specific secrets. Most of the time you would want to use the same secret name across different environments, so you can access them using the same syntax in the workflow.\nThe syntax to access an environment secret is ${{ secrets.SECRET_NAME }}, which is no different from the repository secrets. However, the value of the secret is determined by the environment the job is running in at runtime.\nWe can also define environment “variables” in the same settings page where we define the secrets. The only difference between a variable and a secret is that the variable is not encrypted and you can see its value in the settings page. The syntax to access an environment variable is ${{ vars.VARIABLE_NAME }}.\n\nHere is an example of a job that deploys to an environment, where production is a predefined environment in the repo settings:\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment: production # Specify the environment here\n    steps:\n      - name: Deploy to Production\n        # This is just an example. Don't actually echo secrets in real workflows!\n        # But Github will most certainly mask the secrets in the logs\n        run: | \n          echo \"Deploying to production with secret ${{ secrets.TEST_SECRET }}\"\n          echo \"Environment variable XYZ: ${{ vars.XYZ }}\""
  },
  {
    "objectID": "blog/2025-08-07-github-workflow-tips/index.html#dynamic-parallel-jobs",
    "href": "blog/2025-08-07-github-workflow-tips/index.html#dynamic-parallel-jobs",
    "title": "Github Workflow Tips and Tricks",
    "section": "4 Dynamic Parallel Jobs",
    "text": "4 Dynamic Parallel Jobs\nMatrix strategy is a powerful feature to execute multiple jobs in parallel with different configurations, with use-cases like testing over multiple versions of a language or operating system, building multiple docker images, deploying to several environments, etc.\nThe examples you can find in the official documentation are usually simple, such as:\njob1:\n  # Define the matrix parameters\n  strategy:\n    matrix:\n      python-version: ['3.9', '3.10', '3.11']\n      os: [ubuntu-latest, windows-latest, macos-latest]\n      environment: [staging, production]\n  # We can access individual parameters using dot notation\n  name: Test on ${{ matrix.os }} with Python ${{ matrix.python-version }}\n  runs-on: ${{ matrix.os }}\n  environment: ${{ matrix.environment }}\n  steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-python@v5\n      with:\n        python-version: ${{ matrix.python-version }}\nThe way a matrix works is that it creates a job for every combination of the parameters specified, resulting in a total of 3 (python-version) x 3 (os) x 2 (environment) = 18 jobs. While this has its own applications, sometimes we may need to specify complex configuration for each individual job that is not symmetrically derived from the cartesian product of the parameters. One example would be dynamic construction of the matrix based on the user inputs, or the branch the workflow is triggered on, which brings me to the next point.\nTo create a dynamic matrix construction in a configuration step, we can create an array of JSON objects where each object represents the job configuration. Let’s consider an example where we want to deploy a service to different regions depending on the input parameters:\nname: Multi-Region Deployment\non:\n  push:\n    branches: [main]\n  \n  workflow_dispatch:\n    inputs:\n      include_eu:\n        type: boolean\n        description: 'Deploy to EU regions for GDPR compliance'\n        default: false\n      deployment_scope:\n        type: choice\n        description: 'Deployment scope'\n        default: 'regional'\n        options:\n          - regional\n          - global\n\njobs:\n  configure-regions:\n    runs-on: ubuntu-latest\n    outputs:\n      matrix: ${{ steps.configure.outputs.matrix }}\n    steps:\n      - name: Build Regional Matrix\n        id: configure\n        run: |\n          set -euxo pipefail\n          \n          # Base regions for all deployments\n          # The `regions` variable will be a JSON array of job configurations\n          regions=()\n          regions+=('{\"region\": \"us-east-1\", \"instance_type\": \"t3.medium\"}')\n          regions+=('{\"region\": \"us-west-2\", \"instance_type\": \"t3.medium\"}')\n          \n          # Add EU regions if needed\n          # Production always includes EU (when on main branch)\n          if [[ \"${{ inputs.include_eu }}\" == \"true\" || \\\n                \"${{ github.ref }}\" == \"refs/heads/main\" ]]; then\n            # Append EU region configs\n            regions+=('{\"region\": \"eu-west-1\", \"instance_type\": \"t3.large\", \"compliance\": \"gdpr\"}')\n            regions+=('{\"region\": \"eu-central-1\", \"instance_type\": \"t3.large\", \"compliance\": \"gdpr\"}')\n          fi\n          \n          # Add Asia-Pacific for global rollouts\n          if [[ \"${{ inputs.deployment_scope }}\" == \"global\" ]]; then\n            regions+=('{\"region\": \"ap-southeast-1\", \"instance_type\": \"t3.small\"}')\n          fi\n          \n          # Output as JSON array\n          echo \"matrix=$(IFS=,; echo \"[${regions[*]}]\")\" &gt;&gt; $GITHUB_OUTPUT\n\n  deploy-regions:\n    needs: configure-regions\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        # 'fromJSON' is a Github function to convert a JSON string to an object\n        service: ${{ fromJSON(needs.configure-regions.outputs.matrix) }}\n        environment: [development, staging] # This is fixed in this example\n\n    environment: ${{ matrix.environment }}\n    steps:\n      - name: Deploy to Region\n        run: |\n          echo \"Deploying to ${{ matrix.service.region }} with ${{ matrix.service.instance_type }}\"\n          \n          # EU regions need special compliance handling\n          if [[ \"${{ matrix.service.compliance }}\" == \"gdpr\" ]]; then\n            echo \"Applying GDPR compliance configurations\"\n          fi\nService configurations are dynamically constructed, while the environment array is fixed. You can go crazy with this logic and create any number of jobs with different configurations."
  },
  {
    "objectID": "blog/2025-08-07-github-workflow-tips/index.html#job-dependencies-and-skipping",
    "href": "blog/2025-08-07-github-workflow-tips/index.html#job-dependencies-and-skipping",
    "title": "Github Workflow Tips and Tricks",
    "section": "5 Job Dependencies and Skipping",
    "text": "5 Job Dependencies and Skipping\nJobs in a workflow can depend on each other, and by default, a job won’t run if its dependencies fail. However, sometimes we may want to intentionally skip a job based on a logic but without canceling the dependent jobs. Consider an example where Job C depends on Job B, which in turn depends on Job A (A -&gt; B -&gt; C). Job B may be skipped based on a condition in Job A, but we still want Job C to run in this scenario. The code below demonstrates how to achieve this:\njobs:\n  job_a:\n    runs-on: ubuntu-latest\n    outputs:\n      run_b: ${{ steps.configure.outputs.run_b }}\n    steps:  \n      - name: Configure\n        id: configure\n        run: |\n          # Check if branch is main\n          run_b=${{ github.ref == 'refs/heads/main' }}\n          echo \"run_b=$run_b\" &gt;&gt; $GITHUB_OUTPUT\n  job_b:\n    runs-on: ubuntu-latest\n    # Dependency on job A\n    needs: job_a\n    # `if` condition as to whether to run this job. Here, it's based on the output of job A\n    # Note that the output is a string, so we compare it to string 'true'\n    if: needs.job_a.outputs.run_b == 'true' \n    steps:\n      - run: echo \"This is job B, which runs only if job A's condition is met.\"\n  \n  job_c:\n    runs-on: ubuntu-latest\n    # Dependency on job B\n    needs: job_b\n    # Only run if job B didn't fail and wasn't cancelled\n    if: ${{ !failure() && !cancelled() }}\n    steps:\n      - run: echo \"This is job C, which runs only if job B was either successful or skipped.\"\nThe reason we use a complicated expression like ${{ !failure() && !cancelled() }} is because Github doesn’t provide a built-in way to check if a job was skipped. The failure() function returns true if a dependency job failed, and cancelled() returns true if the workflow was cancelled. By combining the inverse of these two conditions, we ensure that the job runs only if the previous job was either successful or skipped."
  },
  {
    "objectID": "blog/2025-08-07-github-workflow-tips/index.html#exfiltration-of-secrets",
    "href": "blog/2025-08-07-github-workflow-tips/index.html#exfiltration-of-secrets",
    "title": "Github Workflow Tips and Tricks",
    "section": "6 Exfiltration of Secrets",
    "text": "6 Exfiltration of Secrets\nRecently I wanted to take a backup of the secrets of one of the repositories I own. As you may know, the secrets cannot be viewed after they are created, and can only be updated or deleted. I was aware that the most straightforward way to do this was to use a workflow that prints out the secrets to the console, but I was surprised to learn that this is also a security vulnerability that can be exploited by malicious actors, given that it doesn’t require admin or owner-level permissions.\nAll you need to do is to be able to trigger any workflow, whether it’s through a PR workflow that is automatically triggered, or by manually running a workflow that has the workflow_dispatch trigger. But let’s leave the security discussion for another time. I just want to note that I take no responsibility for any incorrect or misuse of this information, and you should only use it for legitimate purposes such as backing up your own secrets or testing your workflows.\nMy preferred way to accomplish this is to modify an existing workflow that has the workflow_dispatch trigger in a non-main branch, encrypt the secrets in the workflow based on an input key and then print them to the console. The encryption is a safety measure to ensure that the secrets are not exposed in the logs, and can only be decrypted by the user who has access to the input key. We’ll use the Fernet encryption from the cryptography package to encrypt the secrets. You first need to generate a key locally using the following code:\nfrom cryptography.fernet import Fernet\nkey = Fernet.generate_key()\nprint(key.decode())\nThen you can use the following workflow to backup the secrets:\nname: Backup Secrets\non:\n  workflow_dispatch: \n    inputs:\n      key:\n        description: 'Encryption key for secrets backup'\n        required: true\njobs:\n  backup:\n    runs-on: ubuntu-latest\n    steps:\n      - run: |\n        pip install cryptography\n        python -c '\n        import os\n        import json\n\n        # Use Fernet encryption\n        from cryptography.fernet import Fernet\n        fernet = Fernet(os.environ[\"KEY\"].encode())\n        \n        # Convert the env variables to JSON and encrypt\n        params = json.dumps(dict(os.environ))\n        print(\"Backing up secrets...\")\n        print(fernet.encrypt(params.encode()).decode())\n        ' \n      env: \n        KEY: ${{ inputs.key }}\n        # Expose the secrets as environment variables\n        SECRET_1: ${{ secrets.SECRET_1 }}\n        SECRET_2: ${{ secrets.SECRET_2 }}\n        SECRET_3: ${{ secrets.SECRET_3 }}\nThis prints out the encrypted secrets to the console, which can then be decrypted using the same key by:\nfrom cryptography.fernet import Fernet\nfernet = Fernet('your-encryption-key')\nencrypted_data = 'your-encrypted-data' # from the workflow logs\ndecrypted_data = fernet.decrypt(encrypted_data.encode()).decode()\nprint(decrypted_data)\n\n\n\n\n\n\nWarningSecurity Warning\n\n\n\n\n\nThe reason that this is safe (as of writing this article) is because Github doesn’t expose the input parameters in the logs, so we can be confident the secrets cannot be decrypted by anyone having the ability to view the run logs. But this may change in the future so a safer way to accomplish the same outcome is to make the encryption key a secret as well, and then pass it on to the workflow as an environment variable.\njobs:\n  backup:\n    runs-on: ubuntu-latest\n    steps:\n      - run: |\n        ...\n      env: \n        KEY: ${{ secrets.ENCRYPTION_KEY }} # Use a secret for the encryption key\n        ..."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Musings from My Corner of the World",
    "section": "",
    "text": "Implementing Gradient Boosted Tree Algorithms from Scratch - LightGBM, XGBoost, CatBoost\n\n\n\nML\n\nAlgorithms\n\nFrom Scratch\n\nGBT\n\n\n\nBuilding a GBT library from scratch to understand what’s actually happening under the hood, and to have the flexibility to experiment with new ideas.\n\n\n\n\n\nDec 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGithub Workflow Tips and Tricks\n\n\n\nGitHub\n\nDevOps\n\nCI/CD\n\nAutomation\n\n\n\nA collection of useful tips and tricks for better leveraging Github workflows\n\n\n\n\n\nAug 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Review of ML Time Series Forecasting Models\n\n\n\nML\n\nTime Series\n\nForecasting\n\nLSTM\n\nN-BEATS\n\nTFT\n\nDeepAR\n\n\n\nA review of some of the well-known machine learning time series forecasting models.\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSaving Time: Post-mortem Debugging in Python\n\n\n\nPython\n\nDebugging\n\n\n\nA bit of Python black magic that lets you efficiently inspect and manipulate execution contexts after crashes, aka, post-mortem debugging.\n\n\n\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRunning Llama 3 in the Browser!\n\n\n\nNLP\n\nLLM\n\nWebLLM\n\nMLC-LLM\n\nWebGPU\n\nTVM\n\n\n\nChat with Llama and other LLMs that run locally in your browser using WebLLM.\n\n\n\n\n\nOct 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nInformation Theory Fundamentals: Entropy, Cross-Entropy, and KL Divergence\n\n\n\nML\n\nInformation Theory\n\n\n\nA guide to information theory covering entropy, cross-entropy, KL divergence, and Jensen-Shannon divergence with proofs, examples, and visualizations.\n\n\n\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCustom Loss Functions for LightGBM and CatBoost\n\n\n\nML\n\nLightGBM\n\nCatBoost\n\nGBT\n\n\n\nGuide on how to implement custom loss functions and evaluation metrics for LightGBM and CatBoost\n\n\n\n\n\nAug 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Multi-Layer LSTM and AdamW from Scratch using NumPy\n\n\n\nML\n\nNLP\n\nOptimization\n\nAnalysis\n\n\n\nThis tutorial walks through the implementation of a multi-layer LSTM model from scratch in pure NumPy, and trains it on the Shakespeare dataset. It also covers the implementation of the AdamW optimizer and the necessary data modules.\n\n\n\n\n\nJun 15, 2024\n\n\n\n\n\nNo matching items\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "project/2017-09-17-d3-force-graph-bio-networks/index.html",
    "href": "project/2017-09-17-d3-force-graph-bio-networks/index.html",
    "title": "D3 Force Layout - Biological Networks",
    "section": "",
    "text": "While working as a research assistant at the Computational Biology Research Center at the Amirkabir University of Technology, I was tasked with implmenting an algorithm for Identification of large disjoint motifs in biological networks. We soon realized we needed a visualization tool to help us better understand the networks we were working with, and the motifs that were identified. This was when I stumbled on the D3.js library, and decided to use its force layout for interactive visualization. The tool below is based on the countless examples and tutrials I found, which I’ve lost track of, and should give a glimpse of the power of D3.js.\nThis example is a visualization of protein-protein interaction network. Its main features are:\nThe source code can be found here. The visualization code is in the output folder."
  },
  {
    "objectID": "project/2017-09-17-d3-force-graph-bio-networks/index.html#network-details",
    "href": "project/2017-09-17-d3-force-graph-bio-networks/index.html#network-details",
    "title": "D3 Force Layout - Biological Networks",
    "section": "Network Details",
    "text": "Network Details\nOrder: \nSize: \nComponents:"
  },
  {
    "objectID": "project/2017-09-17-d3-force-graph-bio-networks/index.html#pattern-details",
    "href": "project/2017-09-17-d3-force-graph-bio-networks/index.html#pattern-details",
    "title": "D3 Force Layout - Biological Networks",
    "section": "Pattern Details",
    "text": "Pattern Details\nOrder: \nSize: \nFrequency:"
  },
  {
    "objectID": "project/2017-09-17-d3-force-graph-bio-networks/index.html#settings",
    "href": "project/2017-09-17-d3-force-graph-bio-networks/index.html#settings",
    "title": "D3 Force Layout - Biological Networks",
    "section": "Settings",
    "text": "Settings\n\nNode Charge  \n\n\nLink Strength  \n\n\nNode Radius  \n\n\nLink Width  \n\n\nVelocity Decay  \n\n\nCenter Gravity  \n\n   Force Static   FishEye Distortion Zoom Function Pin on Drop"
  },
  {
    "objectID": "project/2017-09-17-d3-force-graph-bio-networks/index.html#infoTitle",
    "href": "project/2017-09-17-d3-force-graph-bio-networks/index.html#infoTitle",
    "title": "D3 Force Layout - Biological Networks",
    "section": "About",
    "text": "About"
  },
  {
    "objectID": "project/2024-04-12-custodium/index.html",
    "href": "project/2024-04-12-custodium/index.html",
    "title": "Custodium",
    "section": "",
    "text": "Custodium is a Python package for tracking investment portfolios and calculating adjusted cost basis (ACB) for Canadian capital gain/loss calculations. Primarily designed for educational purposes, so please read the source code and understand the logic before using it for real-world applications and having fun with it!"
  },
  {
    "objectID": "project/2024-04-12-custodium/index.html#key-features",
    "href": "project/2024-04-12-custodium/index.html#key-features",
    "title": "Custodium",
    "section": "1 Key Features",
    "text": "1 Key Features\n\nTrack investment transactions across multiple currencies with automatic exchange rate handling\nCalculate and maintain adjusted cost basis (ACB) over time\nProcess special transaction types (e.g., equity vesting)\nGenerate capital gains reports for tax purposes"
  },
  {
    "objectID": "project/2024-04-12-custodium/index.html#installation",
    "href": "project/2024-04-12-custodium/index.html#installation",
    "title": "Custodium",
    "section": "2 Installation",
    "text": "2 Installation\npip install custodium"
  },
  {
    "objectID": "project/2024-04-12-custodium/index.html#usage",
    "href": "project/2024-04-12-custodium/index.html#usage",
    "title": "Custodium",
    "section": "3 Usage",
    "text": "3 Usage\n\nProcessing Transactions\n\nCreate a CSV file (e.g. my_transactions.csv) with these columns:\n\ndate,description,base_currency,quote_currency,quantity,price,fees,note\n2023-01-15,Buy AAPL shares,AAPL,USD,10,150.25,9.95,Initial purchase\n2023-03-10,Buy MSFT shares,MSFT,USD,15,280.45,9.95,Portfolio diversification\n2023-06-30,Sell AAPL shares,AAPL,USD,-5,175.50,9.95,Partial profit taking\n2023-08-15,Convert USD to CAD,CAD,USD,1000,0.73,5.00,Currency repatriation\n\nThen load and process:\n\nfrom decimal import Decimal\nfrom custodium.portfolio import Asset, Holdings\nfrom custodium.processing import load_transactions, process_transactions\nfrom custodium.reporting import calculate_yearly_gains, plot_holdings_history\nfrom custodium.utils import displayPandas\n\n# Load the transactions from CSV\ntransactions, df_log = load_transactions(\"my_transactions.csv\")\n\n# Create the portfolio\nholdings = Holdings()\nreporting_currency = \"CAD\" # Canadian Dollar\n\n# Create initial holdings\nholdings = Holdings()\nholdings.add(\n    Asset(\n        date=\"2023-01-01\",\n        asset=\"CAD\",\n        quantity=Decimal(\"10000\"),\n        acb=Decimal(\"1\"), # Must be 1 for the reporting currency\n    )\n)\nholdings.add(\n    Asset(\n        date=\"2023-01-01\",\n        asset=\"USD\",\n        quantity=Decimal(\"10000\"),\n        acb=Decimal(\"1.35\"),\n    )\n)\n\n# Process the transactions\nholdings, gains = process_transactions(\n    transactions,\n    holdings=holdings,\n    reporting_currency=reporting_currency,\n)\n\n# View current holdings with readable formatting\ndisplayPandas(holdings.current, precision=2)\n\n# Calculate and display the yearly gains/losses\nyearly_gains = calculate_yearly_gains(gains)\ndisplayPandas(yearly_gains, precision=5)\n\n# Plot the history of holdings (Ploty is required)\nfig = plot_holdings_history(holdings, acb_title=\"ACB $CAD\")\nfig.show()\n\nSample output:"
  },
  {
    "objectID": "project/2024-04-12-custodium/index.html#code-structure",
    "href": "project/2024-04-12-custodium/index.html#code-structure",
    "title": "Custodium",
    "section": "4 Code Structure",
    "text": "4 Code Structure\n├── custodium\n│   ├── exchange.py     # Currency exchange rate management\n│   ├── portfolio.py    # Core data structures (Transaction, Asset, Holdings)\n│   ├── processing.py   # Transaction processing logic and CSV loading\n│   ├── reporting.py    # Capital gains reporting and visualization tools\n│   ├── utils.py        # Utility functions\n│   └── __init__.py     # Package version and imports\n├── requirements        # Package dependencies\n├── tests               # Unit tests for package components\n├── .gitignore\n├── .pre-commit-config.yaml  # Pre-commit hooks configuration\n├── LICENSE\n├── Makefile                 # Build and test automation\n├── pyproject.toml           # CI/CD configuration\n├── README.md\n└── setup.py                 # Package installation configuration"
  },
  {
    "objectID": "project/2024-04-12-custodium/index.html#disclaimer",
    "href": "project/2024-04-12-custodium/index.html#disclaimer",
    "title": "Custodium",
    "section": "5 Disclaimer",
    "text": "5 Disclaimer\nThis package is provided for educational purposes only. Users should not rely on this software for financial advice, tax reporting, or investment decisions without independent verification. The authors accept no responsibility for any financial losses, tax implications, or other issues that may arise from the use of this software."
  },
  {
    "objectID": "project/2024-04-12-custodium/index.html#license",
    "href": "project/2024-04-12-custodium/index.html#license",
    "title": "Custodium",
    "section": "6 LICENSE",
    "text": "6 LICENSE\nThis project is licensed under the MIT License. See the LICENSE file for details."
  },
  {
    "objectID": "project/2025-05-16-pymortem/index.html",
    "href": "project/2025-05-16-pymortem/index.html",
    "title": "Pymortem",
    "section": "",
    "text": "Pymortem is a post-mortem debugging tool that lets you inspect and manipulate execution contexts after exceptions occur. Unlike traditional debuggers that require a separate interactive shell, pymortem gives you direct access to all variables and frames in the exception stack, making it valuable in Jupyter notebooks and interactive environments."
  },
  {
    "objectID": "project/2025-05-16-pymortem/index.html#installation",
    "href": "project/2025-05-16-pymortem/index.html#installation",
    "title": "Pymortem",
    "section": "1 Installation",
    "text": "1 Installation\npip install pymortem"
  },
  {
    "objectID": "project/2025-05-16-pymortem/index.html#features",
    "href": "project/2025-05-16-pymortem/index.html#features",
    "title": "Pymortem",
    "section": "2 Features",
    "text": "2 Features\n\nEnhanced Tracebacks: Rich, visual traceback output showing code context around errors with line numbers and error indicators\nFrame Inspection: Directly examine variables at any level in the call stack without navigating through a separate command interface\nCode Execution in Context: Run arbitrary code in the context of any stack frame without restarting your program\nChained Exception Support: Clear visualization of exception chains, showing both “raised from” and “during handling” relationships\nNo Special Setup: Works with standard Python without requiring breakpoints or special execution modes"
  },
  {
    "objectID": "project/2025-05-16-pymortem/index.html#usage",
    "href": "project/2025-05-16-pymortem/index.html#usage",
    "title": "Pymortem",
    "section": "3 Usage",
    "text": "3 Usage\n\nExamining an Exception after it Occurs\n# In one cell where an error happens:\ndef foo():\n    x = 10\n    output = x / 0\n    return output\n\nfoo()\n# In the next cell, examine the exception:\nimport pymortem\n\n# Get enhanced traceback and frame information\ntraceback_msg, frames = pymortem.extract_from_exception()\n\n# Display the improved traceback\nprint(traceback_msg)\n\n\nInspecting Variables in the Error Context\n# After running the above cells\n# Let's examine the local variables in different frames\n\n# The frame where the error occurred\nprint(\"Locals in error frame:\", frames[-1][\"locals\"])\n\n# Check global variables too\nprint(\"Some globals:\", {k: v for k, v in list(frames[-1][\"globals\"].items())[:5]})\n\n\nExecuting Code in a Frame’s Context\nimport pymortem\nimport sys\n\n# Get the most recent exception\nexception = pymortem.retrieve_the_last_exception() # Store the exception\n_, frames = pymortem.extract_from_exception(exception)\n\n# Choose a frame to work with (e.g., frames[1] for a specific frame)\nwork_frame = frames[-1]\n\n# Execute code in that frame's context\npymortem.execute(\n    \"\"\"\n    # You can access all variables that existed when the error occurred\n    print(\"Available variables:\", list(locals().keys()))\n\n    # Test potential fixes without rerunning the entire notebook\n    try:\n        # Try a fix for a ZeroDivisionError\n        denominator = 2  # Was 0 before\n        fixed_result = x / denominator\n        print(f\"Fix worked! Result = {fixed_result}\")\n    except Exception as e:\n        print(f\"Fix didn't work: {e}\")\n    \"\"\",\n    work_frame\n)\n\n\nHandling Chained Exceptions\n# Create a chained exception scenario\ntry:\n    try:\n        x = {\"key\": \"value\"}\n        result = x[\"missing_key\"]  # Will raise KeyError\n    except KeyError:\n        result = 10 + \"0\"  # Will raise ValueError\nexcept Exception as e:\n    chain_exception = e\n\n# Examine the exception chain\ntraceback_msg, all_frames = pymortem.extract_from_exception(chain_exception)\nprint(traceback_msg)\nprint(\"\")\n\n# Frames are arranged in chronological order, with the first exception first\noriginal_error_frame = all_frames[0]  # Frame from the KeyError\nraised_from_frame = all_frames[-1]    # Frame from the ValueError\n\nprint(f\"First exception type: {type(chain_exception.__cause__)}\")\nprint(f\"Second exception type: {type(chain_exception)}\")"
  },
  {
    "objectID": "project/2025-05-16-pymortem/index.html#why-use-pymortem",
    "href": "project/2025-05-16-pymortem/index.html#why-use-pymortem",
    "title": "Pymortem",
    "section": "4 Why Use Pymortem?",
    "text": "4 Why Use Pymortem?\nPost-mortem debugging in Python traditionally requires using tools like pdb.pm() or %debug in IPython, which launch a separate command interface with its own syntax and navigation model. Pymortem takes a different approach:\n\nDirect Context Access: Instead of a separate debugging shell, access frame data directly in your current Python environment\nBetter Visualization: See more context around exceptions with cleaner, more informative tracebacks\nNatural Code Execution: Run diagnostic code directly in frame contexts using standard Python syntax\nStays in Flow: Particularly valuable in notebooks where switching to a separate debugging interface breaks your workflow\nHandles Complexity: Elegantly deals with nested and chained exceptions that can be confusing in traditional debuggers"
  },
  {
    "objectID": "project/2025-05-16-pymortem/index.html#license",
    "href": "project/2025-05-16-pymortem/index.html#license",
    "title": "Pymortem",
    "section": "5 License",
    "text": "5 License\nMIT"
  },
  {
    "objectID": "project/2025-09-28-tts/index.html",
    "href": "project/2025-09-28-tts/index.html",
    "title": "Voice Cloning in Browser with F5-TTS",
    "section": "",
    "text": "This app is a demo of F5-TTS model that lets you clone any voice in your browser. No servers, no uploads, just pure client-side execution. You can feed a short audio of someone talking, and it will synthesize new speech in their voice saying whatever text you want. There’s also a podcast mode that lets you create multi-speaker conversation easily.\nOnce the models are downloaded, you are free to disconnect and work offline since everything runs locally and your audio never leaves your computer.\nTip: Use the “Load Demo” button to pre-fill the app with sample data to get started quickly.\n\n\n\n\n\n\nNoteUpdate: Nov 13, 2025\n\n\n\nThe source code for this app is now available on nsarang/voice-cloning-f5-tts."
  },
  {
    "objectID": "publication/2018-01-03-protein-design/index.html#abstract",
    "href": "publication/2018-01-03-protein-design/index.html#abstract",
    "title": "Protein design using native secondary sub-structures and solvent accessibility",
    "section": "Abstract",
    "text": "Abstract\nAccording to structure-dependent function of proteins, two main challenging problems called Protein Structure Prediction (PSP) and Inverse Protein Folding (IPF) are investigated. In spite of IPF essential applications, it has not been studied as much as PSP problem. In fact, the ultimate goal of IPF problem or protein design is to create proteins with enhanced properties or even novel functions. One of the major computational challenges in protein design is large protein sequence space, namely searching through all plausible sequences is impossible. In our previous research, we introduced a genetic algorithm called GAPSSIF for designing protein secondary structure. This algorithm benefits from evolutionary information obtained by solved protein structures in PDB. Therefore, we constructed a repository of protein secondary sub-structures to accelerate convergence of the algorithm. The secondary structures of designed sequences by GAPSSIF are comparable with those obtained by Evolver and EvoDesign. In this paper, we modify GAPSSIF so it considers solvent accessibility. Therefore, the simple fitness function of GAPSSIF is improved by a multi-featured one to search through the sequence space more precisely."
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Publications",
    "section": "",
    "text": "Tractable large-scale deep reinforcement learning\n\n\n\nReinforcement Learning\n\nComputer Vision\n\nSattelite Imagery\n\n\n\n\n\n\nJul 1, 2023\n\n\nNima Sarang, Charalambos Poullis\n\n\n\n\n\n\n\nProtein design using native secondary sub-structures and solvent accessibility\n\n\n\nResearch\n\nProtein Design\n\nGenetic Algorithm\n\nSolvent Accessibility\n\n\n\n\n\n\nJan 3, 2018\n\n\nFatemeh Zare-Mirakabad, Marziyeh Movahedi, Nima Sarang, S. Shahriar Arab\n\n\n\n\n\nNo matching items"
  }
]