[
  {
    "objectID": "blog/2024-08-24-information-theory/index.html",
    "href": "blog/2024-08-24-information-theory/index.html",
    "title": "My Notes on Information Theory",
    "section": "",
    "text": "% Meta %\n% Optional argument [#1]: Size modifier (e.g., , ) % #2: Opening delimiter % #3: Closing delimiter % #4: Content\n% Common sets % Real numbers % Integers % Natural numbers % Rational numbers % Complex numbers\n% Probability and statistics % Expectation % Variance % Covariance % Probability measure % Indicator function\n% Linear algebra\n% Matrix\n% Vector % Trace % Rank % Range (image) % Projection\n% Calculus and analysis % For integrals, e.g., f(x) x\n% Partial derivative \\newcommand{[2]}{ #1} % Partial derivative w/o fraction\n% Second partial derivative % Gradient % Divergence % Curl\n% Set theory\n% Set\n% Set builder notation % Union % Intersection % Symmetric difference\n% Logic and proofs % Implies % If and only if % End of proof % Contradiction\n% Norms and inner products\n% Norm\n% Inner product\n% Common functions % Minimization problem % Maximization problem % Argument minimum % Argument maximum\n% Subject to constraints % Sign function % Span of a set\n% Formatting\n% Absolute value\n% Parentheses\n% Brackets\n% Floor function\n% Ceiling function\n% Asymptotic notations % Big O notation % Small o notation % Big Omega notation % Big Theta notation\n% Commonly used in algorithms and complexity % Polynomial time % Polylogarithmic time\n% Additional probability notations % Independent and identically distributed % Distributed as\n% Fourier transform % Fourier transform % Inverse Fourier transform\n% General math % Display style"
  },
  {
    "objectID": "blog/2024-08-24-information-theory/index.html#introduction",
    "href": "blog/2024-08-24-information-theory/index.html#introduction",
    "title": "My Notes on Information Theory",
    "section": "1 Introduction",
    "text": "1 Introduction\nEvery now and then, I find myself revisiting the basics of Information Theory when reading on papers about VAEs, GANs, or other generative models. I decided to compile a brief overview of the concepts for my own reference. Take it with a grain of salt, as it‚Äôs not meant to be a comprehensive guide.\nI‚Äôll update this post as I learn more about the topic, so stay tuned!"
  },
  {
    "objectID": "blog/2024-08-24-information-theory/index.html#entropy",
    "href": "blog/2024-08-24-information-theory/index.html#entropy",
    "title": "My Notes on Information Theory",
    "section": "2 Entropy",
    "text": "2 Entropy\nEntropy is a fundamental concept in information theory that quantifies the uncertainty or randomness in a system. Let‚Äôs consider a couple of examples:\nImagine you‚Äôre trying to guess the next card drawn from a deck. With a well-shuffled deck, each draw is unpredictable ‚Äì this scenario has high entropy. On the other hand, if the cards were arranged in a known order, each draw would be predictable ‚Äì this has low entropy.\nNow, let‚Äôs think about weather prediction. In a climate where it rains 90% of the days, predicting rain is usually correct but not very informative. This scenario has lower entropy than a climate where rain and sun are equally likely (50% each), making predictions more challenging and uncertain.\n\n2.1 Information Content\nWe quantify the uncertainty or information content of an event \\(x\\) with probability \\(p(x)\\) as:\n\\[I(x) = -\\log_2{p(x)}\\]\nIt measures the surprise or ‚Äúnews value‚Äù of observing event \\(x\\). \\(I(x)\\) is also reffered to as the number of bits needed to encode the event, but we‚Äôll get to that later.\nWhy use \\(-\\log p\\)? Let‚Äôs visualize it:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis function captures several intuitive properties:\n\nRare events (low \\(p\\)) have higher information content, as \\(\\displaystyle{\\lim_{p \\to 0^+} (-\\log{p}) = +\\infty}.\\)\nCertain events (\\(p = 1\\)) have zero information content.\nThe negative sign ensures positive information values.\n\n\n\n2.2 Shannon Entropy\nShannon entropy is the expected value (weighted average) of information content across all possible events in a probability distribution:\n\\[H(X) = \\mathbb{E}[-\\log p(X)] = -\\sum_x p(x) \\log{p(x)} \\tag{1}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf \\(p(x) = 0\\), we define \\(0 \\log{0} = 0\\) since \\(\\displaystyle{\\lim_{p \\to 0^+} p\\log{p} = 0}\\) and can be proven using L‚ÄôH√¥pital‚Äôs rule.\n\n\n\nTo put this into perspective, English text typically has an entropy of about 1-1.5 bits per character. In comparison, a random string of characters over the same alphabet has an entropy of \\[\\mathbb{E}[-\\log_2{p(X)}] = -\\sum\\limits_{i=1}^{26} \\frac{1}{26} \\log_2(\\frac{1}{26}) = \\log_2(26) \\approx 4.7\\] bits per character.\nThis concept has practical applications in various fields. For instance, in cybersecurity, a truly random string of characters (like a strong password) approaches the maximum entropy of \\(\\log_2(n)\\) bits per character, where \\(n\\) is the number of possible characters. The password ‚Äúpassword123‚Äù has low entropy and is easily guessable, while ‚ÄúTr0ub4dor&Co‚Äù has higher entropy and is more secure. This difference in entropy reflects the varying degrees of unpredictability and, consequently, the strength of these passwords.\n\n\n2.3 A bit of history\nThe terms ‚Äúcoding‚Äù and ‚Äúencoded‚Äù in information theory stem from Claude Shannon‚Äôs work on efficient information transmission over noisy channels. Entropy \\(H(X)\\) represents the theoretical lower bound on the average number of bits needed to encode symbols from a source, which is why we measure it in bits (using log base 2) - it directly relates to optimal coding length.\nShannon‚Äôs Source Coding Theorem formalizes this: no lossless compression can exceed this efficiency on average. This explains why English text (~1.5 bits of entropy per character) compresses far more than random text (~4.7 bits per character), and why compression algorithms excel with natural language but struggle with random data. Understanding entropy as a limit on coding efficiency makes it a powerful tool across various domains like data compression, cryptography, and machine learning."
  },
  {
    "objectID": "blog/2024-08-24-information-theory/index.html#cross-entropy",
    "href": "blog/2024-08-24-information-theory/index.html#cross-entropy",
    "title": "My Notes on Information Theory",
    "section": "3 Cross Entropy",
    "text": "3 Cross Entropy\nImagine you have a source that produces messages according to a true distribution \\(P\\), but you‚Äôre using a different distribution \\(Q\\) to encode these messages. Cross-entropy measures the average number of bits you‚Äôll need using this possibly inefficient encoding.\nFor example, consider describing a language you‚Äôve never seen before. You start by guessing how often each letter appears.\n\nThe true frequency (\\(P\\)) of letters in this language is: \\(A: 40\\%, \\; B: 40\\%, \\; C: 20\\%\\)\nYour initial guess (\\(Q\\)) is: \\(A: 60\\%, \\; B: 30\\%, \\; C: 10\\%\\)\n\nNow, let‚Äôs use your guess to ‚Äúencode‚Äù messages in this language, i.e., calculate its uncertainty:\n\nFor each ‚ÄòA‚Äô, we use \\(-\\log_2{0.6} \\approx 0.74\\) bits\nFor each ‚ÄòB‚Äô, we use \\(-\\log_2{0.3} \\approx 1.74\\) bits\nFor each ‚ÄòC‚Äô, we use \\(-\\log_2{0.1} \\approx 3.32\\) bits\n\nTo calculate the average number of bits per letter, we use \\(P\\)‚Äôs frequencies because that‚Äôs how often each letter actually appears, while \\(Q\\) determines the cost for each letter. This combination tells us the real-world performance of our encoding scheme.\n\\[ 0.4 \\times 0.74 + 0.4 \\times 1.74 + 0.2 \\times 3.32 \\approx 1.57 \\text{ bits per letter}\\]\nThe resulting average, \\(H(P,Q) \\approx 1.57\\) bits, represents the typical encoding cost per letter when our guess (\\(Q\\)) is used to encode messages that actually follow the true distribution (\\(P\\)).\nThis approach mirrors how we evaluate predictions in ML: Our model makes guesses (Q), but the world behaves according to the true probabilities (P). Cross-entropy quantifies the average encoding length under this scenario.\nFormal Definition:\nThe cross-entropy between two probability distributions \\(p\\) and \\(q\\) over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme used for the set is optimized for an estimated probability distribution \\(q\\), rather than the true distribution \\(p\\).\n\\[H(P, Q) = -\\sum\\limits_{x \\in X} p(x)\\log{q(x)} = \\mathbb{E}_{x \\sim P}[-\\log{q(x)}] \\tag{2}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf \\(p(x) &gt; 0\\) and \\(q(x) = 0\\) for some \\(x\\), then \\(p(x)\\log{q(x)}\\) is undefined. To address this we can add a small epsilon to \\(q(x)\\) to avoid \\(\\log{0}\\). In fact, this is a common practice in ML libraries.\n\n\n\nBinary Cross-Entropy:\n\nIn binary classification, we deal with two classes, and the predicted probability for one class inherently determines the probability for the other. If \\(q\\) is the predicted probability of the positive class (class 1), then \\(1-q\\) is the probability of the negative class (class 0). The binary cross-entropy loss combines the log-likelihood terms for both possible classes (positive and negative) into a single expression. For a single data point, it is defined as:\n\\[-\\left[y\\,log(q)\\;+\\;(1‚àíy)\\,log(1‚àíq)\\right]\\]"
  },
  {
    "objectID": "blog/2024-08-24-information-theory/index.html#kullbackleibler-divergence",
    "href": "blog/2024-08-24-information-theory/index.html#kullbackleibler-divergence",
    "title": "My Notes on Information Theory",
    "section": "4 Kullback‚ÄìLeibler Divergence",
    "text": "4 Kullback‚ÄìLeibler Divergence\nTo measure the expected number of extra bits required to code samples from \\(P\\) using a code optimized for \\(Q\\) rather than the code optimized for \\(P\\), we use the Kullback-Leibler divergence. It\n\\[\n\\begin{aligned}\nD_{KL}(P \\;\\Vert\\; Q) &= H(P, Q) - H(P) && \\hspace{-2em}\\text{\\small{(extra bits to code $P$ using $Q$)}} \\\\[3ex]\n&= \\mathbb{E}_{x \\sim P}[-\\log q(x)] - \\mathbb{E}_{x \\sim P}[-\\log p(x)] \\\\[3ex]\n&= -\\sum\\limits_{x \\in X} p(x)\\log{q(x)} + \\sum\\limits_{x \\in X} p(x)\\log{p(x)} \\\\[1ex]\n&= \\sum\\limits_{x \\in X} p(x)\\log{\\frac{p(x)}{q(x)}} && \\hspace{-2em}\\text{\\small{(as $\\log{a} - \\log{b} = \\log{\\frac{a}{b}}$)}}\n\\end{aligned}\n\\tag{3}\\]\nThe relative entropy \\({D_{\\text{KL}}(P\\parallel Q)}\\) quantifies how far the distribution \\(Q\\) is from the distribution \\(P\\). The cross-entropy alone cannot be thought of as a distance, since \\({H(P,P)=:H(P)}\\) isn‚Äôt zero. This can be fixed by subtracting \\({H(P)}\\) to make \\({D_{\\text{KL}}(P\\parallel Q)}\\) agree more closely with our notion of distance, as the excess loss.\n\n\n\n\n\n\nNote\n\n\n\n\n\nKL divergence is asymmetric, i.e.¬†\\({D_{\\text{KL}}(P\\parallel Q)\\neq D_{\\text{KL}}(Q\\parallel P)}\\) except when \\(P = Q\\).\n\n\n\n\n4.1 Non-Negativity\nIf \\(Q = P\\), then \\(D_{KL}(P \\;\\Vert\\; P) = H(P, P) - H(P) = 0\\). But is zero the lower bound for \\(D_{KL}\\)? Basically the question is whether: \\[H(P, Q) \\stackrel{?}{\\geq} H(P) \\quad \\forall P, Q\\]\nThis is known as Gibbs‚Äô inequality, and it‚Äôs a fundamental property of the Kullback-Leibler divergence. It tells us that if you try to use a probability distribution \\(Q\\) to encode data that actually follows a different distribution \\(P\\), you will always need more bits on average, compared to using \\(P\\) itself.\nTo prove \\(D_{KL}(P \\;\\Vert\\; Q) \\stackrel{?}{\\ge} 0\\), we can use the fact that \\(-\\log\\) is a convex function (see Figure¬†1). By Jensen‚Äôs inequality, we have:\n\\[\n\\begin{aligned}\n\\sum\\limits_{x} p(x)\\log{\\frac{p(x)}{q(x)}} &= \\sum\\limits_{x} p(x)\\times -\\log{\\frac{q(x)}{p(x)}} \\\\[2ex]\n&\\ge -\\log\\left(\\sum_{x} p(x) {\\frac{q(x)}{p(x)}}\\right) && \\text{\\small{(Jensen's inequality)}} \\\\[2ex]\n&= -\\log\\left(\\sum_{x} q(x)\\right) = -\\log {1} && \\text{\\small{(sum of probabilities = 1)}} \\\\[2ex]\n& = 0\n\\end{aligned}\n\\] \\[ \\therefore \\quad D_{KL}(P \\;\\Vert\\; Q) \\ge 0\\]\n\n\n\n\n\n\nJensen‚Äôs Inequality\n\n\n\n\n\nJensen‚Äôs inequality states that for a convex function \\(f\\) and a random variable \\(X\\):\n\\[f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)]\\]\nSimilarly, for a concave function \\(f\\): \\[f(\\mathbb{E}[X]) \\geq \\mathbb{E}[f(X)]\\]\n\n\n\n\n\n4.2 At Zero Probabilities\nLet‚Äôs consider the cases where \\(p(x) = 0\\) or \\(q(x) = 0\\), because it‚Äôll be useful later on:\n\n\\(p(ùëã)=0 \\text{ and } q(ùëã)\\gt 0\\):\nAs a result, \\(\\displaystyle\\lim_{p \\to 0} p\\log{p} = 0\\). This makes sense since we sample according to distribution \\(p\\), we‚Äôll never sample event \\(x\\). Hence, it does not weight in \\({D_{KL}(P \\;\\Vert\\; Q)}\\), meaning \\(Q\\) can make mistakes about events that \\(P\\) considers impossible without penalty.\n\\(p(ùëã)\\gt 0 \\text{ and } q(ùëã)= 0\\):\nThen¬†\\(\\displaystyle\\lim_{q \\to 0^+} \\log{\\frac{1}{q}} = +\\infty\\). An infinite divergence effectively signals that the model \\(Q\\) fails to account for an event \\(x\\) that is observed under \\(P\\).\n\\(p(ùëã)=0 \\text{ and } q(ùëã)= 0\\): Then it‚Äôs really undefined :)\n\n\n\n4.3 Reverse KL vs Forward KL\nThis is something that I had never thought about before, because I had only seen KL divergence in the context of variational inference. But it turns out that there are two ways to use KL divergence in an optimization problem. The practical difference between the two arises when the model cannot perfectly fit the true distribution, which is most often the case. This is where the two divergences diverge (pun intended).\nLet‚Äôs say we have a model \\(Q\\) that we want to optimize to approximate the true distribution \\(P\\). It‚Äôd be fair assumption that \\(P\\) would be a more complex distribution than \\(Q\\), with more modes and a more complex structure, since \\(Q\\) is a simpler model. As example, \\(P\\) could be a Gaussian mixture model with 10 components (multimodal), while \\(Q\\) is a single Gaussian (unimodal).\nWe had previously established that \\(D_{KL}\\) is asymmetric. This means that minimizing \\(D_{KL}(P \\;\\Vert\\; Q)\\) is not the same as minimizing \\(D_{KL}(Q \\;\\Vert\\; P)\\). The former is known as the Forward KL and the latter is known as the Reverse KL. Let‚Äôs look into each of them.\n\nForward KL\n\\(D_{KL}(P \\;\\Vert\\; Q)\\) measures the extra bits needed to encode samples from \\(P\\) using a code optimized for \\(Q\\). By minimizing this, we‚Äôre trying to make \\(Q\\) as close as possible to \\(P\\):\n\\[ \\begin{aligned}\n\\mathop{\\mathrm{arg\\,min}}\\limits_Q\\; D_{KL}(P \\;\\Vert\\; Q) &= \\mathop{\\mathrm{arg\\,min}}\\limits_Q\\; H(P, Q) - H(P) \\\\[1ex]\n&= \\mathop{\\mathrm{arg\\,min}}\\limits_Q\\; H(P, Q) && \\text{\\small{(since $H(P)$ is constant w.r.t. $Q$)}} \\\\[1ex]\n&= \\mathop{\\mathrm{arg\\,min}}\\limits_Q\\; -\\sum\\limits_{x \\in X} p(x)\\log{q(x)}\n\\end{aligned} \\]\nSo basically minimizing the forward KL is equivalent to maximizing the cross-entropy between \\(P\\) and \\(Q\\), a common objective in supervised classification problems. As I previously mentioned in Section¬†4.2, this loss ladnscape is interesting around zero probabilities:\n\nIf \\(p(x) = 0\\) then \\(q(x)\\) can be anything, because the loss is zero. The con of this is that it can lead to overconfident predictions in regions with little data.\n\nIf \\(P(x) \\gt 0\\) and \\(Q(x) \\le \\epsilon\\), then the loss becomes significanly large. As a consequence, \\(Q\\) would rather assign a low probability to events that are very unlikely under \\(P\\) than missing them entirely.\n\nIn the interactive plot below you can see how the forward KL divergence changes as you adjust the mean and variance of the Gaussian distribution \\(Q\\).\n\n\nd3 = require(\"d3\")\nPlot = require(\"@observablehq/plot\")\nimport {slider} from '@jashkenas/inputs'\n\n// Cell 2: Input sliders\nviewof mean = slider({\n  min: -3, \n  max: 3, \n  step: 0.1, \n  value: -1.4, \n  title: \"Mean\", \n})\nviewof variance = slider({\n  min: 0.5, \n  max: 3, \n  step: 0.1, \n  value: 1.5, \n  title: \"Variance\",\n})\n\n// Static P distribution: Gaussian Mixture Model\nP = generateDataFromGMM([\n  {mean: -2.5, variance: 0.3, weight: 0.4},\n  {mean: 2, variance: 0.5, weight: 0.6}\n]);\n\n// Cell 3: Gaussian mixture function, data generation, and KL divergence calculation\nfunction gaussian(x, mean, variance) {\n  return (1 / Math.sqrt(2 * Math.PI * variance)) * Math.exp(-Math.pow(x - mean, 2) / (2 * variance));\n}\n\n// Define Gaussian Mixture Model for P\nfunction gaussianMixture(x, components) {\n  // components is an array of objects with {mean, variance, weight}\n  return components.reduce((sum, {mean, variance, weight}) =&gt; {\n    return sum + weight * gaussian(x, mean, variance);\n  }, 0);\n}\n\nfunction generateDataFromGMM(components) {\n  return d3.range(-5, 5, 0.1).map(x =&gt; ({\n    x: x,\n    y: gaussianMixture(x, components)\n  }));\n}\n\n// Generate data for Q (simple Gaussian)\nfunction generateData(mean, variance) {\n  return d3.range(-5, 5, 0.1).map(x =&gt; ({\n    x: x,\n    y: gaussian(x, mean, variance)\n  }));\n}\n\n// Calculate KL divergence\nfunction calculateKLDivergence(P, Q) {\n  return P.reduce((sum, p, i) =&gt; {\n    const q = Q[i];\n    if (p.y &gt; 0 && q.y &gt; 0) {\n      return sum + p.y * Math.log(p.y / q.y);\n    }\n    return sum;\n  }, 0);\n}\n\n// Cell 4: Calculate KL divergence for contour plot\nfunction calculateKLDivergenceGrid(dist1, meanRange, varianceRange, forward=true) {\n   const grid = [];\n   let kl;\n   for (let m of meanRange) {\n      for (let v of varianceRange) {\n        const dist2 = generateData(m, v);\n        if (forward) {\n            kl = calculateKLDivergence(dist1, dist2);\n        } else {\n            kl = calculateKLDivergence(dist2, dist1);\n        }\n        grid.push({mean: m, variance: v, kl: kl});\n      }\n   }\n   return grid;\n}\n\n// Cell 5: Create and update the Gaussian mixture model plot\nfunction createGuassianPlot(mean, variance, P) {\n  const width = 600;\n  const height = 400;\n  const margin = {top: 20, right: 20, bottom: 40, left: 60};\n\n  const dataQ = generateData(mean, variance);\n  \n  const x = d3.scaleLinear()\n    .domain([-5, 5])\n    .range([margin.left, width - margin.right]);\n\n  const y = d3.scaleLinear()\n    .domain([0, d3.max([...P, ...dataQ], d =&gt; d.y)])\n    .range([height - margin.bottom, margin.top]);\n\n  const line = d3.line()\n    .x(d =&gt; x(d.x))\n    .y(d =&gt; y(d.y));\n\n  const svg = d3.create(\"svg\")\n    .attr(\"viewBox\", [0, 0, width, height]);\n\n  // X-axis\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n    .call(d3.axisBottom(x));\n  \n  // Y-axis\n  const yAxis = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},0)`)\n    .call(d3.axisLeft(y));\n  \n  // X-axis label\n  svg.append(\"text\")\n    .attr(\"x\", width / 2)\n    .attr(\"y\", height - 5)\n    .attr(\"text-anchor\", \"middle\")\n    .text(\"X\");\n\n  // Y-axis label (centered and rotated)\n  svg.append(\"text\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"x\", -height / 2)\n    .attr(\"y\", 15)\n    .attr(\"text-anchor\", \"middle\")\n    .text(\"Probability Density\");\n\n  // Line for Q (interactive)\n  const pathQ = svg.append(\"path\")\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"steelblue\")\n    .attr(\"stroke-width\", 2);\n  \n  // Line for P (static GMM)\n  svg.append(\"path\")\n    .datum(P)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"orange\")\n    .attr(\"stroke-width\", 2)\n    .attr(\"d\", line);\n  \n  // Legend\n  const legend = svg.append(\"g\")\n    .attr(\"transform\", `translate(${width - margin.right - 25},${margin.top})`);\n  \n  legend.append(\"rect\")\n    .attr(\"width\", 20)\n    .attr(\"height\", 20)\n    .attr(\"fill\", \"steelblue\");\n  \n  legend.append(\"text\")\n    .attr(\"x\", 25)\n    .attr(\"y\", 15)\n    .text(\"Q\");\n  \n  legend.append(\"rect\")\n    .attr(\"y\", 25)\n    .attr(\"width\", 20)\n    .attr(\"height\", 20)\n    .attr(\"fill\", \"orange\");\n  \n  legend.append(\"text\")\n    .attr(\"x\", 25)\n    .attr(\"y\", 40)\n    .text(\"P\");\n\n  // Update Q distribution\n  y.domain([0, d3.max([...P, ...dataQ], d =&gt; d.y)]);\n  yAxis.call(d3.axisLeft(y));\n  pathQ.datum(dataQ).attr(\"d\", line);\n  \n  return svg.node();\n}\n\n// Cell 6: Refactored KL divergence contour plot\nfunction createklContourPlot(mean, variance, klGrid, cntTitle) {\n  const width = 600;\n  const height = 400;\n  const margin = {top: 35, right: 30, bottom: 40, left: 50};\n  const plotWidth = width - margin.left - margin.right;\n  const plotHeight = height - margin.top - margin.bottom;\n\n  const uniqueMeans = Array.from(new Set(klGrid.map(d =&gt; d.mean)));\n  const uniqueVariances = Array.from(new Set(klGrid.map(d =&gt; d.variance)));\n\n  // Create 2D array of KL values\n  const klValues = uniqueVariances.map(v =&gt; \n    uniqueMeans.map(m =&gt; {\n      const point = klGrid.find(d =&gt; d.mean === m && d.variance === v);\n      return point ? point.kl : null;\n    })\n  );\n\n  // Scales\n  const xScale = d3.scaleLinear()\n    .domain(d3.extent(uniqueMeans))\n    .range([0, plotWidth]);\n\n  const yScale = d3.scaleLinear()\n    .domain(d3.extent(uniqueVariances))\n    .range([plotHeight, 0]);\n\n  const colorScale = d3.scaleSequential(d3.interpolateViridis)\n    .domain(d3.extent(klGrid, d =&gt; d.kl));\n  \n  // Generate contours\n  const contours = d3.contours()\n    .size([uniqueMeans.length, uniqueVariances.length])\n    .thresholds(50)\n    (klValues.flat());\n\n  // Create SVG element\n  const svg = d3.create(\"svg\")\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"style\", \"max-width: 100%; height: auto;\");\n\n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`);\n\n  // Draw contour paths\n  g.append(\"g\")\n    .selectAll(\"path\")\n    .data(contours)\n    .join(\"path\")\n    .attr(\"fill\", d =&gt; colorScale(d.value))\n    .attr(\"d\", d3.geoPath()\n      .projection(d3.geoTransform({\n        point: function(x, y) {\n          // Projection mapping based on exact data grid size\n          const scaleX = plotWidth / (uniqueMeans.length - 1);\n          const scaleY = plotHeight / (uniqueVariances.length - 1);\n          this.stream.point(x * scaleX, plotHeight - y * scaleY);  // Corrected projection\n        }\n      }))\n    );\n\n  // X-axis\n  g.append(\"g\")\n    .attr(\"transform\", `translate(0,${plotHeight})`)\n    .call(d3.axisBottom(xScale).ticks(10))\n    .append(\"text\")\n    .attr(\"x\", plotWidth / 2)\n    .attr(\"y\", 30)\n    .attr(\"fill\", \"currentColor\")\n    .attr(\"text-anchor\", \"middle\")\n    .text(\"Mean\");\n\n  // Y-axis (centered label and rotated)\n  g.append(\"g\")\n    .call(d3.axisLeft(yScale).ticks(10))\n    .append(\"text\")\n    .attr(\"transform\", \"rotate(-90)\")\n    .attr(\"x\", -plotHeight / 2)\n    .attr(\"y\", -35)\n    .attr(\"text-anchor\", \"middle\")\n    .attr(\"fill\", \"currentColor\")\n    .text(\"Variance\");\n\n  svg.append(\"text\")\n   .attr(\"x\", width / 2)\n   .attr(\"y\", margin.top - 20)\n   .attr(\"text-anchor\", \"middle\")\n   .style(\"font-size\", \"16px\")\n   .text(cntTitle);\n\n  // Function to update point dynamically\n  function updatePoint(mean, variance) {\n    const nearestPoint = klGrid.reduce((a, b) =&gt; \n      (Math.abs(b.mean - mean) + Math.abs(b.variance - variance) &lt; \n       Math.abs(a.mean - mean) + Math.abs(a.variance - variance)) ? b : a\n    );\n\n    g.selectAll(\".current-point, .kl-value\").remove();\n\n    g.append(\"circle\")\n      .attr(\"class\", \"current-point\")\n      .attr(\"cx\", xScale(mean))\n      .attr(\"cy\", yScale(variance))\n      .attr(\"r\", 5)\n      .attr(\"fill\", \"red\")\n      .attr(\"stroke\", \"white\");\n\n    g.append(\"text\")\n      .attr(\"class\", \"kl-value\")\n      .attr(\"x\", xScale(mean))\n      .attr(\"y\", yScale(variance) - 10)\n      .attr(\"text-anchor\", \"middle\")\n      .attr(\"fill\", \"white\")\n      .attr(\"stroke\", \"black\")\n      .attr(\"stroke-width\", 2)\n      .attr(\"paint-order\", \"stroke\")\n      .attr(\"font-weight\", \"bold\")\n      .text(nearestPoint.kl.toFixed(2));\n  }\n\n  updatePoint(mean, variance);\n\n  return Object.assign(svg.node(), {updatePoint});\n}\n\nklGrid = calculateKLDivergenceGrid(\n    P,\n    d3.range(-3, 3.1, 0.1),\n    d3.range(0.5, 3.1, 0.1),\n    true\n);\ngaussianPlot = createGuassianPlot(mean, variance, P);\nklContourPlot = createklContourPlot(\n    mean, variance, klGrid,\n    \"KL Divergence (P, Q) Contour Plot\"\n);\n\nhtml`&lt;div style=\"display: flex; align-items: center; column-gap: 1em;\"&gt;\n &lt;div style=\"flex-basis:50%\"&gt;${gaussianPlot}&lt;/div&gt;\n &lt;div style=\"flex-basis:50%\"&gt;${klContourPlot} &lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, the minimum as achieved when \\(Q\\) is spread out enough to cover the modes of \\(P\\). This is the mean-seeking behavior of the forward KL divergence.\n\n\nReverse KL\n\\(D_{KL}(Q \\;\\Vert\\; P)\\) measures the extra bits needed to encode samples from \\(Q\\) using a code optimized for \\(P\\). Similar to the Section¬†4.2 analysis:\n\nIt penalizes distributions \\(Q\\) that assign non-zero probability to regions where p is zero. Conversely, it doesn‚Äôt penalize \\(Q\\) for missing regions where \\(P\\) has non-zero probability. This is called zero-forcing.\nIt tends to capturing the dominant mode of \\(P\\) and ignoring the rest.\n\nThe plot below is same as the previous one, but this time it the contour plot shows the reverse KL divergence.\n\n\nviewof mean2 = slider({\n  min: -3, \n  max: 3, \n  step: 0.1, \n  value: 0, \n  title: \"Mean\", \n})\nviewof variance2 = slider({\n  min: 0.5, \n  max: 3, \n  step: 0.1, \n  value: 1, \n  title: \"Variance\",\n})\n\nklGrid2 = calculateKLDivergenceGrid(\n    P,\n    d3.range(-3, 3.1, 0.1),\n    d3.range(0.5, 3.1, 0.1),\n    false,\n);\ngaussianPlot2 = createGuassianPlot(mean2, variance2, P);\nklContourPlot2 = createklContourPlot(\n    mean2, variance2, klGrid2,\n    \"KL Divergence (Q, P) Contour Plot\"\n);\n\nhtml`&lt;div style=\"display: flex; align-items: center;\"&gt;\n &lt;div style=\"flex-basis:55%\"&gt;${gaussianPlot2}&lt;/div&gt;\n &lt;div style=\"flex-basis:45%\"&gt;${klContourPlot2} &lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can see how the minimum is achieved when \\(Q\\) is concentrated around the dominant mode of \\(P\\). This is the mode-seeking/zero-forcing behavior of the reverse KL divergence. This property makes it desirable in the context of density estimation and variational inference.\n\n\n\n4.4 Relations to Mutual Information\nLet \\((X,Y)\\) be a pair of random variables with values over the space \\({\\mathcal {X}}\\times {\\mathcal {Y}}\\). If their joint distribution is \\({P_{(X,Y)}}\\) and the marginal distributions are \\({P_{Y}}\\) and \\({P_{Y}}\\), the mutual information is defined as:\n\\[{I(X;Y)=D_{\\mathrm {KL} }(P_{(X,Y)}\\|P_{X}P_{Y})} \\tag{4}\\]\nNotice, as per property of the KL divergence that \\(I(X;Y)\\) is equal to zero precisely when the joint distribution coincides with the product of the marginals, i.e.¬†when \\(X\\) and \\(Y\\) are independent (and hence observing \\(Y\\) tells you nothing about \\(X\\)). Intuitively, \\(I(X;Y)\\) is a measure of the price for encoding \\((X,Y)\\) as a pair of independent random variables when in reality they may not be.\nMutual information can also be expressed using conditional entropy:\n\\[{I(X;Y) \\equiv H(X) - H(X \\vert Y)}\\]\n Check out this video for a more detailed explanation: [1]"
  },
  {
    "objectID": "blog/2024-08-24-information-theory/index.html#jensen-shannon-divergence",
    "href": "blog/2024-08-24-information-theory/index.html#jensen-shannon-divergence",
    "title": "My Notes on Information Theory",
    "section": "5 Jensen-Shannon Divergence",
    "text": "5 Jensen-Shannon Divergence\nJS divergence is a symmetric measure, derived from KL divergence, and it does satisfy the properties of a metric. It is defined as the average of the KL divergence from each distribution to the average of both distributions:\n\\[{JS(P \\;\\Vert\\; Q) = \\frac{1}{2}KL(P \\;\\Vert\\; M) + \\frac{1}{2}KL(Q \\;\\Vert\\; M)} \\tag{5}\\]\nwhere \\(M=\\dfrac{1}{2} (P+Q)\\) is a mixture of \\(P\\) and \\(Q\\).\nThe averaging process and the introduction of \\(M\\) ensure that the divergence remains finite, even if one distribution assigns zero probability to an event that the other distribution assigns a positive probability.\nThis can be a desirable property in scenarios where you want a more stable and bounded measure of divergence. However, it also means that JS divergence is less sensitive to cases where one distribution completely ignores an event that the other considers possible."
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDCGAN on MNIST\n\n\n\nGAN\n\n\nCNN\n\n\nKeras\n\n\n\nA Keras implementation of Deep Convolutional Generative Adversarial Networks (DCGAN) trained on MNIST dataset.\n\n\n\nJun, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nD3 Force Layout - Biological Networks\n\n\n\nd3\n\n\ngraph\n\n\nprotein-networks\n\n\nvisualization\n\n\n\nAn interactive graph visualization of protein-protein interaction networks.\n\n\n\nSep, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Musings from My Corner of the World",
    "section": "",
    "text": "Running Llama 3.2 in the Browser!\n\n\n\n\n\n\nNLP\n\n\nLLM\n\n\nWebLLM\n\n\nMLC-LLM\n\n\nWebGPU\n\n\nTVM\n\n\n\nChat with Llama that runs locally in your browser using WebLLM.\n\n\n\n\n\nOct 6, 2024\n\n\nNima Sarang\n\n\n\n\n\n\n\n\n\n\n\n\nMy Notes on Information Theory\n\n\n\n\n\n\nML\n\n\nInformation Theory\n\n\n\nA brief overview of Information Theory concepts that I compiled for my own reference.\n\n\n\n\n\nAug 24, 2024\n\n\nNima Sarang\n\n\n\n\n\n\n\n\n\n\n\n\nCustom Loss Functions for LightGBM and CatBoost\n\n\n\n\n\n\nML\n\n\nLightGBM\n\n\nCatBoost\n\n\nGBT\n\n\n\nGuide on how to implement custom loss functions and evaluation metrics for LightGBM and CatBoost\n\n\n\n\n\nAug 11, 2024\n\n\nNima Sarang\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Multi-Layer LSTM and AdamW from Scratch using NumPy\n\n\n\n\n\n\nML\n\n\nNLP\n\n\nOptimization\n\n\nAnalysis\n\n\n\nThis tutorial walks through the implementation of a multi-layer LSTM model from scratch in pure NumPy, and trains it on the Shakespeare dataset. It also covers the implementation of the AdamW optimizer and the necessary data modules.\n\n\n\n\n\nJun 15, 2024\n\n\nNima Sarang\n\n\n\n\n\n\nNo matching items\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About Me",
    "section": "",
    "text": "CV\n  \n  \n    \n     GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Twitter\n  \n\n  \n  \n\nüëã Hey there!\n\n\nI‚Äôm Nima Sarang. I‚Äôm a Machine Learning Scientist at Expedia Group. I build and manage machine learning models for Search Engine Marketing (SEM), for Expedia Group‚Äôs global brands, like Expedia, Hotels.com and Vrbo.\nI‚Äôm addicted to learning and sharing knowledge, and am enthusiastic about applying statistical and machine learning techniques to solve real-world problems."
  },
  {
    "objectID": "about/index.html#about-me",
    "href": "about/index.html#about-me",
    "title": "About Me",
    "section": "About Me",
    "text": "About Me\nPrior to working at Expedia Group, I was a Machine Learning Engineer at Divar, where I worked on pricing and computer vision models for the largest e-commerce platform in Iran. I had the joy and fortune of working with a lot of talented people, and learned how to build and deploy machine learning models at scale.\nI completed my master‚Äôs degree in Computer Science at Concordia University, where I was a member of the Immersive & Creative Technologies Lab. I worked on applying deep reinforement learning to satellite imagery data for extracting road networks using controllable agents.\nI‚Äôm a passionate about solving problems. In my free time, I often participate in Kaggle-like competitions to learn about applying ML techniques to different problems. I try to keep up with the latest research in the field, and am always looking for ways to improve my skills. One of the reason I started this website is to write about things I discover and share cool stuff.\n\nLet‚Äôs do something awesome together!\nIf you‚Äôre interested in working with me, please drop me a line at contact@nimasarang.com!\n\n\nA little bit more about me‚Ä¶\n\nHere is a timeline of my professional experience and the projects I‚Äôve worked on: \n\n    \n        2022 - Present\n        \n          \n          Expedia Group\n          Machine Learning Scientist\n        \n        Capital Allocation\n            Capital optimization for search ads bidding on Expedia Group's global brands. My team manages a $100M+ annual budget.\n        \n        Sparsity-Aware ML Models\n            Novel ML, bayesian and tree-based methods for training on large-scale and highly sparse data.\n        \n        Real-time Controllers\n            Developed control-loop systems for real-time bidding on search engine marketing platforms.\n        \n    \n\n    \n        Sep 2021 - Mar 2022\n        \n          \n          Concordia University\n          Instructor\n        \n        Ericsson ML/AI Upskill Training Program\n            I taught PyTorch and Computer Vision tutorials to Ericsson employees.\n            I also mentored three teams throughout the program,\n             guiding them through the implementation and debugging of their projects.\n        \n    \n\n    \n        2020 - 2022\n        \n          \n          Immersive & Creative Technologies Lab\n          ML Researcher\n        \n        Tractable Large-scale Deep Reinforcement Learning\n            Leveraged deep reinforcement learning to solve massive-scale environments and developed an automatic extraction system for urban road networks from high-resolution aerial imagery, in collaboration with CAE.\n        \n    \n\n    \n        2021\n        \n          Stock Trading Agent\n          I spent a good chunk of my free time developing a stock trading agent that uses reinforcement learning and forecasting models to make trading decisions. Even though it wasn't able to beat the B&H strategy, I learned a lot about trading, forecasting, scalability, and offline policy evaluation.\n        \n    \n\n    \n        2019 - 2020\n        \n          \n          Divar\n          Machine Learning Engineer\n        \n        \n          License Plate Detection and Anonymization\n          Developed a real-time pose estimation model for automatically hiding vehicle license plates in images, and published an educational technical blog on the implementation details. \n          This is an example in action.\n        \n        \n          Used Vehicle Price Estimation Model\n          Developed a used-car price valuation model that was deployed as a free \n          SaaS to all users.\n        \n        \n          Client-Side ML for Merchandise Valuation\n          Developed a client-side multi-task AI model for image classification and price estimation of merchandise and commodities in real-time. Deployed on Android using Java and TensorFlow Lite. Used fastText and TF-IDF to automatically tag unlabeled data.\n        \n    \n\n    \n        2019\n        \n          Augmented Reality Soccer Using Deep Learning\n          As my Bachelor's thesis, I developed an two-player augmented reality soccer game played witha virtual ball and field.\n          Built using Unity, an optimized semantic segmentation model, and an object tracking algorithm.\n        \n    \n\n    \n        2017 - 2018\n        Computational Biology Research Center\n        Research Assistant\n        \n          Protein Design\n          Worked on designing protein sequences that can fold into a given tertiary structure using AI and evolutionary profiles.\n        \n    \n\n    \n        2016 - 2017\n        \n          \n          Amirkabir University\n          Competitive Programmer\n        \n        ICPC - ACM International Collegiate Programming Contest\n            I was a member of the university's competitive programming team, and participated in the ACM ICPC regional contest. The algorithmic problems we solved varied from graph theory and dynamic programming, to computational\n            geometry and greedy."
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Publications",
    "section": "",
    "text": "Tractable large-scale deep reinforcement learning\n\n\nThis work presents a reinforcement learning framework for road extraction from satellite images, offering reduced computational costs and improved stability through novel techniques and self-supervised loss\n\n\n\nReinforcement Learning\n\n\nComputer Vision\n\n\nSattelite Imagery\n\n\n\n\n\n\nJul 1, 2023\n\n\nNima Sarang, Charalambos Poullis\n\n\n\n\n\n\n\nProtein design using native secondary sub-structures and solvent accessibility\n\n\nAdvancing protein design through an improved genetic algorithm. This work builds on GAPSSIF, adding solvent accessibility to enhance sequence space exploration\n\n\n\nResearch\n\n\nProtein Design\n\n\nGenetic Algorithm\n\n\nSolvent Accessibility\n\n\n\n\n\n\nJan 3, 2018\n\n\nFatemeh Zare-Mirakabad, Marziyeh Movahedi, Nima Sarang, S. Shahriar Arab\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication/2018-01-03-protein-design/index.html",
    "href": "publication/2018-01-03-protein-design/index.html",
    "title": "Protein design using native secondary sub-structures and solvent accessibility",
    "section": "",
    "text": "According to structure-dependent function of proteins, two main challenging problems called Protein Structure Prediction (PSP) and Inverse Protein Folding (IPF) are investigated. In spite of IPF essential applications, it has not been studied as much as PSP problem. In fact, the ultimate goal of IPF problem or protein design is to create proteins with enhanced properties or even novel functions. One of the major computational challenges in protein design is large protein sequence space, namely searching through all plausible sequences is impossible. In our previous research, we introduced a genetic algorithm called GAPSSIF for designing protein secondary structure. This algorithm benefits from evolutionary information obtained by solved protein structures in PDB. Therefore, we constructed a repository of protein secondary sub-structures to accelerate convergence of the algorithm. The secondary structures of designed sequences by GAPSSIF are comparable with those obtained by Evolver and EvoDesign. In this paper, we modify GAPSSIF so it considers solvent accessibility. Therefore, the simple fitness function of GAPSSIF is improved by a multi-featured one to search through the sequence space more precisely."
  },
  {
    "objectID": "publication/2018-01-03-protein-design/index.html#abstract",
    "href": "publication/2018-01-03-protein-design/index.html#abstract",
    "title": "Protein design using native secondary sub-structures and solvent accessibility",
    "section": "",
    "text": "According to structure-dependent function of proteins, two main challenging problems called Protein Structure Prediction (PSP) and Inverse Protein Folding (IPF) are investigated. In spite of IPF essential applications, it has not been studied as much as PSP problem. In fact, the ultimate goal of IPF problem or protein design is to create proteins with enhanced properties or even novel functions. One of the major computational challenges in protein design is large protein sequence space, namely searching through all plausible sequences is impossible. In our previous research, we introduced a genetic algorithm called GAPSSIF for designing protein secondary structure. This algorithm benefits from evolutionary information obtained by solved protein structures in PDB. Therefore, we constructed a repository of protein secondary sub-structures to accelerate convergence of the algorithm. The secondary structures of designed sequences by GAPSSIF are comparable with those obtained by Evolver and EvoDesign. In this paper, we modify GAPSSIF so it considers solvent accessibility. Therefore, the simple fitness function of GAPSSIF is improved by a multi-featured one to search through the sequence space more precisely."
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "",
    "text": "When I started writing this post, my goal was to refresh my knowledge of LSTMs by implementing one from scratch. I was initially tempted to use PyTorch or Karpathy‚Äôs micrograd, but since I also wanted to implement the backpropagation part myself without relying on an autograd engine, I decided to go with NumPy. This choice meant that the optimizer and training loop would also have to be implemented in NumPy, turning the project into a comprehensive deep dive. So here we are üòÖ\nOn the bright side, it‚Äôs been a great learning experience. I‚Äôve refreshed my understanding of computational graphs, gradient accumulation in recurrent models, and the inner workings of the Adam optimizer. In this post, I‚Äôll walk you through the implementation which resembles a PyTorch-like API. The areas covered are:\n\nMulti-layer LSTM Model\n\nAdamW Optimizer\n\nDataset and Dataloader\n\nTraining on the Shakespeare dataset\n\n I‚Äôll be using a similar presentation style to labml.ai as I find it much easier to follow the code when the explanation is right beside it. I hope you find it helpful."
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#introduction",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#introduction",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "",
    "text": "When I started writing this post, my goal was to refresh my knowledge of LSTMs by implementing one from scratch. I was initially tempted to use PyTorch or Karpathy‚Äôs micrograd, but since I also wanted to implement the backpropagation part myself without relying on an autograd engine, I decided to go with NumPy. This choice meant that the optimizer and training loop would also have to be implemented in NumPy, turning the project into a comprehensive deep dive. So here we are üòÖ\nOn the bright side, it‚Äôs been a great learning experience. I‚Äôve refreshed my understanding of computational graphs, gradient accumulation in recurrent models, and the inner workings of the Adam optimizer. In this post, I‚Äôll walk you through the implementation which resembles a PyTorch-like API. The areas covered are:\n\nMulti-layer LSTM Model\n\nAdamW Optimizer\n\nDataset and Dataloader\n\nTraining on the Shakespeare dataset\n\n I‚Äôll be using a similar presentation style to labml.ai as I find it much easier to follow the code when the explanation is right beside it. I hope you find it helpful."
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#multi-layer-lstm",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#multi-layer-lstm",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "2 Multi-Layer LSTM",
    "text": "2 Multi-Layer LSTM\nLong Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture specifically designed to handle long-term dependencies in sequential data. It incorporates a memory state, a hidden state, and three gating mechanisms: the input gate, forget gate, and output gate. These gates control the flow of information into, out of, and within the memory and hidden states, allowing the LSTM to selectively remember or forget information at each time step.\nThe memory state in an LSTM acts as a long-term storage unit, allowing the network to retain information over long sequences. The input gate determines how much new information should be stored in the memory state, while the forget gate controls the amount of old information to be discarded. The output gate regulates the flow of information from the memory state and hidden state to the next time step.\n\n\n\nLSTM Architecture [1]\n\n\nThe LSTM cell consists of the following components: \\[\n\\begin{aligned}\nf_t &= \\sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf}) \\\\\ni_t &= \\sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi}) \\\\\no_t &= \\sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho}) \\\\\n\\tilde{C}_t &= \\tanh(W_{ic}x_t + b_{ic} + W_{hc}h_{t-1} + b_{hc}) \\\\\nC_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\\\\nh_t &= o_t \\odot \\tanh(C_t)\n\\end{aligned}\n\\]\nwhere \\(f_t\\), \\(i_t\\), and \\(o_t\\) are the forget, input, and output gates, respectively. \\(\\tilde{C}_t\\) is the candidate memory state, \\(C_t\\) is the memory state, and \\(h_t\\) is the hidden state at time step \\(t\\). \\(x_t\\) is the input at time step \\(t\\), \\(h_{t-1}\\) is the hidden state at time step \\(t-1\\), and \\(W\\) and \\(b\\) are the weights and biases of each gate.\n\nCIFG LSTM\nIn this post, we‚Äôll implement a special of type of LSTM called Coupled Input and Forget Gate (CIFG) [2]. In CIFG LSTM, the input gate is computed as: \\[i_t = 1 - f_t\\] This reduces the number of parameters in the model and has been shown to perform well in practice.\n\n\nMulti-layers\nA multi-layer LSTM is simply stacking multiple LSTM cells on top of each other. The output of the previous LSTM cell is fed as input to the next LSTM cell. The hidden state of the last LSTM cell is the input to the classification layer.\n\n\n\nMulti-layer LSTM Example [3]. Each row of the green rectangles represent an LSTM cell.\n\n\nNow let‚Äôs get into the implementation, step by step.\n\n\nlstm.py\n        \n            \n        \n            \n                \n                    #\n                \n                Import the dependencies.  \nThe activation functions are defined in a separate module\n\n            \n            \n                1import numpy as np\n2from collections import defaultdict\n3from copy import deepcopy\n4from op import sigmoid, tanh, softmax\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                LSTM Classifier\nMulti-layer LSTM classifier for sequence classification tasks.\nIt consists of an embedding layer, multiple LSTM cells, and a classification head.\nThe model is used to process input sequences and generate output logits.\n\n            \n            \n                5class LSTMClassifier:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                6    def __init__(\n7        self, embed_size, hidden_size, vocab_size, n_cells=1, dropout=0\n8    ) -&gt; None:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Define internal variables\n\n            \n            \n                 9        self.embed_size = embed_size\n10        self.hidden_size = hidden_size\n11        self.vocab_size = vocab_size\n12        self.n_cells = n_cells\n13        self.layers = dict()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Embedding layer\n\n            \n            \n                14        self.layers[\"embedding\"] = np.empty((vocab_size, embed_size))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Create LSTM layers\n\n            \n            \n                15        for cell_index in range(n_cells):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                forget, output, and cell gates\n\n            \n            \n                16            for layer_name in [\"f\", \"o\", \"c\"]:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                The input size of the first layer is embed_size + hidden_size, since the input is the concatenation of the input features and the previous hidden state. For subsequent layers, the input size is 2 x hidden_size.\n\n            \n            \n                17                linp_sz = hidden_size + (\n18                    embed_size if cell_index == 0 else hidden_size\n19                )\n20                self.layers[f\"W{layer_name}_{cell_index}\"] = np.empty(\n21                    (linp_sz, hidden_size)\n22                )\n23                self.layers[f\"b{layer_name}_{cell_index}\"] = np.empty(\n24                    (hidden_size)\n25                )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Classification head (projection layer) to generate the output logits\n\n            \n            \n                26        self.layers[\"W_head\"] = np.empty((hidden_size, vocab_size))\n27        self.layers[\"b_head\"] = np.empty((vocab_size))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize gradients\n\n            \n            \n                28        self.grad = {k: np.empty_like(v) for k, v in self.layers.items()}\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize weights\n\n            \n            \n                29        self.init_weights()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the total number of parameters in the model.\nThe size property of a numpy array returns the number of elements in the array.\n\n            \n            \n                30    @property\n31    def num_parameters(self):\n32        return sum(l.size for l in self.layers.values())\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Glorot/Xavier initialization\nThe weights are initialized from a uniform distribution in the range \\([-d, d]\\), where\n\\(d = \\sqrt{\\frac{6.0}{(r + c)}}\\), and \\(r\\) and \\(c\\) are the number of rows and columns\nin the weight matrix. This makes the variance of the weights inversely proportional to the\nnumber the units, and helps in preventing the gradients from vanishing or exploding during\ntraining. The biases are initialized to zero.\n\n            \n            \n                33    def init_weights(self):\n34        for name, layer in self.layers.items():\n35            if layer.ndim == 1:\n36                self.layers[name] = np.zeros((layer.shape[0]))\n37            elif layer.ndim == 2:\n38                r, c = layer.shape\n39                d = np.sqrt(6.0 / (r + c))\n40                self.layers[name] = np.random.uniform(-d, d, (r, c))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize the hidden and cell states for the LSTM layers.\n\n            \n            \n                41    def init_state(self, batch_size):\n42        state = dict()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                For each LSTM cell, initialize the hidden and cell states to zeros\n\n            \n            \n                43        state[\"h\"] = np.zeros((self.n_cells, batch_size, self.hidden_size))\n44        state[\"c\"] = np.zeros((self.n_cells, batch_size, self.hidden_size))\n45        return state\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Forward pass through the LSTM model.\n\ninputs: Input sequences of shape (batch_size, seq_len, features)\nstate: Hidden and cell states of the LSTM layers. If None, initialize the states to zeros.\nteacher_forcing: If True, use inputs as the input at each timestep.\nIf False, inputs is used as the prefix.\ngeneration_length: Length of the generated sequence when teacher_forcing is False.\n\n\n            \n            \n                46    def forward(\n47        self, inputs, state=None, teacher_forcing=True, generation_length=0\n48    ):\n49        batch_sz, seq_len = inputs.shape[:2]\n50\n51        if teacher_forcing is True:\n52            assert generation_length == 0\n53\n54        n_timestamps = seq_len + generation_length\n55        activations = defaultdict(lambda: defaultdict(list))\n56        outputs = np.zeros((batch_sz, n_timestamps, self.vocab_size))\n57\n58        if state is None:\n59            state = self.init_state(batch_sz)\n60        else:\n61            state = state.copy()  # make a shallow copy\n62        for k in [\"h\", \"c\"]:\n63            activations[k][-1] = state[k]\n64\n65        for timestep in range(n_timestamps):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                If teacher forcing is False and the prefix is consumed, use the previous prediction as the input\nfor the next timestep\n\n            \n            \n                66            if teacher_forcing is False and timestep &gt;= 1:\n67                word_indices = np.argmax(outputs[:, timestep - 1], axis=1)\n68            else:\n69                word_indices = inputs[:, timestep]\n70            features = self.layers[\"embedding\"][word_indices]\n71            activations[\"input\"][timestep] = word_indices\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Forward pass through the LSTM cells\n\n            \n            \n                72            for cell_idx in range(self.n_cells):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Previous cell states\n\n            \n            \n                73                h_prev = state[\"h\"][cell_idx]\n74                c_prev = state[\"c\"][cell_idx]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Concatenate the input features and the previous hidden state\n\n            \n            \n                75                X = np.concatenate((features, h_prev), axis=-1)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Apply the gates, which are linear operations followed by activation functions\n\\(\n\\begin{aligned}\nf_t &= \\sigma(W_{if}x_t + b_{if} \\;+\\; W_{hf}h_{t-1} + b_{hf}) \\\\[1ex]\ni_t &= 1 - f_t \\qquad\\qquad \\text{Coupled forget and input gates} \\\\[1ex]\no_t &= \\sigma(W_{io}x_t + b_{io} \\;+\\; W_{ho}h_{t-1} + b_{ho}) \\\\[1ex]\n\\tilde{C}_t &= \\tanh(W_{ic}x_t + b_{ic} \\;+\\; W_{hc}h_{t-1} + b_{hc}) \\\\[1ex]\n\\end{aligned}\n\\)\n\n            \n            \n                76                f = sigmoid(\n77                    X @ self.layers[f\"Wf_{cell_idx}\"]\n78                    + self.layers[f\"bf_{cell_idx}\"]\n79                )\n80                i = 1 - f\n81                o = sigmoid(\n82                    X @ self.layers[f\"Wo_{cell_idx}\"]\n83                    + self.layers[f\"bo_{cell_idx}\"]\n84                )\n85                c_bar = tanh(\n86                    X @ self.layers[f\"Wc_{cell_idx}\"]\n87                    + self.layers[f\"bc_{cell_idx}\"]\n88                )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                New memory cell and hidden state\n\\(\n\\begin{aligned}\nC_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\\\[1ex]\nh_t &= o_t \\odot \\tanh(C_t)\n\\end{aligned}\n\\)\n\n            \n            \n                89                c = f * c_prev + i * c_bar\n90                h = o * tanh(c)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Classification head\n\n            \n            \n                91                if cell_idx == self.n_cells - 1:\n92                    logits = h @ self.layers[\"W_head\"] + self.layers[\"b_head\"]\n93                    probs = softmax(logits, axis=1)\n94                    outputs[:, timestep] = probs\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Update the state for the next timestep\n\n            \n            \n                95                state[\"c\"][cell_idx] = c\n96                state[\"h\"][cell_idx] = h\n97                features = h\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Save the activations for backpropagation\n\n            \n            \n                 98                for k, v in zip(\n 99                    [\"x\", \"f\", \"o\", \"c_bar\", \"c\", \"h\"], [X, f, o, c_bar, c, h]\n100                ):\n101                    activations[k][timestep].append(v)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                102        return outputs, state, activations\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Alias for the forward method, similar to PyTorch's nn.Module.\nThis enables model(inputs) \\(\\equiv\\) model.forward(inputs)\n\n            \n            \n                103    __call__ = forward\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Backward pass to compute the gradients.\n\ngrad: Gradient of the loss with respect to the output of the model, i.e. logits (pre-softmax scores)\nactivations: Activations from the forward pass.\n\n\n            \n            \n                104    def backward(self, grad, activations):\n105        batch_sz, seq_len = grad.shape[:2]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Intialize the gradients of the next timestep to zeros. This will be updated as we move backward in time.\n\n            \n            \n                106        grad_next = {\n107            k: np.zeros((self.n_cells, batch_sz, self.hidden_size))\n108            for k in [\"h\", \"c\"]\n109        }\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Helper function to compute the gradients of the linear layer.\nThe gradients are computed with respect to the input, weights, and biases respectively.\n\nX: Input to the linear layer\nW: Weights of the linear layer\ndY: Gradient of the loss with respect to the output of the linear layer\n\n\n            \n            \n                110        def _lin_grad(X, W, dY):\n111            return (dY @ W.T, X.T @ dY, dY)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Backpropagation through time\n\n            \n            \n                112        for timestep in reversed(range(seq_len)):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Classification head\n\n            \n            \n                113            dout_t = grad[:, timestep]\n114            h_t = activations[\"h\"][timestep][-1]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \\(\\text{logits}_t = h_t \\mathbf{W}_{\\text{head}} + \\mathbf{b}_{\\text{head}}\\)\n\n            \n            \n                115            dh_t, dW_head, db_head = _lin_grad(\n116                X=h_t, W=self.layers[\"W_head\"], dY=dout_t\n117            )\n118            self.grad[f\"W_head\"] += dW_head\n119            self.grad[f\"b_head\"] += np.sum(db_head, axis=0)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Iterate over the LSTM cells in reverse order\n\n            \n            \n                120            for cell_idx in reversed(range(self.n_cells)):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Get the activations for the current timestep\n\n            \n            \n                121                x, f, o, c_bar, c_t = (\n122                    activations[key][timestep][cell_idx]\n123                    for key in [\"x\", \"f\", \"o\", \"c_bar\", \"c\"]\n124                )\n125                c_p = activations[\"c\"][timestep - 1][cell_idx]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Gradients flowing from the next timestep. The gradient of the hidden state \\(h_t\\) is the sum of\nthe gradients from the next cell and the next timestep.\n\n            \n            \n                126                dh_t += grad_next[\"h\"][cell_idx]\n127                dc_t = grad_next[\"c\"][cell_idx]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \\(h_t = o * tanh(c_t)\\)\n\n            \n            \n                128                do = dh_t * tanh(c_t)\n129                dc_t = dh_t * o * tanh(c_t, grad=True)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \\(c_t = f \\times c_p + (1 - f) \\times c_{\\text{bar}}\\)\n\n            \n            \n                130                df = dc_t * (c_p - c_bar)\n131                dc_p = dc_t * f\n132                dc_bar = dc_t * (1 - f)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Pre-activation gradients\n\n            \n            \n                133                dc_bar *= tanh(c_bar, grad=True)\n134                do *= sigmoid(o, grad=True)\n135                df *= sigmoid(f, grad=True)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                f, o, c Gates\nSince all the gates are linear operations, the calculation will be similar\n\n            \n            \n                136                dinp, dh_prev = 0, 0\n137                for gate, doutput in zip([\"f\", \"o\", \"c\"], [df, do, dc_bar]):\n138                    dX, dW, db = _lin_grad(\n139                        X=x, W=self.layers[f\"W{gate}_{cell_idx}\"], dY=doutput\n140                    )\n141                    self.grad[f\"W{gate}_{cell_idx}\"] += dW\n142                    self.grad[f\"b{gate}_{cell_idx}\"] += np.sum(db, axis=0)\n143                    dinp_gate, dh_prev_gate = (\n144                        dX[:, : -self.hidden_size],\n145                        dX[:, -self.hidden_size :],\n146                    )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Accumulate the gradients for the input and the hidden state,\nsince they are shared between the gates\n\n            \n            \n                147                    dinp += dinp_gate\n148                    dh_prev += dh_prev_gate\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Update the gradients for the previous timestep\n\n            \n            \n                149                dh_t = dinp\n150                grad_next[\"c\"][cell_idx] = dc_p\n151                grad_next[\"h\"][cell_idx] = dh_prev\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Embedding layer\n\n            \n            \n                152            word_indices = activations[\"input\"][timestep]\n153            self.grad[\"embedding\"][word_indices] += dinp\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Helper method to serialize the model state, similar to PyTorch's state_dict.\nThe state dictionary contains the model configuration, weights, and gradients.\nIt can be used to save and load the model.\n\n            \n            \n                154    @property\n155    def state_dict(self):\n156        return dict(\n157            config=dict(\n158                embed_size=self.embed_size,\n159                hidden_size=self.hidden_size,\n160                vocab_size=self.vocab_size,\n161                n_cells=self.n_cells,\n162            ),\n163            weights=deepcopy(self.layers),\n164            grad=deepcopy(self.grad),\n165        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                166    @classmethod\n167    def from_state_dict(cls, state_dict):\n168        obj = cls(**state_dict[\"config\"])\n169        for src, tgt in zip(\n170            [state_dict[\"weights\"], state_dict[\"grad\"]],\n171            [obj.layers, obj.grad],\n172        ):\n173            for k, v in src.items():\n174                tgt[k][:] = v\n175        return obj"
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#activation-and-loss-functions",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#activation-and-loss-functions",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "3 Activation and Loss Functions",
    "text": "3 Activation and Loss Functions\nThe activation functions used in LSTM are the sigmoid, tanh (hyperbolic tangent), and softmax functions.\n\nSigmoid is used to compute the gates, which are values between 0 and 1 that control the flow of information.\n\ntanh function is used to compute the candidate memory state.\n\nSoftmax is used to compute the output probabilities.\n\nThe loss function used is the cross-entropy loss, which is suitable for classification tasks. Next token prediction is indeed a classification task where the model predicts the probability distribution over the vocabulary for the next token in the sequence.\n\n\nop.py\n        \n            \n        \n            \n                \n                    #\n                \n                Sigmoid function\nThe sigmoid squashes the input to the range [0, 1].\n\nIf the flag grad is False, returns the sigmoid of x: $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\nOtherwise, \\(x = \\sigma(z)\\) and the derivate \\(\\frac{\\partial \\sigma(z)}{\\partial z}\\) is returned:\n$$\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z) * (1 - \\sigma(z))= x(1-x)$$.\n\n\n            \n            \n                1import numpy as np\n2\n3\n4def sigmoid(x, grad=False):\n5    if not grad:\n6        return 1 / (1 + np.exp(-x))\n7    return x * (1 - x)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Hyperbolic tangent function\nThe tanh function squashes the input to the range [-1, 1]. It's defined as:\n$$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n\n            \n            \n                 8def tanh(x, grad=False):\n 9    if not grad:\n10        return np.tanh(x)\n11    return 1 - x**2\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Softmax function\nApplies the softmax function to the input array along the specified axis. Softmax converts\na vector of real numbers into a probability distribution. The logits are first exponentiated\nto make them positive and increase their separation. It's defined as:\n$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n\n            \n            \n                12def softmax(x, axis):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Subtracting the maximum value for numerical stability. Softmax is invariant to to a constant shift\n\n            \n            \n                13    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n14    return exps / np.sum(exps, axis=axis, keepdims=True)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Cross-entropy loss function\nComputes the cross-entropy loss between the predicted and target distributions. The cross-entropy loss is defined as:\n$$H(y, p) = -\\sum_{i} y_i \\log(p_i)$$\n\nprediction: The predicted array of probabilities of shape (batch_size, num_classes).\ntarget: The target array of shape (batch_size,) containing the class indices.\n\n\n            \n            \n                15def cross_entropy(prediction, target, reduction=\"mean\"):\n16    eps = np.finfo(prediction.dtype).eps\n17    prediction = np.clip(prediction, eps, 1 - eps)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Take the negative log of the predicted probability of the target class\n\n            \n            \n                18    loss = -np.take_along_axis(\n19        np.log(prediction), target[..., np.newaxis], axis=-1\n20    )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Aggregate the loss\n\n            \n            \n                21    if reduction == \"mean\":\n22        loss = loss.mean()\n23    elif reduction == \"sum\":\n24        loss = loss.sum()\n25    return loss"
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#adamw",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#adamw",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "4 AdamW",
    "text": "4 AdamW\nAdamW is a variant of the Adam optimizer that decouples weight penalty from the optimization steps, where the weight penalty is applied directly to the gradients. Adam optimizer uses both the first and second moments of the gradients to adapt the learning rate tailored to each parameter. The benefit of Adam/AdamW is that it requires little tuning of hyperparameters compared to RMSprop and SGD. We‚Äôll go over each step of the optimization in the implementation.\n\n\noptim.py\n        \n            \n        \n            \n                \n                    #\n                \n                Import NumPy\n\n            \n            \n                1import numpy as np\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                2class AdamW:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                AdamW Optimizer\nParameters:\n\nparams (dict): Dictionary referencing the model parameters\ngrads (dict): Dictionary referencing the gradients of the model parameters\nlr (float): Learning rate\nbetas (Tuple[float, float]): Coefficients used for computing running averages of gradient and its square\neps (float): Term added to the denominator to improve numerical stability\nweight_decay (float): Weight decay (L2 penalty) coefficient\namsgrad (bool): Whether to use the AMSGrad variant of the algorithm\n\n\n            \n            \n                 3    def __init__(\n 4        self,\n 5        params: dict,\n 6        grads: dict,\n 7        lr=0.001,\n 8        betas: tuple[float, float] = (0.9, 0.999),\n 9        eps: float = 1e-8,\n10        weight_decay: float = 1e-2,\n11        amsgrad: bool = False,\n12    ):\n13        self.params = params\n14        self.grads = grads\n15        self.lr = lr\n16        self.betas = betas\n17        self.eps = eps\n18        self.weight_decay = weight_decay\n19        self.amsgrad = amsgrad\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Counter for the number of iterations\n\n            \n            \n                20        self.n_iters = 0\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize first moment vector (mean of gradients) for each parameter\n\n            \n            \n                21        self.m = {k: np.zeros_like(v) for k, v in params.items()}\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize second moment vector (uncentered variance of gradients) for each parameter\n\n            \n            \n                22        self.v = {k: np.zeros_like(v) for k, v in params.items()}\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Initialize maximum of second moment vector for AMSGrad if needed\n\n            \n            \n                23        self.v_m = (\n24            {k: np.zeros_like(v) for k, v in params.items()}\n25            if amsgrad\n26            else None\n27        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Resets all gradients to zero. This is typically used before computing new\ngradients in the training loop.\n\n            \n            \n                28    def zero_grad(self):\n29        for v in self.grads.values():\n30            v[:] = 0\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Perform a single optimization step.\nUpdates the parameters of the model using the AdamW update rule, which\nincludes bias correction, optional AMSGrad, and weight decay.\n\n            \n            \n                31    def step(self):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Increment the iteration counter\n\n            \n            \n                32        self.n_iters += 1\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Unpack the beta values\n\n            \n            \n                33        beta1, beta2 = self.betas\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Iterate over the parameters and their gradients\n\n            \n            \n                34        for (name, param), grad in zip(\n35            self.params.items(), self.grads.values()\n36        ):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Update the first moment estimate:\n$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\nwhere \\(\\beta_1\\) is the exponential decay rate for the first moment estimates,\nand \\(g_t\\) is the gradient at time step \\(t\\).  \n\\(m_{t}\\) is simply an exponential moving average (EMA) of the past gradients.\n\n            \n            \n                37            m_t = self.m[name] = beta1 * self.m[name] + (1 - beta1) * grad\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Update the second moment estimate:\n$$v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$$\nwhere \\(\\beta_2\\) is the exponential decay rate for the second moment estimates.\n\n            \n            \n                38            v_t = self.v[name] = beta2 * self.v[name] + (1 - beta2) * (\n39                grad**2\n40            )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute bias-corrected first moment estimate:\n$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\nWithout correction, the bias causes the algorithm to move very slowly at the beginning of training,\nas the moment estimates are underestimated. In the early iterations, \\(t\\) is small, so \\(\\beta_1^t\\) is close to 1,\nmaking \\(1 - \\beta_1^t\\) a small number. Dividing by this small number effectively increases the estimate.\n\n            \n            \n                41            m_t_hat = m_t / (1 - beta1**self.n_iters)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Compute bias-corrected second moment estimate:\n$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n\n            \n            \n                42            v_t_hat = v_t / (1 - beta2**self.n_iters)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                AMSGrad update:\n$$\\hat{v}_t = \\max(\\hat{v}_t, v_{t-1})$$\nwhere \\(v_{t-1}\\) is the previous second moment estimate.\nThis ensures \\(v_t\\) is always non-decreasing, preventing the learning rate from growing too large.\n\n            \n            \n                43            if self.amsgrad:\n44                v_t_hat = self.v_m[name] = np.maximum(self.v_m[name], v_t_hat)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Adjusted gradient:\n$$\\hat{g} = \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\nwhere \\(\\epsilon\\) is a small constant to avoid division by zero.  \n\\(\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t}}\\) can be thought of as the signal-to-noise ratio of the gradient.\nI'll leave the intuition behind this to another blog post.\n\n            \n            \n                45            g_hat = m_t_hat / (np.sqrt(v_t_hat) + self.eps)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Add weight penalty to the update:\n$$\\text{update} = \\hat{g} + \\lambda \\cdot p$$\nwhere \\(\\lambda\\) is the weight_decay coefficient.\nThis is equivalent to adding the L2 penalty to the loss function, which penalizes large weights.\n\n            \n            \n                46            update = g_hat + self.weight_decay * param\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Update the parameters in the direction of the negative gradient, scaled by the learning rate:\n$$ p_t = p_{t-1} - \\eta \\cdot \\text{update}$$\nwhere \\(p_{t-1}\\) is the previous parameter value.\n\n            \n            \n                47            self.params[name] -= self.lr * update"
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#data-utilities",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#data-utilities",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "5 Data Utilities",
    "text": "5 Data Utilities\nIn this section we‚Äôll implement the Dataset and Dataloader classes to handle the Shakespeare dataset. We follow the best practices of PyTorch‚Äôs Dataset and DataLoader classes to make the implementation more modular and reusable.\n\nThe Dataset class implements the __getitem__ method, which returns a single sample from the dataset.\nThe DataLoader class will be used to sample mini-batches from the dataset, by calling the __getitem__ method of the Dataset.\n\n\n\ndata.py\n        \n            \n        \n            \n                \n                    #\n                \n                Import NumPy\n\n            \n            \n                1import numpy as np\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Dataset\nA dataset for next character prediction tasks.\nFor a sequence of characters \\([c_1, c_2, ..., c_n]\\) and a given sequence length \\(l\\),\nthis dataset creates input/target pairs of the form:\n\nInput \\(x_i\\):  \\([c_i, c_{i+1}, ..., c_{i+l-1}]\\)\nTarget \\(y_i\\): \\([c_{i+1}, c_{i+2}, ..., c_{i+l}]\\)\n\nwhere \\(i\\) ranges from 1 to \\(n-l\\).\nEach item in the dataset is a tuple \\((x_i, y_i)\\) where both \\(x_i\\) and \\(y_i\\) have length \\(l\\).\nThe task is to predict each character in \\(y_i\\) given the corresponding prefix in \\(x_i\\).\nFor example, given \\(x_i = [c_i, c_{i+1}, c_{i+2}]\\), the model would aim to predict:\n\n\\(c_{i+1}\\) given \\([c_i]\\)\n\\(c_{i+2}\\) given \\([c_i, c_{i+1}]\\)\n\\(c_{i+3}\\) given \\([c_i, c_{i+1}, c_{i+2}]\\)\n\n\n            \n            \n                2class NextCharDataset:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                3    def __init__(self, data, seq_length):\n4        self.data = data.copy()\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Create a sliding window view of the data\n\n            \n            \n                5        self.window_view = np.lib.stride_tricks.sliding_window_view(\n6            self.data, window_shape=seq_length + 1\n7        )\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                8    def __len__(self):\n9        return len(self.window_view)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \\(\\text{Input}_i\\):  \\([c_i, c_{i+1}, ..., c_{i+l-1}]\\)\n\\(\\text{Target}_i\\): \\([c_{i+1}, c_{i+2}, ..., c_{i+l}]\\)\n\n            \n            \n                10    def __getitem__(self, idx):\n11        x, y = self.window_view[idx, :-1], self.window_view[idx, 1:]\n12        return x, y\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                DataLoader\nA simple DataLoader to iterate over a dataset in batches.\n\n            \n            \n                13class DataLoader:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                14    def __init__(self, dataset, batch_size, shuffle=False, drop_last=False):\n15        self.dataset = dataset\n16        self.batch_size = batch_size\n17        self.shuffle = shuffle\n18        self.drop_last = drop_last\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                The __iter__ method returns an iterator that yields batches of data. It's mainly\nused in a for loop to iterate over the dataset. e.g.:\nfor inputs, targets in dataloader:\n    ...\n\n\n\n            \n            \n                19    def __iter__(self):\n20        indices = np.arange(len(self.dataset))\n21\n22        if self.shuffle:\n23            np.random.shuffle(indices)\n24\n25        if self.drop_last:\n26            remainder = len(self.dataset) % self.batch_size\n27            if remainder:\n28                indices = indices[:-remainder]\n29\n30        for i in range(0, len(indices), self.batch_size):\n31            batch_indices = indices[i : i + self.batch_size]\n32            batch = [self.dataset[j] for j in batch_indices]\n33            yield self.collate_fn(batch)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                34    def __len__(self):\n35        if self.drop_last:\n36            return len(self.dataset) // self.batch_size\n37        else:\n38            return np.ceil(len(self.dataset) / self.batch_size).astype(int)\n39\n40    def collate_fn(self, batch):\n41        if isinstance(batch[0], (tuple, list)):\n42            return [np.array(samples) for samples in zip(*batch)]\n43        elif isinstance(batch[0], dict):\n44            return {\n45                key: np.array([d[key] for d in batch]) for key in batch[0]\n46            }\n47        else:\n48            return np.array(batch)"
  },
  {
    "objectID": "blog/2024-06-15-lstm-from-scratch/index.html#training-on-shakespeare-dataset",
    "href": "blog/2024-06-15-lstm-from-scratch/index.html#training-on-shakespeare-dataset",
    "title": "Implementing Multi-Layer LSTM and AdamW from Scratch using NumPy",
    "section": "6 Training on Shakespeare dataset",
    "text": "6 Training on Shakespeare dataset\nNow it‚Äôs time to put everything together and train the model on the a dataset. We‚Äôll use the Shakespeare dataset, which consists of a collection of Shakespeare‚Äôs plays. The model will be trained to predict the next character in the sequence given a sequence of characters.\nAn important distinction to make between the text generation at training time and inference time is that at training time, we feed the ground truth characters to the model to predict the next character; This is called teacher forcing. At inference time, we feed the model‚Äôs prediction at time step \\(t\\) as the input at time step \\(t+1\\) to predict the next character.\n\n6.1 Load\n\nimport numpy as np\nimport matplotlib.pylab as plt\n\nDownload the Shakespeare dataset which is a single text file from the following link: Shakespeare dataset\n\nwith open(\"shakespeare.txt\") as file:\n    data = file.read()\n\n\nprint(data[:200])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you\n\n\n\n\n6.2 Preprocess\nWe need to convert the text data into numerical data. Using scikit-learn‚Äôs LabelEncoder we can map each character to a unique integer. The same encoder will be used to inverse transform the predictions back to characters.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nchar_data = np.array(list(data))\nencoder = LabelEncoder()\nindices_data = encoder.fit_transform(char_data)\n\n\nvocabulary = encoder.classes_\nvocabulary\n\narray(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n      dtype='&lt;U1')\n\n\nAn example of the mapped data:\n\nindices_data[:200]\n\narray([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n       44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n       52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n       51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31,\n       54, 43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56,\n       57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39,\n       56, 43,  1, 39, 50, 50,  1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56,\n       39, 58, 46, 43, 56,  1, 58, 53,  1, 42, 47, 43,  1, 58, 46, 39, 52,\n        1, 58, 53,  1, 44, 39, 51, 47, 57, 46, 12,  0,  0, 13, 50, 50, 10,\n        0, 30, 43, 57, 53, 50, 60, 43, 42,  8,  1, 56, 43, 57, 53, 50, 60,\n       43, 42,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43,\n       52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63, 53, 59])\n\n\n\n\n6.3 Initialize\nNow let‚Äôs define the dataloader, the model and the optimizer. I used the following hyperparameters below, but feel free to experiment with different values.\n\nSEQUENCE_LENGTH = 128\nBATCH_SIZE = 32\nVOCAB_SIZE = len(vocabulary)\nTRAIN_SPLIT = 0.8\nLEARNING_RATE = 0.001\nSHUFFLE_TRAIN = True\n\nEMBED_SIZE = 256\nHIDDEN_SIZE = 512\nNUM_LAYERS = 2\nNUM_EPOCHS = 5\n\nDefine the train and test data loaders\n\nfrom data import NextCharDataset, DataLoader\n\ntrainset_size = int(len(indices_data) * TRAIN_SPLIT)\ntrain_data = indices_data[:trainset_size]\ntest_data = indices_data[trainset_size:]\n\ntrainset = NextCharDataset(train_data, SEQUENCE_LENGTH)\ntestset = NextCharDataset(test_data, SEQUENCE_LENGTH)\n\ntrainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=SHUFFLE_TRAIN)\ntestloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n\nDefine the model and optimizer\n\nfrom lstm import LSTMClassifier\nfrom optim import AdamW\n\nmodel = LSTMClassifier(EMBED_SIZE, HIDDEN_SIZE, VOCAB_SIZE, NUM_LAYERS)\noptimizer = AdamW(params=model.layers, grads=model.grad, lr=LEARNING_RATE)\n\n\n\n6.4 Training loop\n\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\nfrom op import cross_entropy\n\nstate = None\ntrain_losses = defaultdict(list)\ntest_losses = defaultdict(list)\n\nfor epoch in tqdm(range(NUM_EPOCHS), desc=\"Epoch\"):\n    # training loop\n    for inputs, targets in (pbar := tqdm(trainloader, leave=False)):\n        if SHUFFLE_TRAIN:\n            state = None\n        probabilities, state, activations = model.forward(inputs, state)\n\n        # cross entropy loss\n        loss = cross_entropy(probabilities, targets)\n        # accuracy\n        accuracy = np.mean(np.argmax(probabilities, axis=-1) == targets)\n\n        # loss gradient w.r.t logits (before softmax)\n        gradient = np.copy(probabilities)\n        # Subtract 1 from the probabilities of the true classes\n        # Since the gradient is p_i - y_i\n        gradient[np.arange(targets.shape[0])[:, None], \n                 np.arange(targets.shape[1]), targets] -= 1\n        # Subtract 1 from the probabilities of the true classes\n        gradient /= gradient.shape[0]\n\n        # backpropagate and update\n        optimizer.zero_grad()\n        model.backward(gradient, activations)\n        optimizer.step()\n\n        # log\n        pbar.set_postfix({\"loss\": f\"{loss:.5f}\", \n                          \"accuracy\": f\"{accuracy*100:.2f}\"})\n        train_losses[epoch].append(loss)\n\n    # testing loop\n    loss_sum = 0\n    accuracy_sum = 0\n    for iter, (inputs, targets) in (pbar := tqdm(enumerate(testloader),\n                                                 leave=False)):\n        probabilities, state, _ = model.forward(\n            inputs, state=None, teacher_forcing=False\n        )\n        loss = cross_entropy(probabilities, targets)\n        accuracy = np.mean(np.argmax(probabilities, axis=-1) == targets)\n\n        loss_sum += loss\n        accuracy_sum += accuracy\n        pbar.set_postfix(\n            {\n                \"loss\": f\"{loss_sum / (iter + 1):.5f}\",\n                \"accuracy\": f\"{accuracy_sum / (iter + 1)*100:.2f}\",\n            }\n        )\n        test_losses[epoch].append(loss)\n\nFor checkpointing, we can save the model to disk:\n\nckpt = model.state_dict\nnp.save(\"checkpoint.npy\", ckpt)\n\nTo reload from the checkpoint, use the from_state_dict method:\n\nstate_dict = np.load(\"checkpoint.npy\", allow_pickle=True).item()\nmodel = LSTMClassifier.from_state_dict(state_dict)\n\nstate_dict.keys()\n\ndict_keys(['config', 'weights', 'grad'])\n\n\n\n\n6.5 Generating text\nAt inference time, we feed the model a prefix text and let it generate the next characters. We can control the number of characters to generate by setting the generate_length parameter in forward. I used greedy decoding to generate the text which works by selecting the character with the highest probability at each time step.\n\ndef generate(model, prefix: str, length: int):\n    inputs = np.array(list(prefix))\n    inputs = encoder.transform(inputs)\n    inputs = inputs[np.newaxis]\n    state = None\n\n    probabilities, state, _ = model.forward(\n        inputs, state, teacher_forcing=False, generation_length=length\n    )\n    tokens = np.argmax(probabilities[0, len(prefix) - 1 :], axis=-1)\n\n    output = prefix + \"\".join(encoder.inverse_transform(tokens))\n    return output\n\n\nprint(generate(model, prefix=\"I will\", length=400))\n\nI will rest blood that bear blood at all,\nAnd stay the king to the consulships?\n\nMENENIUS:\nNay, then he will stay the king to the cause of my son's exile is banished.\n\nROMEO:\nAnd stay the common people: there is no need, that I may call thee back.\n\nNORTHUMBERLAND:\nHere comes the county strict ready to give me leave to see him as he fall be thine, my lord.\n\nKING RICHARD II:\nNorfolk, throw down the coronat\n\n\nAs an alternative to basic sampling, more advanced techniques like beam search, Top-K sampling, and nucleus sampling can significantly enhance the text generation quality but that‚Äôd be beyond the scope of this post.\n\nI hope you found this post helpful. If you have any questions or suggestions, feel free to leave a comment. Thanks for reading!"
  },
  {
    "objectID": "blog/2024-08-11-gbt-custom-loss/index.html",
    "href": "blog/2024-08-11-gbt-custom-loss/index.html",
    "title": "Custom Loss Functions for LightGBM and CatBoost",
    "section": "",
    "text": "% Meta %\n% Optional argument [#1]: Size modifier (e.g., , ) % #2: Opening delimiter % #3: Closing delimiter % #4: Content\n% Common sets % Real numbers % Integers % Natural numbers % Rational numbers % Complex numbers\n% Probability and statistics % Expectation % Variance % Covariance % Probability measure % Indicator function\n% Linear algebra\n% Matrix\n% Vector % Trace % Rank % Range (image) % Projection\n% Calculus and analysis % For integrals, e.g., f(x) x\n% Partial derivative \\newcommand{[2]}{ #1} % Partial derivative w/o fraction\n% Second partial derivative % Gradient % Divergence % Curl\n% Set theory\n% Set\n% Set builder notation % Union % Intersection % Symmetric difference\n% Logic and proofs % Implies % If and only if % End of proof % Contradiction\n% Norms and inner products\n% Norm\n% Inner product\n% Common functions % Minimization problem % Maximization problem % Argument minimum % Argument maximum\n% Subject to constraints % Sign function % Span of a set\n% Formatting\n% Absolute value\n% Parentheses\n% Brackets\n% Floor function\n% Ceiling function\n% Asymptotic notations % Big O notation % Small o notation % Big Omega notation % Big Theta notation\n% Commonly used in algorithms and complexity % Polynomial time % Polylogarithmic time\n% Additional probability notations % Independent and identically distributed % Distributed as\n% Fourier transform % Fourier transform % Inverse Fourier transform\n% General math % Display style"
  },
  {
    "objectID": "blog/2024-08-11-gbt-custom-loss/index.html#introduction",
    "href": "blog/2024-08-11-gbt-custom-loss/index.html#introduction",
    "title": "Custom Loss Functions for LightGBM and CatBoost",
    "section": "1 Introduction",
    "text": "1 Introduction\nAs a data scientist, I‚Äôve often found myself pushing the boundaries of popular gradient boosting frameworks. Recently I‚Äôve been exploring the implementation of custom loss functions in LightGBM and CatBoost, two powerful tools in learning from tabular data. These frameworks offer a wide range of built-in loss functions, but sometimes you need to optimize for a specific metric or tackle a unique problem that requires a custom loss.\nIn this blog post, I‚Äôll walk through the process of creating custom loss functions, using Mean Squared Error (MSE) and Mean Squared Logarithmic Error (MSLE) as practical examples. We‚Äôll start by deriving the gradients and Hessians for these functions, to provide the mathematical foundation for our implementations. Then we‚Äôll move on to the code, showing how to integrate these custom losses into LightGBM and CatBoost. Each of these frameworks has its own API for custom losses, so we‚Äôll cover the specifics for each one separately."
  },
  {
    "objectID": "blog/2024-08-11-gbt-custom-loss/index.html#lightgbm",
    "href": "blog/2024-08-11-gbt-custom-loss/index.html#lightgbm",
    "title": "Custom Loss Functions for LightGBM and CatBoost",
    "section": "2 LightGBM",
    "text": "2 LightGBM\n\n2.1 Interface\nThe loss function and the evaluation metric must have the following structure:\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                Objective Function\nCalculate the gradient and hessian of a custom loss function for LightGBM.\nParameters:\n\ntarget (np.ndarray) : The true target values\nprediction (np.ndarray) : The predicted values from the model\nweight (np.ndarray), optional : Sample weights. If None, uniform weights are assumed.\n\nReturns:\n\ngrad (np.ndarray) : First order gradient of the loss with respect to the predictions.\nhess (np.ndarray) : Second order gradient (Hessian) of the loss with respect to the predictions.\n\n\n            \n            \n                1def lgbm_objective_function(\n2    target: np.ndarray,\n3    prediction: np.ndarray,\n4    weight: np.ndarray = None,\n5):\n6    ...\n7\n8    return grad, hess\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Evaluation Metric\nCalculate a custom evaluation metric for LightGBM.\nParameters:\nSame as loss_function.\nReturns:\nA tuple containing three elements:\n\neval_name (str): The name of the metric.\neval_result (float): The value of the metric.\nis_higher_better (bool): Whether a higher value of the metric is better.\n\n\n            \n            \n                 9def lgbm_evaluation_metric(\n10    target: np.ndarray,\n11    prediction: np.ndarray,\n12    weight: np.ndarray = None,\n13):\n14    ...\n15\n16    return eval_name, eval_result, is_higher_better\n\n            \n        \n    \n        \n    \n\n\n\n\n2.2 Mean Squared Error\nMean Squared Error (MSE) is a commonly used loss function for regression and forecasting problems. It is defined as the average of the squared differences between the predicted and actual values: \\[\n\\text{MSE}(y, p) \\;=\\; {\\frac{1}{|\\mathcal{D}|} \\sum_{i=1}^{|\\mathcal{D}|} (y_i - p_i)^2}\n\\tag{1}\\]\nwhere \\(y_i\\) and \\(p_i\\) are the target and predicted values for the \\(i\\)-th sample respectively, and \\(|\\mathcal{D}|\\) is the number of samples in the dataset.\nTo implement MSE as a loss function, we need to derive the gradient and hessian of the loss value with respect to the predicted values.\nThe gradient for sample \\(i\\), ignoring the constant factor of \\(\\frac{1}{|\\mathcal{D}|}\\), is: \\[\n\\frac{\\partial}{\\partial p_i} \\text{MSE}(y, p) \\;=\\; \\frac{\\partial}{\\partial p_i} (y_i - p_i)^2 \\;=\\; -2(y_i - p_i) \\;=\\; 2 (p_i - y_i) \\;\\propto\\; p_i - y_i\n\\tag{2}\\]\nThe reason we can ignore the constant factors is that it will not affect the optimum solution, and its effect will be absorbed by the learning rate.\nThe hessian for sample \\(i\\) is: \\[\n\\frac{\\partial^2}{\\partial p_i^2} \\text{MSE}(y, p) \\;\\propto\\; \\frac{\\partial}{\\partial p_i} p_i - y_i \\;=\\; 1\n\\tag{3}\\]\n\n\n\n\n\n\nPropotionality\n\n\n\n\n\nThe proportionality sign (\\(\\propto\\)) is used to indicate that the expression on the right-hand side is proportional to the expression on the left-hand side, up to a constant factor. Read more about it here.\n\n\n\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                MSE Objective Function\n\n            \n            \n                1import numpy as np\n2\n3\n4def lgbm_mse_objective_function(\n5    target: np.ndarray,\n6    prediction: np.ndarray,\n7    weight: np.ndarray = None,\n8):\n9    gradient = prediction - target\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Hessian is always 1 for MSE\n\n            \n            \n                10    hessian = np.ones_like(gradient)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Apply sample weights\n\n            \n            \n                11    if weight is not None:\n12        gradient *= weight\n13        hessian *= weight\n14    return gradient, hessian\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                MSE Metric\n\n            \n            \n                15def lgbm_mse_metric(\n16    target: np.ndarray,\n17    prediction: np.ndarray,\n18    weight: np.ndarray = None,\n19):\n20    squared_error = (prediction - target) ** 2\n21    mse = np.average(squared_error, weights=weight)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                (metric name, value, is_higher_better)\n\n            \n            \n                22    return \"MSE\", mse, False\n\n            \n        \n    \n        \n    \n\n\n\n\n2.3 Mean Squared Logarithmic Error\nMean Squared Logarithmic Error (MSLE) is defined as the average of the squared differences between the logarithm of the predicted and actual values. It‚Äôs an scale-invariant metric that is commonly used in regression problems where the target values have a wide range of values. I think it‚Äôs a good metric for modeling ratios or percentages.\n\\[\n\\text{MSLE}(y, p) \\;=\\; \\frac{1}{|\\mathcal{D}|} \\sum_{i=1}^{|\\mathcal{D}|} (\\log_e (1 + y_i) - \\log_e (1 + p_i) )^2\n\\tag{4}\\]\nwhere \\(y_i\\) and \\(p_i\\) are the target and predicted values for the \\(i\\)-th sample respectively, and \\(|\\mathcal{D}|\\) is the number of samples in the dataset. The +1 in the logarithm means the target values must be greater than -1, otherwise the logarithm will be undefined. If your target values are in a different range, you can either add a constant to the target values or use a different constant in the logarithm.\nThe gradient for sample \\(i\\) is:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial p_i} \\text{MSLE}(y, p)\n&= \\frac{\\partial}{\\partial p_i} \\Big(\\log_e (1 + y_i) - \\log_e (1 + p_i) \\Big)^2 \\\\[3ex]\n&= \\frac{-2}{1 + p_i} \\Big(\\log_e (1 + y_i) - \\log_e (1 + p_i)\\Big) \\\\[3ex]\n&\\propto \\frac{\\log_e (1 + p_i) - \\log_e (1 + y_i)}{1 + p_i} \\end{aligned}\n\\tag{5}\\]\nThe hessian is calculated by taking another derivative of the gradient:\n\\[\n\\begin{aligned}\n    \\frac{\\partial^2}{\\partial p_i^2} \\text{MSLE}(y, p)\n     & = \\frac{\\partial}{\\partial p_i} \\frac{\\log_e (1 + p_i) - \\log_e (1 + y_i)}{1 + p_i}  \\\\[3ex]\n     & = \\frac{\\splitfrac{(1 + p_i)\\frac{\\partial \\Big(\\log_e (1 + p_i) - \\log_e (1 + y_i)\\Big)}{\\partial p_i} }{ - \\Big(\\log_e (1 + p_i) - \\log_e (1 + y_i)\\Big) \\frac{\\partial (1 + p_i)}{\\partial p_i}}}{(1 + p_i)^2} \\\\[3ex]\n     & = \\frac{(1 + p_i)\\times \\frac{1}{1 + p_i}  - \\Big(\\log_e (1 + p_i) - \\log_e (1 + y_i)\\Big) \\times 1}{(1 + p_i)^2}   \\\\[3ex]\n     & = \\frac{1 - \\log_e (1 + p_i) + \\log_e (1 + y_i)}{(1 + p_i)^2}   \\\\[3ex]\n\\end{aligned}\n\\tag{6}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nSince there is \\(\\log_e{(1 + p_i)}\\) in the formulae, we need to ensure that \\(1 + p_i &gt; 0\\) for all \\(i\\). This can be achieved by clipping the predicted values to a minimum value greater than \\(-1\\).\n\n\n\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                MSLE Objective Function\n\n            \n            \n                1import numpy as np\n2\n3\n4def lgbm_msle_objective(\n5    target: np.ndarray,\n6    prediction: np.ndarray,\n7    weight: np.ndarray = None,\n8):\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Ensure predictions are at least -1 + 1e-6 to avoid log(0)\n\n            \n            \n                9    prediction = np.maximum(prediction, -1 + 1e-6)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Gradient\n\n            \n            \n                10    gradient = (\n11        np.log1p(prediction) - np.log1p(target)\n12    ) / (prediction + 1)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Hessian\n\n            \n            \n                13    hessian = (\n14        -np.log1p(prediction) + np.log1p(target) + 1\n15    ) / ((prediction + 1) ** 2)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Apply sample weights\n\n            \n            \n                16    if weight is not None:\n17        gradient *= weight\n18        hessian *= weight\n19\n20    return gradient, hessian\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                MSLE Metric\n\n            \n            \n                21def lgbm_msle_metric(\n22    target: np.ndarray,\n23    prediction: np.ndarray,\n24    weight: np.ndarray = None,\n25):\n26    preds = np.maximum(prediction, -1 + 1e-6)\n27    squared_log_error = (\n28        np.log1p(preds) - np.log1p(target)\n29    ) ** 2\n30    msle = np.average(squared_log_error, weights=weight)\n31    return \"MSLE\", msle, False\n\n            \n        \n    \n        \n    \n\n\n\n\n2.4 Usage\nNow we can use the loss functions in LightGBM by setting the objective parameter in the model, and the evaluation metric by setting the eval_metric parameter in the fit method.\nimport lightgbm as lgb\n\nregressor = lgb.LGBMRegressor(objective=lgbm_msle_objective)\nregressor.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=lgbm_msle_metric)"
  },
  {
    "objectID": "blog/2024-08-11-gbt-custom-loss/index.html#catboost",
    "href": "blog/2024-08-11-gbt-custom-loss/index.html#catboost",
    "title": "Custom Loss Functions for LightGBM and CatBoost",
    "section": "3 CatBoost",
    "text": "3 CatBoost\n\n3.1 Interface\nThe CatBoost interface has a few differences with LightGBM:\n\nThe objective function and the evaluation metric are implemented as as class rather than a function, and must implement a few specific methods.\n\nCatBoost expects the negative gradient and hessian to be returned by the loss function.\n\nIt‚Äôs not necessary to use NumPy arrays for doing vectorized operations. Using for loops will suffice, since under the hood CatBoost will convert our function into machine code using Numba.\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nCatBoost requires the negative gradient and hessian to be returned by the loss function, so we need to apply a negation at the end of the calculations. Intuitively, this is because to minimize the loss function, we need to move in the opposite direction of the gradient, but I personally would‚Äôve prefrred CatBoost hanlded this internally similar to LightGBM and XGBoost.\n\n\n\nThe overall interface is as follows:\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                Objective Function\nCustom loss function for CatBoost.\n\n            \n            \n                1class CatBoostObjectiveFunction:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the derivatives (gradient and hessian) of the loss with respect to the predictions.\nParameters:\n\napproxes (list) : The predicted values from the model\ntargets (list) : The true target values\nweights (list), optional : Sample weights. If None, uniform weights are assumed\n\nReturns:\n\nList[Tuple[float, float]] : List of tuples, each containing the negative gradient and hessian for a sample\n\n\n            \n            \n                 2    def calc_ders_range(\n 3        self,\n 4        approxes: list,\n 5        targets: list,\n 6        weights: list = None,\n 7    ):\n 8        result = []\n 9        for index in range(len(targets)):\n10            grad_i = ...\n11            hess_i = ...\n12\n13            result.append((-grad_i, -hess_i))\n14        return result\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Evaluation Metric\nCustom evaluation metric for CatBoost.\n\n            \n            \n                15class CatBoostEvalMetric:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Parameters:\n\napproxes (List[List]) : List containing the predicted values from the model\ntarget (List) : The true target values\nweight (List), optional : Sample weights. If None, uniform weights are assumed.\n\nReturns:\nA tuple containing two elements:\n\nfloat : The accumulated weighted error\nfloat : The total weight\n\n\n            \n            \n                16    def evaluate(\n17        self,\n18        approxes: list[list],\n19        target: list,\n20        weight: list = None,\n21    ) -&gt; tuple[float, float]:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                I'm not sure why approxes is a list of lists in the first place, but\nwe'll just have to roll with it\n\n            \n            \n                22        assert len(approxes) == 1\n23        approx = approxes[0]\n24        assert len(target) == len(approx)\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the weighted error and total weight\n\n            \n            \n                25        error_sum = 0.0\n26        weight_sum = 0.0\n27\n28        for i in range(len(approx)):\n29            error_i = ...\n30            weight_i = (\n31                1.0 if weight is None else weight[i]\n32            )\n33            weight_sum += weight_i\n34            error_sum += weight_i * error_i\n35\n36        return error_sum, weight_sum\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the final metric value from the accumulated error and total weight\nParameters:\n\nerror (float) : The accumulated weighted error\nweight (float) : The total weight\n\nReturns:\n\nfloat : The final metric value\n\n\n            \n            \n                37    def get_final_error(\n38        self, error: float, weight: float\n39    ) -&gt; float:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Indicate whether a higher metric value is better.\nReturns:\n\nbool : True if higher metric values are better, False otherwise\n\n\n            \n            \n                40    def is_max_optimal(self) -&gt; bool:\n\n            \n        \n    \n        \n    \n\n\nIf you‚Äôre interested in checking out the implementation of CatBoost‚Äôs official loss functions, you can find the C++ code here. Took some digging to find! üòÑ\n\n\n3.2 Mean Squared Error\nWe previously derived the gradient (Equation¬†2) and hessian (Equation¬†3) of the MSE loss function for LightGBM. We can use the same equations for CatBoost, but we need to implement them as a class.\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                MSE Objective Function\nMean Squared Error objective function for CatBoost.\n\n            \n            \n                1class MSEObjective:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                 2    def calc_ders_range(\n 3        self,\n 4        approx: list,\n 5        target: list,\n 6        weights: list = None,\n 7    ):\n 8        result = []\n 9        for i in range(len(target)):\n10            grad_i = approx[i] - target[i]\n11            hess_i = 1.0\n12\n13            if weights is not None:\n14                grad_i *= weights[i]\n15                hess_i *= weights[i]\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Add the negation before appending to the result\n\n            \n            \n                16            result.append((-grad_i, -hess_i))\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                17        return result\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                MSE Evaluation Metric\nMean Squared Error metric for CatBoost.\n\n            \n            \n                18class MSEMetric:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                19    def evaluate(\n20        self,\n21        approxes: list[list],\n22        target: list,\n23        weight: list = None,\n24    ) -&gt; tuple[float, float]:\n25        assert len(approxes) == 1\n26        approx = approxes[0]\n27        assert len(target) == len(approx)\n28\n29        error_sum = 0.0\n30        weight_sum = 0.0\n31\n32        for i in range(len(approx)):\n33            error_i = (approx[i] - target[i]) ** 2\n34            weight_i = (\n35                1.0 if weight is None else weight[i]\n36            )\n37            weight_sum += weight_i\n38            error_sum += weight_i * error_i\n39\n40        return error_sum, weight_sum\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the final metric value from the accumulated error and total weight\n\n            \n            \n                41    def get_final_error(\n42        self, error: float, weight: float\n43    ) -&gt; float:\n44        return error / weight\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Indicate whether a higher metric value is better. MSE is an error metric, so lower is better.\n\n            \n            \n                45    def is_max_optimal(self) -&gt; bool:\n46        return False\n\n            \n        \n    \n        \n    \n\n\n\n\n3.3 Mean Squared Logarithmic Error\nBased on the derivations from the LightGBM section (Equation¬†5 and Equation¬†6), we can implement the MSLE objective function for CatBoost:\n\n\n\n        \n            \n        \n            \n                \n                    #\n                \n                MSLE Objective Function\nMean Squared Logarithmic Error objective function for CatBoost.\n\n            \n            \n                1import numpy as np\n2\n3\n4class MSLEObjective:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                 5    def calc_ders_range(\n 6        self,\n 7        approx: list,\n 8        target: list,\n 9        weights: list = None,\n10    ):\n11        result = []\n12        for i in range(len(target)):\n13            approx_i = max(approx[i], -1 + 1e-6)\n14            grad_i = (\n15                np.log1p(approx_i) - np.log1p(target[i])\n16            ) / (approx_i + 1)\n17            hess_i = (\n18                -np.log1p(approx_i)\n19                + np.log1p(target[i])\n20                + 1\n21            ) / ((approx_i + 1) ** 2)\n22\n23            if weights is not None:\n24                grad_i *= weights[i]\n25                hess_i *= weights[i]\n26\n27            result.append((-grad_i, -hess_i))\n28        return result\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                MSLE Evaluation Metric\nMean Squared Logarithmic Error metric for CatBoost.\n\n            \n            \n                29class MSLEMetric:\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                \n            \n            \n                30    def evaluate(\n31        self,\n32        approxes: list[list],\n33        target: list,\n34        weight: list = None,\n35    ) -&gt; tuple[float, float]:\n36        assert len(approxes) == 1\n37        approx = approxes[0]\n38        assert len(target) == len(approx)\n39\n40        error_sum = 0.0\n41        weight_sum = 0.0\n42\n43        for i in range(len(approx)):\n44            approx_i = max(approx[i], -1 + 1e-6)\n45            error_i = (\n46                np.log1p(approx_i) - np.log1p(target[i])\n47            ) ** 2\n48            weight_i = (\n49                1.0 if weight is None else weight[i]\n50            )\n51\n52            weight_sum += weight_i\n53            error_sum += weight_i * error_i\n54\n55        return error_sum, weight_sum\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Calculate the final metric value from the accumulated error and total weight\n\n            \n            \n                56    def get_final_error(\n57        self, error: float, weight: float\n58    ) -&gt; float:\n59        return error / weight\n\n            \n        \n    \n\n        \n            \n                \n                    #\n                \n                Indicate whether a higher metric value is better. MSLE is an error metric, so lower is better.\n\n            \n            \n                60    def is_max_optimal(self) -&gt; bool:\n61        return False\n\n            \n        \n    \n        \n    \n\n\n\n\n3.4 Usage\nTo use the custom loss functions in CatBoost, we need to pass it as a parameter during model initialization.\nimport catboost as cb\n\nregressor = cb.CatBoostRegressor(loss_function=MSLEObjective(), eval_metric=MSLEMetric())\nNote that since the functions are implemented as classes, we need to instantiate them before passing them to the model. I fell victim to this mistake a few times, so be careful!"
  },
  {
    "objectID": "publication/2023-07-01-tractable-reinforcement-learning/index.html",
    "href": "publication/2023-07-01-tractable-reinforcement-learning/index.html",
    "title": "Tractable large-scale deep reinforcement learning",
    "section": "",
    "text": "Reinforcement learning (RL) has emerged as one of the most promising and powerful techniques in deep learning. The training of intelligent agents requires a myriad of training examples which imposes a substantial computational cost. Consequently, RL is seldom applied to real-world problems and historically has been limited to computer vision tasks, similar to supervised learning. This work proposes an RL framework for complex, partially observable, large-scale environments. We introduce novel techniques for tractable training on commodity GPUs, and significantly reduce computational costs. Furthermore, we present a self-supervised loss that improves the learning stability in applications with a long-time horizon, shortening the training time. We demonstrate the effectiveness of the proposed solution on the application of road extraction from high-resolution satellite images. We present experiments on satellite images of fifteen cities that demonstrate comparable performance to state-of-the-art methods. To the best of our knowledge, this is the first time RL has been applied for extracting road networks. The code is publicly available at https://github.com/nsarang/road-extraction-rl."
  },
  {
    "objectID": "publication/2023-07-01-tractable-reinforcement-learning/index.html#abstract",
    "href": "publication/2023-07-01-tractable-reinforcement-learning/index.html#abstract",
    "title": "Tractable large-scale deep reinforcement learning",
    "section": "",
    "text": "Reinforcement learning (RL) has emerged as one of the most promising and powerful techniques in deep learning. The training of intelligent agents requires a myriad of training examples which imposes a substantial computational cost. Consequently, RL is seldom applied to real-world problems and historically has been limited to computer vision tasks, similar to supervised learning. This work proposes an RL framework for complex, partially observable, large-scale environments. We introduce novel techniques for tractable training on commodity GPUs, and significantly reduce computational costs. Furthermore, we present a self-supervised loss that improves the learning stability in applications with a long-time horizon, shortening the training time. We demonstrate the effectiveness of the proposed solution on the application of road extraction from high-resolution satellite images. We present experiments on satellite images of fifteen cities that demonstrate comparable performance to state-of-the-art methods. To the best of our knowledge, this is the first time RL has been applied for extracting road networks. The code is publicly available at https://github.com/nsarang/road-extraction-rl."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nima Sarang",
    "section": "",
    "text": "Welcome to my personal website! This is where I share what I‚Äôm up to - my latest projects, random musings, and adventures.\nI‚Äôm always eager to learn new things and connect with people. If you‚Äôre up for a collaboration or just want to chat, drop me a line!\nLearn more about me ‚Üí"
  },
  {
    "objectID": "project/2019-06-12-dc-gan/index.html",
    "href": "project/2019-06-12-dc-gan/index.html",
    "title": "DCGAN on MNIST",
    "section": "",
    "text": "A Keras implementation of a custom Deep Convolutional Generative Adversarial Networks (DCGAN) on MNIST dataset. The code is based on the Francois Chollet‚Äôs Deep Learning with Python, page 308.\nDCGAN is an extension of the GAN architecture, where the generator and discriminator are deep convolutional networks. The discriminator that takes an image as input and outputs a single scalar value representing the probability that the input image is real (as opposed to fake). The generator takes a random vector as input and decodes it into a synthetic image, and is trained to generate images that the discriminator identifies as real.\nThe code is available on my GitHub.\n\nWhat worked\n\nUsing dropout in the discriminator\n\nAdding noise to the labels\n\nUsing strided convolutions\n\nNormalizing the data to \\([-1, 1]\\)\nRMSprop optimizer\n\n\n\nWhat didn‚Äôt work\n\nUsing high learning rates (&gt;1e-3)\nI initially normalized the data between 0 and 1, but since I was using ‚Äútanh‚Äù as the last layer of the generator‚Äôs output, it made the convergence a lot harder\nConstructing separate batches for real and fake images\nTraining the discriminator with more steps than the generator\nUsing SGD optimizer\n\n\n\nResults\nThis is the first 5000 iterations of the training history. The random vectors are fixed at the beginning of the training. As the training progresses, the generator learns to generate more realistic images."
  },
  {
    "objectID": "project/2017-09-17-d3-force-graph-bio-networks/index.html",
    "href": "project/2017-09-17-d3-force-graph-bio-networks/index.html",
    "title": "D3 Force Layout - Biological Networks",
    "section": "",
    "text": "While working as a research assistant at the Computational Biology Research Center at the Amirkabir University of Technology, I was tasked with implmenting an algorithm for Identification of large disjoint motifs in biological networks. We soon realized we needed a visualization tool to help us better understand the networks we were working with, and the motifs that were identified. This was when I stumbled on the D3.js library, and decided to use its force layout for interactive visualization. The tool below is based on the countless examples and tutrials I found, which I‚Äôve lost track of, and should give a glimpse of the power of D3.js.\nThis example is a visualization of protein-protein interaction network. Its main features are:\nThe source code can be found here. The visualization code is in the output folder."
  },
  {
    "objectID": "project/2017-09-17-d3-force-graph-bio-networks/index.html#network-details",
    "href": "project/2017-09-17-d3-force-graph-bio-networks/index.html#network-details",
    "title": "D3 Force Layout - Biological Networks",
    "section": "Network Details",
    "text": "Network Details\nOrder: \nSize: \nComponents:"
  },
  {
    "objectID": "project/2017-09-17-d3-force-graph-bio-networks/index.html#pattern-details",
    "href": "project/2017-09-17-d3-force-graph-bio-networks/index.html#pattern-details",
    "title": "D3 Force Layout - Biological Networks",
    "section": "Pattern Details",
    "text": "Pattern Details\nOrder: \nSize: \nFrequency:"
  },
  {
    "objectID": "project/2017-09-17-d3-force-graph-bio-networks/index.html#settings",
    "href": "project/2017-09-17-d3-force-graph-bio-networks/index.html#settings",
    "title": "D3 Force Layout - Biological Networks",
    "section": "Settings",
    "text": "Settings\n\nNode Charge  \n\n\nLink Strength  \n\n\nNode Radius  \n\n\nLink Width  \n\n\nVelocity Decay  \n\n\nCenter Gravity  \n\n   Force Static   FishEye Distortion Zoom Function Pin on Drop"
  },
  {
    "objectID": "project/2017-09-17-d3-force-graph-bio-networks/index.html#infoTitle",
    "href": "project/2017-09-17-d3-force-graph-bio-networks/index.html#infoTitle",
    "title": "D3 Force Layout - Biological Networks",
    "section": "About",
    "text": "About"
  },
  {
    "objectID": "blog/2024-09-29-llama-in-browser/index.html",
    "href": "blog/2024-09-29-llama-in-browser/index.html",
    "title": "Running Llama 3.2 in the Browser!",
    "section": "",
    "text": "1 Introduction\nRecently, I came across a few cool projects that allow running LLMs (large language models) in the browser, and utilize GPU for fast inference! This is exciting for two main reasons: first, it‚Äôs great to run LLMs locally without sending your data to any external server. You can use custom prompts, configure the inference the way you want, and ensure your privacy. Second, you don‚Äôt have to deal with setting up Python environments, installing dependencies, and configuring the GPU. The browser takes care of all that! It‚Äôs incredibly convenient. With the release of smaller Llama 3.2 models, like the 1B parameter version, it‚Äôs now possible to generate quality text directly in your browser, perhaps even matching ChatGPT 3.5 on some tasks.\nThe chat UI below will allow you to download a number of models hosted on HuggingFace, including Llama 3 variants. Once you download the model, you can disconnect and continue chatting offline. I‚Äôve tried adding a bunch of customizations to play around, but the UI is meant more as a demonstration to showcase the potential of a web UI.\n\n\n2 Chat UI\nYour browser must support WebGPU for a fast inference experience. You can see the list of browsers that support it here. I personally tested it on Apple M1-M3 and Nvidia GPUs, and it worked quite well. If WebGPU is not supported, the engine will fall back to WebAssembly, which is slower but still functional.\n\n\n\n\n\n\nNote\n\n\n\nBased on your browser‚Äôs information, you have the following backend ‚òû \nFor more info, visit https://webgpureport.org/.\nIf you see ‚ÄúWebGPU not supported‚Äù in the status above but you‚Äôre certain your browser supports it, you can leave a comment below and I‚Äôll try to help you troubleshoot the issue.\n\n\n\n\n  \n    Model Settings\n    \n      Model:\n      ‚ìò\n      \n      \n       First download may take a little bit. Subsequent loads will read from cache.\n      \n        \n        \n      \n    \n    Initialize Model\n    \n        \n        \n    \n    \n      \n        Temperature:\n        ‚ìò\n      \n      \n        \n        0.7\n      \n    \n    \n      \n        Max Tokens:\n        ‚ìò\n      \n      \n    \n    \n      \n        Top P:\n        ‚ìò\n      \n      \n        \n        0.9\n      \n    \n    \n      \n        Frequency Penalty:\n        ‚ìò\n      \n      \n        \n        0\n      \n    \n    \n      \n        Presence Penalty:\n        ‚ìò\n      \n      \n        \n        0\n      \n    \n  \n  \n    \n      My Chat\n      \n        \n      \n    \n    \n    \n    \n      \n      \n        \n      \n    \n  \n\n\n\n\n3 How It Works\nThe code above runs using WebLLM, a project that enables running LLMs in browser without requiring a server. It‚Äôs built on top of the MLC-LLM (Machine Learning Compiler for LLM) framework, which optimizes models for efficient execution on various hardware platforms. MLC-LLM itself is an application of Apache TVM, a machine learning compiler stack that takes in pre-trained models, compiles and generates deployable modules that can be embedded and run everywhere. MLC-LLM specializes in LLM-specific graph transformations and optimized kernels for common operations.\nOne of the major advantages of WebLLM is that it can utilize WebGPU for accelerated inference. WebGPU is a web standard provided by most modern browsers that allows developers low-level access to the GPU for general-purpose computing tasks. It‚Äôs a new feature that is being adopted by browsers.\n\n// Import WebLLM\nimport * as webllm from 'https://esm.run/@mlc-ai/web-llm';\n\n// Initialize the engine\nengine = new webllm.MLCEngine();\n// Download and initialize the model or load from cache if available\nawait engine.reload('Llama-3.2-1B-Instruct-q4f32_1-MLC', config);\n// List of available models can be found in\nconsole.log(webllm.prebuiltAppConfig.model_list);\n\n\nWith the engine initialized, I use the following to generate responses:\n\n/**\n * Generate a response using the engine\n * @param {Array} messages - Array of conversation history. Each message is a \n     dictionary with keys `role` and `content`.\n     - `role`: `user` or `assistant` or `system`. The prompt is encoded as the\n        first message with the role `system`.\n     - `content`: the message content\n * @param {Function} onUpdate - Callback function to update the UI with the generated message\n * @param {Function} onFinish - Callback function to handle the final message\n * @param {Function} onError - Callback function to handle errors    \n */\nconst streamingGenerating = async (messages, onUpdate, onFinish, onError) =&gt; {\n    try {\n        let curMessage = \"\";\n        let usage;\n        // The model configuration such as temperature, max_tokens, etc.\n        const config = modelConfig.getConfig();\n        const completion = await engine.chat.completions.create({\n            stream: true,\n            messages,\n            ...config,\n            stream_options: { include_usage: true },\n        });\n        // Stream the completion\n        for await (const chunk of completion) {\n            const curDelta = chunk.choices[0]?.delta.content;\n            if (curDelta) {\n                curMessage += curDelta;\n            }\n            if (chunk.usage) {\n                usage = chunk.usage;\n            }\n            // Update the UI\n            onUpdate(curMessage);\n        }\n        // Get the final message\n        const finalMessage = await engine.getMessage();\n        onFinish(finalMessage, usage);\n    } catch (err) {\n        onError(err);\n    }\n};\n\nYou learn more about the other serve options and how to use them in the documentation.\n\n\n4 Final Thoughts\nAs of writing this I can WebLLM is the fastest library to run chat LLMs over web. For other tasks like speech to text, image generation, etc., you can look into the combination of Transformers.js and ONNX Runtime Web, or just ONNX Runtime Web if you‚Äôre looking for a more general-purpose solution.If you have any questions or feedback, feel free to leave a comment below.\n\n\n5 Change Log\n\n2024/10/12: Added ‚ÄúHow It Works‚Äù section.\n2024/10/04: Added Markdown support for the output messages.\n2024/10/02: Added download size estimation for the models from the HF repo.\n2024/10/01: Model configuration settings added to the UI with tooltips.\n2024/09/29: Initial release.\n\n\n\n\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{sarang2024,\n  author = {Sarang, Nima},\n  title = {Running {Llama} 3.2 in the {Browser!}},\n  date = {2024-10-06},\n  url = {https://www.nimasarang.com/blog/2024-09-29-llama-in-browser/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSarang, Nima. 2024. ‚ÄúRunning Llama 3.2 in the Browser!‚Äù\nOctober 6, 2024. https://www.nimasarang.com/blog/2024-09-29-llama-in-browser/."
  }
]