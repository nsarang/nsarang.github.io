<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nima Sarang">
<meta name="dcterms.date" content="2025-08-07">
<meta name="description" content="A collection of useful tips and tricks for better leveraging GBT algorithms">

<title>GBT Algorithms and Tips – Nima Sarang</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../_assets/favicon.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-74902e5a8cbd768d28bd07d6d8042d58.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-aa6766b93b57ee48e38855e0f300849d.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-66663b93f90c1ea2d93231846f8aa662.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-708d3faa3aeb82d854165159d4bb2526.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-HWSGHN8N8N"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-HWSGHN8N8N', { 'anonymize_ip': true});
</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">

<!-- ==== Legacy ==== -->
<link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@300;400;700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Petrona:wght@400;500&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Red+Hat+Text:ital,wght@0,400;0,500;1,400;1,500&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=B612:ital,wght@0,400;0,700;1,400;1,700&amp;family=B612+Mono:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet">

<!-- ==== Main ==== -->
<link href="https://fonts.googleapis.com/css2?family=PT+Serif:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:ital,opsz,wght@0,8..60,200..900;1,8..60,200..900&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400..700;1,400..700&amp;display=swap" rel="stylesheet">
<!-- Source Serif Pro -->
<link href="https://use.typekit.net/sie7yap.css" rel="stylesheet">

<!-- ==== Code ==== -->
<!-- JetBrains Mono -->
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&amp;display=swap" rel="stylesheet">
<!-- Fira Code -->
<link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&amp;display=swap" rel="stylesheet">
<!-- Roboto Mono -->
<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&amp;display=swap" rel="stylesheet">
<meta name="quarto:status" content="draft">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="GBT Algorithms and Tips">
<meta property="og:description" content="A collection of useful tips and tricks for better leveraging GBT algorithms">
<meta property="og:image" content="https://www.nimasarang.com/blog/2025-10-17-gbt-algorithms/featured.png">
<meta property="og:site_name" content="Nima Sarang">
<meta property="og:image:height" content="2990">
<meta property="og:image:width" content="4167">
<meta name="twitter:title" content="GBT Algorithms and Tips">
<meta name="twitter:description" content="A collection of useful tips and tricks for better leveraging GBT algorithms">
<meta name="twitter:image" content="https://www.nimasarang.com/blog/2025-10-17-gbt-algorithms/featured.png">
<meta name="twitter:image-height" content="2990">
<meta name="twitter:image-width" content="4167">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="GBT Algorithms and Tips">
<meta name="citation_author" content="Nima Sarang">
<meta name="citation_publication_date" content="2025-08-07">
<meta name="citation_cover_date" content="2025-08-07">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-08-07">
<meta name="citation_fulltext_html_url" content="https://www.nimasarang.com/blog/2025-10-17-gbt-algorithms/">
<meta name="citation_language" content="en">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../_assets/favicon.svg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Nima Sarang</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../project/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publication/index.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://www.github.com/nsarang" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.linkedin.com/in/nima-sarang" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-linkedin"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns">
<div id="title-block-header-title" class="quarto-title page-columns page-full page-layout-full featured-image p-4" style="background-image: url(featured.png), url(featured.jpg), url(../featured.jpg); background-repeat: no-repeat;">
<h1 class="title">GBT Algorithms and Tips</h1>
    <div class="quarto-categories">
        <div class="quarto-category">GitHub</div>
        <div class="quarto-category">DevOps</div>
        <div class="quarto-category">CI/CD</div>
        <div class="quarto-category">Automation</div>
      </div>
  </div>

<div>
  <div id="title-block-title-desc" class="description pt-4">
    A collection of useful tips and tricks for better leveraging GBT algorithms
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Nima Sarang </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  



</header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="1">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#complete-summary-gradient-boosting-discussion" id="toc-complete-summary-gradient-boosting-discussion" class="nav-link active" data-scroll-target="#complete-summary-gradient-boosting-discussion"><span class="header-section-number">1</span> Complete Summary: Gradient Boosting Discussion</a>
  <ul class="collapse">
  <li><a href="#part-1-mathematical-foundations" id="toc-part-1-mathematical-foundations" class="nav-link" data-scroll-target="#part-1-mathematical-foundations"><span class="header-section-number">1.1</span> Part 1: Mathematical Foundations</a>
  <ul class="collapse">
  <li><a href="#taylor-expansion-for-loss-functions" id="toc-taylor-expansion-for-loss-functions" class="nav-link" data-scroll-target="#taylor-expansion-for-loss-functions"><span class="header-section-number">1.1.1</span> Taylor Expansion for Loss Functions</a></li>
  </ul></li>
  <li><a href="#part-2-core-gradient-boosting-concepts" id="toc-part-2-core-gradient-boosting-concepts" class="nav-link" data-scroll-target="#part-2-core-gradient-boosting-concepts"><span class="header-section-number">1.2</span> Part 2: Core Gradient Boosting Concepts</a>
  <ul class="collapse">
  <li><a href="#why-gradient-boosting-generalizes-beyond-mse" id="toc-why-gradient-boosting-generalizes-beyond-mse" class="nav-link" data-scroll-target="#why-gradient-boosting-generalizes-beyond-mse"><span class="header-section-number">1.2.1</span> Why Gradient Boosting Generalizes Beyond MSE</a></li>
  <li><a href="#why-fit-a-model-to-gradients" id="toc-why-fit-a-model-to-gradients" class="nav-link" data-scroll-target="#why-fit-a-model-to-gradients"><span class="header-section-number">1.2.2</span> Why Fit a Model to Gradients?</a></li>
  </ul></li>
  <li><a href="#part-3-baseline-gradient-boosting-friedman-2001" id="toc-part-3-baseline-gradient-boosting-friedman-2001" class="nav-link" data-scroll-target="#part-3-baseline-gradient-boosting-friedman-2001"><span class="header-section-number">1.3</span> Part 3: Baseline Gradient Boosting (Friedman 2001)</a>
  <ul class="collapse">
  <li><a href="#conceptual-order" id="toc-conceptual-order" class="nav-link" data-scroll-target="#conceptual-order"><span class="header-section-number">1.3.1</span> Conceptual Order</a></li>
  <li><a href="#algorithm-first-order-gradient-boosting" id="toc-algorithm-first-order-gradient-boosting" class="nav-link" data-scroll-target="#algorithm-first-order-gradient-boosting"><span class="header-section-number">1.3.2</span> Algorithm (First-Order Gradient Boosting)</a></li>
  <li><a href="#part-3.1-initial-prediction-f0" id="toc-part-3.1-initial-prediction-f0" class="nav-link" data-scroll-target="#part-3.1-initial-prediction-f0"><span class="header-section-number">1.3.3</span> Part 3.1: Initial Prediction <span class="math inline">\(F^0\)</span></a></li>
  <li><a href="#part-3.2-learning-rate-shrinkage-theory" id="toc-part-3.2-learning-rate-shrinkage-theory" class="nav-link" data-scroll-target="#part-3.2-learning-rate-shrinkage-theory"><span class="header-section-number">1.3.4</span> Part 3.2: Learning Rate (Shrinkage) Theory</a></li>
  </ul></li>
  <li><a href="#part-4-xgboosts-improvements" id="toc-part-4-xgboosts-improvements" class="nav-link" data-scroll-target="#part-4-xgboosts-improvements"><span class="header-section-number">1.4</span> Part 4: XGBoost’s Improvements</a>
  <ul class="collapse">
  <li><a href="#from-gradient-descent-to-newton-raphson" id="toc-from-gradient-descent-to-newton-raphson" class="nav-link" data-scroll-target="#from-gradient-descent-to-newton-raphson"><span class="header-section-number">1.4.1</span> From Gradient Descent to Newton-Raphson</a></li>
  <li><a href="#why-newtons-method-is-superior" id="toc-why-newtons-method-is-superior" class="nav-link" data-scroll-target="#why-newtons-method-is-superior"><span class="header-section-number">1.4.2</span> Why Newton’s Method Is Superior</a></li>
  <li><a href="#the-complete-xgboost-objective" id="toc-the-complete-xgboost-objective" class="nav-link" data-scroll-target="#the-complete-xgboost-objective"><span class="header-section-number">1.4.3</span> The Complete XGBoost Objective</a></li>
  <li><a href="#taylor-approximation-making-it-tractable" id="toc-taylor-approximation-making-it-tractable" class="nav-link" data-scroll-target="#taylor-approximation-making-it-tractable"><span class="header-section-number">1.4.4</span> Taylor Approximation (Making It Tractable)</a></li>
  <li><a href="#deriving-optimal-leaf-values" id="toc-deriving-optimal-leaf-values" class="nav-link" data-scroll-target="#deriving-optimal-leaf-values"><span class="header-section-number">1.4.5</span> Deriving Optimal Leaf Values</a></li>
  <li><a href="#plugging-optimal-weights-back" id="toc-plugging-optimal-weights-back" class="nav-link" data-scroll-target="#plugging-optimal-weights-back"><span class="header-section-number">1.4.6</span> Plugging Optimal Weights Back</a></li>
  <li><a href="#computing-gain-from-splits" id="toc-computing-gain-from-splits" class="nav-link" data-scroll-target="#computing-gain-from-splits"><span class="header-section-number">1.4.7</span> Computing Gain from Splits</a></li>
  <li><a href="#complete-xgboost-algorithm-one-iteration" id="toc-complete-xgboost-algorithm-one-iteration" class="nav-link" data-scroll-target="#complete-xgboost-algorithm-one-iteration"><span class="header-section-number">1.4.8</span> Complete XGBoost Algorithm (One Iteration)</a></li>
  <li><a href="#concrete-examples" id="toc-concrete-examples" class="nav-link" data-scroll-target="#concrete-examples"><span class="header-section-number">1.4.9</span> Concrete Examples</a></li>
  <li><a href="#connection-to-weighted-least-squares" id="toc-connection-to-weighted-least-squares" class="nav-link" data-scroll-target="#connection-to-weighted-least-squares"><span class="header-section-number">1.4.10</span> Connection to Weighted Least Squares</a></li>
  </ul></li>
  <li><a href="#part-5-handling-missing-values" id="toc-part-5-handling-missing-values" class="nav-link" data-scroll-target="#part-5-handling-missing-values"><span class="header-section-number">1.5</span> Part 5: Handling Missing Values</a>
  <ul class="collapse">
  <li><a href="#the-xgboost-approach" id="toc-the-xgboost-approach" class="nav-link" data-scroll-target="#the-xgboost-approach"><span class="header-section-number">1.5.1</span> The XGBoost Approach</a></li>
  </ul></li>
  <li><a href="#part-6-histogram-based-split-finding" id="toc-part-6-histogram-based-split-finding" class="nav-link" data-scroll-target="#part-6-histogram-based-split-finding"><span class="header-section-number">1.6</span> Part 6: Histogram-Based Split Finding</a>
  <ul class="collapse">
  <li><a href="#exact-greedy-baseline" id="toc-exact-greedy-baseline" class="nav-link" data-scroll-target="#exact-greedy-baseline"><span class="header-section-number">1.6.1</span> Exact Greedy (Baseline)</a></li>
  <li><a href="#histogram-based-xgboostlightgbm" id="toc-histogram-based-xgboostlightgbm" class="nav-link" data-scroll-target="#histogram-based-xgboostlightgbm"><span class="header-section-number">1.6.2</span> Histogram-Based (XGBoost/LightGBM)</a></li>
  </ul></li>
  <li><a href="#part-7-tree-growth-strategies" id="toc-part-7-tree-growth-strategies" class="nav-link" data-scroll-target="#part-7-tree-growth-strategies"><span class="header-section-number">1.7</span> Part 7: Tree Growth Strategies</a>
  <ul class="collapse">
  <li><a href="#level-wise-xgboost-default-traditional" id="toc-level-wise-xgboost-default-traditional" class="nav-link" data-scroll-target="#level-wise-xgboost-default-traditional"><span class="header-section-number">1.7.1</span> Level-wise (XGBoost default, traditional)</a></li>
  <li><a href="#leaf-wise-lightgbm-best-first" id="toc-leaf-wise-lightgbm-best-first" class="nav-link" data-scroll-target="#leaf-wise-lightgbm-best-first"><span class="header-section-number">1.7.2</span> Leaf-wise (LightGBM, best-first)</a></li>
  </ul></li>
  <li><a href="#part-8-subsampling-and-regularization" id="toc-part-8-subsampling-and-regularization" class="nav-link" data-scroll-target="#part-8-subsampling-and-regularization"><span class="header-section-number">1.8</span> Part 8: Subsampling and Regularization</a>
  <ul class="collapse">
  <li><a href="#row-subsampling-stochastic-gradient-boosting" id="toc-row-subsampling-stochastic-gradient-boosting" class="nav-link" data-scroll-target="#row-subsampling-stochastic-gradient-boosting"><span class="header-section-number">1.8.1</span> Row Subsampling (Stochastic Gradient Boosting)</a></li>
  <li><a href="#column-subsampling" id="toc-column-subsampling" class="nav-link" data-scroll-target="#column-subsampling"><span class="header-section-number">1.8.2</span> Column Subsampling</a></li>
  <li><a href="#regularization-parameters-summary" id="toc-regularization-parameters-summary" class="nav-link" data-scroll-target="#regularization-parameters-summary"><span class="header-section-number">1.8.3</span> Regularization Parameters Summary</a></li>
  </ul></li>
  <li><a href="#part-9-lightgbm-contributions" id="toc-part-9-lightgbm-contributions" class="nav-link" data-scroll-target="#part-9-lightgbm-contributions"><span class="header-section-number">1.9</span> Part 9: LightGBM Contributions</a>
  <ul class="collapse">
  <li><a href="#main-algorithmic-innovations" id="toc-main-algorithmic-innovations" class="nav-link" data-scroll-target="#main-algorithmic-innovations"><span class="header-section-number">1.9.1</span> Main Algorithmic Innovations</a></li>
  </ul></li>
  <li><a href="#part-10-catboost-contributions" id="toc-part-10-catboost-contributions" class="nav-link" data-scroll-target="#part-10-catboost-contributions"><span class="header-section-number">1.10</span> Part 10: CatBoost Contributions</a>
  <ul class="collapse">
  <li><a href="#problem-1-target-leakage-in-categorical-features" id="toc-problem-1-target-leakage-in-categorical-features" class="nav-link" data-scroll-target="#problem-1-target-leakage-in-categorical-features"><span class="header-section-number">1.10.1</span> Problem 1: Target Leakage in Categorical Features</a></li>
  <li><a href="#problem-2-prediction-shift-in-gradient-boosting" id="toc-problem-2-prediction-shift-in-gradient-boosting" class="nav-link" data-scroll-target="#problem-2-prediction-shift-in-gradient-boosting"><span class="header-section-number">1.10.2</span> Problem 2: Prediction Shift in Gradient Boosting</a></li>
  <li><a href="#other-catboost-details" id="toc-other-catboost-details" class="nav-link" data-scroll-target="#other-catboost-details"><span class="header-section-number">1.10.3</span> Other CatBoost Details</a></li>
  </ul></li>
  <li><a href="#part-11-multiclass-classification" id="toc-part-11-multiclass-classification" class="nav-link" data-scroll-target="#part-11-multiclass-classification"><span class="header-section-number">1.11</span> Part 11: Multiclass Classification</a>
  <ul class="collapse">
  <li><a href="#approach-1-one-vs-all-ova" id="toc-approach-1-one-vs-all-ova" class="nav-link" data-scroll-target="#approach-1-one-vs-all-ova"><span class="header-section-number">1.11.1</span> Approach 1: One-vs-All (OvA)</a></li>
  <li><a href="#approach-2-softmax-xgboostlightgbm-default" id="toc-approach-2-softmax-xgboostlightgbm-default" class="nav-link" data-scroll-target="#approach-2-softmax-xgboostlightgbm-default"><span class="header-section-number">1.11.2</span> Approach 2: Softmax (XGBoost/LightGBM default)</a></li>
  </ul></li>
  <li><a href="#part-12-feature-importance" id="toc-part-12-feature-importance" class="nav-link" data-scroll-target="#part-12-feature-importance"><span class="header-section-number">1.12</span> Part 12: Feature Importance</a>
  <ul class="collapse">
  <li><a href="#gain-based-importance-default-in-xgboost" id="toc-gain-based-importance-default-in-xgboost" class="nav-link" data-scroll-target="#gain-based-importance-default-in-xgboost"><span class="header-section-number">1.12.1</span> 1. Gain-based Importance (Default in XGBoost)</a></li>
  <li><a href="#cover-based-importance" id="toc-cover-based-importance" class="nav-link" data-scroll-target="#cover-based-importance"><span class="header-section-number">1.12.2</span> 2. Cover-based Importance</a></li>
  <li><a href="#frequency-based-importance-weight" id="toc-frequency-based-importance-weight" class="nav-link" data-scroll-target="#frequency-based-importance-weight"><span class="header-section-number">1.12.3</span> 3. Frequency-based Importance (Weight)</a></li>
  <li><a href="#permutation-importance-model-agnostic" id="toc-permutation-importance-model-agnostic" class="nav-link" data-scroll-target="#permutation-importance-model-agnostic"><span class="header-section-number">1.12.4</span> 4. Permutation Importance (Model-agnostic)</a></li>
  <li><a href="#shap-values-shapley-additive-explanations" id="toc-shap-values-shapley-additive-explanations" class="nav-link" data-scroll-target="#shap-values-shapley-additive-explanations"><span class="header-section-number">1.12.5</span> 5. SHAP Values (Shapley Additive Explanations)</a></li>
  <li><a href="#comparison-summary" id="toc-comparison-summary" class="nav-link" data-scroll-target="#comparison-summary"><span class="header-section-number">1.12.6</span> Comparison Summary</a></li>
  </ul></li>
  <li><a href="#part-13-early-stopping" id="toc-part-13-early-stopping" class="nav-link" data-scroll-target="#part-13-early-stopping"><span class="header-section-number">1.13</span> Part 13: Early Stopping</a>
  <ul class="collapse">
  <li><a href="#basic-early-stopping" id="toc-basic-early-stopping" class="nav-link" data-scroll-target="#basic-early-stopping"><span class="header-section-number">1.13.1</span> Basic Early Stopping</a></li>
  <li><a href="#key-parameters" id="toc-key-parameters" class="nav-link" data-scroll-target="#key-parameters"><span class="header-section-number">1.13.2</span> Key Parameters</a></li>
  <li><a href="#advanced-cross-validation-with-early-stopping" id="toc-advanced-cross-validation-with-early-stopping" class="nav-link" data-scroll-target="#advanced-cross-validation-with-early-stopping"><span class="header-section-number">1.13.3</span> Advanced: Cross-Validation with Early Stopping</a></li>
  </ul></li>
  <li><a href="#part-14-implementation-checklist" id="toc-part-14-implementation-checklist" class="nav-link" data-scroll-target="#part-14-implementation-checklist"><span class="header-section-number">1.14</span> Part 14: Implementation Checklist</a>
  <ul class="collapse">
  <li><a href="#baseline-gradient-boosting" id="toc-baseline-gradient-boosting" class="nav-link" data-scroll-target="#baseline-gradient-boosting"><span class="header-section-number">1.14.1</span> Baseline Gradient Boosting</a></li>
  <li><a href="#xgboost-style-second-order" id="toc-xgboost-style-second-order" class="nav-link" data-scroll-target="#xgboost-style-second-order"><span class="header-section-number">1.14.2</span> XGBoost-style (Second-order)</a></li>
  <li><a href="#histogram-based-optimization" id="toc-histogram-based-optimization" class="nav-link" data-scroll-target="#histogram-based-optimization"><span class="header-section-number">1.14.3</span> Histogram-based Optimization</a></li>
  <li><a href="#loss-functions-implementation" id="toc-loss-functions-implementation" class="nav-link" data-scroll-target="#loss-functions-implementation"><span class="header-section-number">1.14.4</span> Loss Functions Implementation</a></li>
  <li><a href="#catboost-ordered-target-statistics" id="toc-catboost-ordered-target-statistics" class="nav-link" data-scroll-target="#catboost-ordered-target-statistics"><span class="header-section-number">1.14.5</span> CatBoost Ordered Target Statistics</a></li>
  <li><a href="#catboost-ordered-boosting-conceptual" id="toc-catboost-ordered-boosting-conceptual" class="nav-link" data-scroll-target="#catboost-ordered-boosting-conceptual"><span class="header-section-number">1.14.6</span> CatBoost Ordered Boosting (Conceptual)</a></li>
  </ul></li>
  <li><a href="#part-15-summary-of-differences" id="toc-part-15-summary-of-differences" class="nav-link" data-scroll-target="#part-15-summary-of-differences"><span class="header-section-number">1.15</span> Part 15: Summary of Differences</a></li>
  <li><a href="#key-takeaways-for-implementation" id="toc-key-takeaways-for-implementation" class="nav-link" data-scroll-target="#key-takeaways-for-implementation"><span class="header-section-number">1.16</span> Key Takeaways for Implementation</a></li>
  </ul></li>
  </ul>
<div class="quarto-code-links"><h2>Code Links</h2><ul><li><a href="https://github.com/nsarang/github_workflows_tip_and_tricks"><i class="bi bi-file-code"></i>Full Examples</a></li></ul></div></nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">
<style>
  body {
    visibility: hidden; /* initially hidden until DOM ready to prevent flicker */
  }
</style>
<noscript>
  <style>
    body {
      visibility: visible !important; /* Override hidden state when JS is disabled */
    }
  </style>
</noscript>

<!-- source: https://github.com/gadenbuie/garrickadenbuie-com/blob/main/_partials/title-block-link-buttons/title-block.html -->
<!-- 
<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title"> 
-->
<!-- <header id="title-block-header" class="quarto-title-block default page-columns"> -->
<!-- <div class="quarto-title page-columns page-full featured-image p-4" style="background-image: url(featured.png), url(featured.jpg), url(../featured.jpg);"> -->



<div class="hidden">
% Meta %
<p>% Optional argument [#1]: Size modifier (e.g., , ) % #2: Opening delimiter % #3: Closing delimiter % #4: Content</p>
<p>% Common sets % Real numbers % Integers % Natural numbers % Rational numbers % Complex numbers</p>
<p>% Probability and statistics % Expectation % Variance % Covariance % Probability measure % Indicator function</p>
% Linear algebra
% Matrix
<p>% Vector % Trace % Rank % Range (image) % Projection</p>
% Calculus and analysis % For integrals, e.g., f(x) x
% Partial derivative \newcommand{[2]}{ #1} % Partial derivative w/o fraction
<p>% Second partial derivative % Gradient % Divergence % Curl</p>
% Set theory
% Set
<p>% Set builder notation % Union % Intersection % Symmetric difference</p>
<p>% Logic and proofs % Implies % If and only if % End of proof % Contradiction</p>
% Norms and inner products
% Norm
<p>% Inner product</p>
<p>% Common functions % Minimization problem % Maximization problem % Argument minimum % Argument maximum</p>
<p>% Subject to constraints % Sign function % Span of a set</p>
% Formatting
% Absolute value
% Parentheses
% Brackets
% Floor function
<p>% Ceiling function</p>
<p>% Asymptotic notations % Big O notation % Small o notation % Big Omega notation % Big Theta notation</p>
<p>% Commonly used in algorithms and complexity % Polynomial time % Polylogarithmic time</p>
<p>% Additional probability notations % Independent and identically distributed % Distributed as</p>
<p>% Fourier transform % Fourier transform % Inverse Fourier transform</p>
<p>% General math % Display style</p>
</div>
<style>
</style>
<section id="complete-summary-gradient-boosting-discussion" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Complete Summary: Gradient Boosting Discussion</h1>
<section id="part-1-mathematical-foundations" class="level2" data-number="1.1">
<h2 data-number="1.1" data-anchor-id="part-1-mathematical-foundations"><span class="header-section-number">1.1</span> Part 1: Mathematical Foundations</h2>
<section id="taylor-expansion-for-loss-functions" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" data-anchor-id="taylor-expansion-for-loss-functions"><span class="header-section-number">1.1.1</span> Taylor Expansion for Loss Functions</h3>
<p>When approximating loss around a point <span class="math inline">\(p_0\)</span>, the correct 2nd-order Taylor expansion is:</p>
<p><span class="math display">\[\mathcal{L}(y, p) \approx \mathcal{L}(y, p_0) + \mathcal{L}'(y, p_0)(p - p_0) + \frac{1}{2}\mathcal{L}''(y, p_0)(p - p_0)^2\]</span></p>
<p><strong>Critical point:</strong> Derivatives are evaluated at <span class="math inline">\(p_0\)</span> (the expansion point), NOT at <span class="math inline">\(p\)</span>. After substitution <span class="math inline">\(p = p_0 + f(x)\)</span>, you still differentiate with respect to <span class="math inline">\(p\)</span> and evaluate at <span class="math inline">\(p_0\)</span>.</p>
</section>
</section>
<section id="part-2-core-gradient-boosting-concepts" class="level2" data-number="1.2">
<h2 data-number="1.2" data-anchor-id="part-2-core-gradient-boosting-concepts"><span class="header-section-number">1.2</span> Part 2: Core Gradient Boosting Concepts</h2>
<section id="why-gradient-boosting-generalizes-beyond-mse" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" data-anchor-id="why-gradient-boosting-generalizes-beyond-mse"><span class="header-section-number">1.2.1</span> Why Gradient Boosting Generalizes Beyond MSE</h3>
<p>The mathematical chain:</p>
<ol type="1">
<li><p><strong>For MSE:</strong> Residuals <span class="math inline">\(r_i = y_i - F(x_i)\)</span> are proportional to negative gradients: <span class="math display">\[-\frac{\partial L_{\text{MSE}}}{\partial F(x_i)} = y_i - F(x_i) = r_i\]</span></p></li>
<li><p><strong>For any differentiable loss:</strong> Can compute negative gradient: <span class="math display">\[-\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\]</span></p></li>
</ol>
<p><strong>Key insight:</strong> Traditional boosting (fitting to residuals) was secretly doing gradient descent in function space for MSE.</p>
<p><strong>Generalization:</strong> Replace residuals with negative gradients for any loss function.</p>
</section>
<section id="why-fit-a-model-to-gradients" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" data-anchor-id="why-fit-a-model-to-gradients"><span class="header-section-number">1.2.2</span> Why Fit a Model to Gradients?</h3>
<p><strong>The problem:</strong> Gradients <span class="math inline">\(g_i = \frac{\partial L}{\partial F(x_i)}\)</span> are computed at training points <span class="math inline">\(x_1, ..., x_n\)</span> only (n discrete numbers).</p>
<p><strong>The need:</strong> <span class="math inline">\(F(x)\)</span> must be defined for ALL <span class="math inline">\(x\)</span>, not just training points.</p>
<p><strong>The solution:</strong> Fit model <span class="math inline">\(h\)</span> that learns mapping <span class="math inline">\(x \rightarrow -g\)</span>: - Minimize: <span class="math inline">\(\sum_{i=1}^n (h(x_i) - (-g_i))^2\)</span> - Now <span class="math inline">\(h(x)\)</span> gives gradient direction for ANY <span class="math inline">\(x\)</span> - Update: <span class="math inline">\(F(x) \leftarrow F(x) + \eta \cdot h(x)\)</span></p>
<p>This is NOT about boosting - it’s about generalizing discrete gradient information to a continuous function over the entire input space.</p>
</section>
</section>
<section id="part-3-baseline-gradient-boosting-friedman-2001" class="level2" data-number="1.3">
<h2 data-number="1.3" data-anchor-id="part-3-baseline-gradient-boosting-friedman-2001"><span class="header-section-number">1.3</span> Part 3: Baseline Gradient Boosting (Friedman 2001)</h2>
<section id="conceptual-order" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" data-anchor-id="conceptual-order"><span class="header-section-number">1.3.1</span> Conceptual Order</h3>
<ol type="1">
<li><p><strong>Goal:</strong> Minimize <span class="math inline">\(\sum_{i=1}^n L(y_i, F(x_i))\)</span> via gradient descent in function space</p></li>
<li><p><strong>Gradient descent update:</strong> <span class="math display">\[F^{t+1}(x) = F^t(x) - \eta \cdot \frac{\partial L(y, F^t(x))}{\partial F^t(x)}\]</span></p></li>
<li><p><strong>Problem:</strong> Can only compute gradients at training points</p></li>
<li><p><strong>Solution:</strong> Fit tree <span class="math inline">\(h_t\)</span> to approximate negative gradients, then update <span class="math inline">\(F\)</span></p></li>
</ol>
</section>
<section id="algorithm-first-order-gradient-boosting" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" data-anchor-id="algorithm-first-order-gradient-boosting"><span class="header-section-number">1.3.2</span> Algorithm (First-Order Gradient Boosting)</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>Initialize: F⁰(x) <span class="op">=</span> constant (see Part <span class="fl">3.1</span> <span class="cf">for</span> details)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>For t <span class="op">=</span> <span class="dv">1</span> to T:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute gradients at training points</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="op">=</span> <span class="dv">1</span> to n:</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        g_i <span class="op">=</span> ∂L(y_i, F<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}(x_i))<span class="op">/</span>∂F<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}(x_i)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit tree to negative gradients</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> fit_tree({(x_i, <span class="op">-</span>g_i)})</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update model</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    F<span class="op">^</span>t(x) <span class="op">=</span> F<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}(x) <span class="op">+</span> η·h_t(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Examples:</strong> - <strong>For squared loss:</strong> <span class="math inline">\(g_i = -(y_i - F(x_i)) = -r_i\)</span> (negative residual), so you fit to residuals. - <strong>For logloss:</strong> <span class="math inline">\(g_i = p_i - y_i\)</span> where <span class="math inline">\(p_i = \sigma(F(x_i))\)</span>, so you fit to probability errors.</p>
</section>
<section id="part-3.1-initial-prediction-f0" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" data-anchor-id="part-3.1-initial-prediction-f0"><span class="header-section-number">1.3.3</span> Part 3.1: Initial Prediction <span class="math inline">\(F^0\)</span></h3>
<p>The initial model <span class="math inline">\(F^0(x)\)</span> should minimize the loss on training data:</p>
<p><span class="math display">\[F^0 = \arg\min_c \sum_{i=1}^n L(y_i, c)\]</span></p>
<p><strong>Concrete examples:</strong></p>
<ol type="1">
<li><strong>Squared loss (MSE):</strong>
<ul>
<li><span class="math inline">\(F^0 = \frac{1}{n}\sum_{i=1}^n y_i\)</span> (mean of targets)</li>
<li>Derivative: <span class="math inline">\(\frac{\partial}{\partial c}\sum(y_i - c)^2 = -2\sum(y_i - c) = 0\)</span></li>
</ul></li>
<li><strong>Absolute loss (MAE):</strong>
<ul>
<li><span class="math inline">\(F^0 = \text{median}(y_1, ..., y_n)\)</span></li>
<li>Median minimizes sum of absolute deviations</li>
</ul></li>
<li><strong>Binary classification (logloss):</strong>
<ul>
<li>Let <span class="math inline">\(p = \frac{1}{n}\sum_{i=1}^n y_i\)</span> (fraction of positive class)</li>
<li><span class="math inline">\(F^0 = \log\frac{p}{1-p}\)</span> (log-odds)</li>
<li>Why? For logloss <span class="math inline">\(L = -[y\log(p) + (1-y)\log(1-p)]\)</span> where <span class="math inline">\(p = \sigma(F)\)</span></li>
<li>Setting <span class="math inline">\(F^0\)</span> to log-odds ensures <span class="math inline">\(\sigma(F^0) = p\)</span> (initial probability matches class frequency)</li>
</ul></li>
<li><strong>Multiclass (softmax):</strong>
<ul>
<li>For class <span class="math inline">\(k\)</span>: <span class="math inline">\(F_k^0 = \log(p_k)\)</span> where <span class="math inline">\(p_k = \frac{n_k}{n}\)</span> (fraction in class k)</li>
<li>Or set all to 0 and let first iteration learn the base rates</li>
</ul></li>
</ol>
<p><strong>Why this matters:</strong> Good initialization reduces number of iterations needed and improves convergence.</p>
</section>
<section id="part-3.2-learning-rate-shrinkage-theory" class="level3" data-number="1.3.4">
<h3 data-number="1.3.4" data-anchor-id="part-3.2-learning-rate-shrinkage-theory"><span class="header-section-number">1.3.4</span> Part 3.2: Learning Rate (Shrinkage) Theory</h3>
<p>The learning rate <span class="math inline">\(\eta \in (0, 1]\)</span> acts as regularization:</p>
<p><span class="math display">\[F^t(x) = F^{t-1}(x) + \eta \cdot h_t(x)\]</span></p>
<p><strong>Why shrinkage helps:</strong></p>
<ol type="1">
<li><strong>Regularization via smaller steps:</strong> Each tree makes a smaller contribution, preventing any single tree from dominating</li>
<li><strong>More opportunities to correct:</strong> With <span class="math inline">\(\eta = 0.1\)</span> and 1000 trees vs <span class="math inline">\(\eta = 1.0\)</span> and 100 trees:
<ul>
<li>Smaller <span class="math inline">\(\eta\)</span> allows more “chances” to refine in different directions</li>
<li>Each step is less likely to overfit</li>
</ul></li>
<li><strong>Empirical rule:</strong> Smaller <span class="math inline">\(\eta\)</span> with more trees typically generalizes better
<ul>
<li><span class="math inline">\(\eta \in [0.01, 0.1]\)</span> is common in practice</li>
<li>Trade-off: Training time increases with smaller <span class="math inline">\(\eta\)</span></li>
</ul></li>
<li><strong>Connection to stochastic optimization:</strong> Similar to learning rate in SGD - smaller steps explore loss surface more carefully</li>
</ol>
<p><strong>Mathematical intuition:</strong> If we think of gradient boosting as function space gradient descent: <span class="math display">\[F^{t}(x) = F^{t-1}(x) - \eta \cdot \nabla L|_{F^{t-1}}\]</span></p>
<p>Standard gradient descent theory tells us smaller learning rates lead to more stable convergence but require more iterations.</p>
</section>
</section>
<section id="part-4-xgboosts-improvements" class="level2" data-number="1.4">
<h2 data-number="1.4" data-anchor-id="part-4-xgboosts-improvements"><span class="header-section-number">1.4</span> Part 4: XGBoost’s Improvements</h2>
<section id="from-gradient-descent-to-newton-raphson" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" data-anchor-id="from-gradient-descent-to-newton-raphson"><span class="header-section-number">1.4.1</span> From Gradient Descent to Newton-Raphson</h3>
<p>Newton’s method uses second-order information: - <strong>Update direction:</strong> <span class="math inline">\(-\frac{g}{h}\)</span> where <span class="math inline">\(h\)</span> is the Hessian (second derivative)</p>
<p>For gradient boosting: - <span class="math inline">\(h_i = \frac{\partial^2 L(y_i, F^t(x_i))}{\partial F^t(x_i)^2}\)</span> - Fit tree to: <span class="math inline">\(-g_i/h_i\)</span> (Newton direction), weighted by <span class="math inline">\(h_i\)</span></p>
</section>
<section id="why-newtons-method-is-superior" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" data-anchor-id="why-newtons-method-is-superior"><span class="header-section-number">1.4.2</span> Why Newton’s Method Is Superior</h3>
<p><strong>1. Convergence rate:</strong> - Gradient descent: Linear convergence (error decreases by constant factor) - Newton’s method: Quadratic convergence (error squared each iteration) - Near optimum: Newton converges <em>much</em> faster</p>
<p><strong>2. Adaptive scaling:</strong> The Newton direction <span class="math inline">\(-\frac{g}{h}\)</span> automatically adapts to loss landscape: - High curvature (<span class="math inline">\(h\)</span> large) → small step - Low curvature (<span class="math inline">\(h\)</span> small) → large step - Acts like per-example adaptive learning rate</p>
<p><strong>3. Handles ill-conditioned problems:</strong> When loss has different scales in different directions (e.g., logloss with imbalanced classes): - Gradient descent: Struggles, may oscillate - Newton: Accounts for curvature, more stable</p>
<p><strong>Example - Binary classification:</strong> - For rare positive class (1% positive): - Gradient <span class="math inline">\(g = p - y\)</span> is small for most examples - But Hessian <span class="math inline">\(h = p(1-p)\)</span> is also small for confident predictions - Ratio <span class="math inline">\(g/h\)</span> properly scales the update</p>
<p><strong>4. Better than manual learning rate tuning:</strong> Instead of guessing global <span class="math inline">\(\eta\)</span>, Newton uses local curvature information.</p>
</section>
<section id="the-complete-xgboost-objective" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" data-anchor-id="the-complete-xgboost-objective"><span class="header-section-number">1.4.3</span> The Complete XGBoost Objective</h3>
<p>At iteration <span class="math inline">\(t\)</span>, minimize:</p>
<p><span class="math display">\[\text{Obj}^{(t)} = \sum_{i=1}^n L(y_i, F^{t-1}(x_i) + h_t(x_i)) + \Omega(h_t)\]</span></p>
<p><strong>Regularization (design choice):</strong> <span class="math display">\[\Omega(h_t) = \gamma J + \frac{\lambda}{2}\sum_{j=1}^J w_j^2\]</span></p>
<p>where: - <span class="math inline">\(J\)</span> = number of leaves - <span class="math inline">\(w_j\)</span> = output value (increment) of leaf <span class="math inline">\(j\)</span> - <span class="math inline">\(\gamma\)</span> = complexity penalty per leaf (hyperparameter) - <span class="math inline">\(\lambda\)</span> = L2 penalty on leaf values (hyperparameter)</p>
<p><strong>Why these penalties?</strong> - <span class="math inline">\(\gamma J\)</span>: Occam’s razor - penalize tree complexity (more leaves = more complex) - <span class="math inline">\(\lambda \sum w_j^2\)</span>: L2 regularization - prevent extreme leaf values (like ridge regression)</p>
</section>
<section id="taylor-approximation-making-it-tractable" class="level3" data-number="1.4.4">
<h3 data-number="1.4.4" data-anchor-id="taylor-approximation-making-it-tractable"><span class="header-section-number">1.4.4</span> Taylor Approximation (Making It Tractable)</h3>
<p>Second-order Taylor expansion around <span class="math inline">\(F^{t-1}(x_i)\)</span>:</p>
<p><span class="math display">\[L(y_i, F^{t-1}(x_i) + h_t(x_i)) \approx L(y_i, F^{t-1}(x_i)) + g_i h_t(x_i) + \frac{1}{2}h_i h_t(x_i)^2\]</span></p>
<p>Drop constant <span class="math inline">\(L(y_i, F^{t-1}(x_i))\)</span>:</p>
<p><span class="math display">\[\text{Obj}^{(t)} \approx \sum_{i=1}^n \left[g_i h_t(x_i) + \frac{1}{2}h_i h_t(x_i)^2\right] + \gamma J + \frac{\lambda}{2}\sum_{j=1}^J w_j^2\]</span></p>
</section>
<section id="deriving-optimal-leaf-values" class="level3" data-number="1.4.5">
<h3 data-number="1.4.5" data-anchor-id="deriving-optimal-leaf-values"><span class="header-section-number">1.4.5</span> Deriving Optimal Leaf Values</h3>
<p><strong>Tree structure:</strong> Each example <span class="math inline">\(x_i\)</span> lands in some leaf <span class="math inline">\(j\)</span>, gets value <span class="math inline">\(w_j\)</span>: <span class="math display">\[h_t(x_i) = w_{q(x_i)}\]</span></p>
<p><strong>Group by leaves</strong> (examples in leaf <span class="math inline">\(j\)</span> form set <span class="math inline">\(I_j\)</span>):</p>
<p><span class="math display">\[\text{Obj}^{(t)} = \sum_{j=1}^J \left[\left(\sum_{i \in I_j} g_i\right) w_j + \frac{1}{2}\left(\sum_{i \in I_j} h_i\right) w_j^2 + \frac{\lambda}{2}w_j^2\right] + \gamma J\]</span></p>
<p><strong>Define:</strong> - <span class="math inline">\(G_j = \sum_{i \in I_j} g_i\)</span> (total gradient in leaf) - <span class="math inline">\(H_j = \sum_{i \in I_j} h_i\)</span> (total hessian in leaf)</p>
<p><span class="math display">\[\text{Obj}^{(t)} = \sum_{j=1}^J \left[G_j w_j + \frac{1}{2}(H_j + \lambda)w_j^2\right] + \gamma J\]</span></p>
<p><strong>Minimize w.r.t. <span class="math inline">\(w_j\)</span></strong> (take derivative, set to zero):</p>
<p><span class="math display">\[\frac{\partial \text{Obj}}{\partial w_j} = G_j + (H_j + \lambda)w_j = 0\]</span></p>
<p><strong>Optimal leaf value:</strong> <span class="math display">\[w_j^* = -\frac{G_j}{H_j + \lambda}\]</span></p>
<p><strong>Interpretation:</strong> - <strong>Numerator:</strong> Gradient signal (direction to move) - <strong>Denominator:</strong> Curvature + regularization (how much to trust/scale the move) - Similar to SNR or Adam optimizer’s adaptive scaling</p>
</section>
<section id="plugging-optimal-weights-back" class="level3" data-number="1.4.6">
<h3 data-number="1.4.6" data-anchor-id="plugging-optimal-weights-back"><span class="header-section-number">1.4.6</span> Plugging Optimal Weights Back</h3>
<p><strong>Minimum objective for tree structure:</strong> <span class="math display">\[\text{Obj}^{(t)} = -\frac{1}{2}\sum_{j=1}^J \frac{G_j^2}{H_j + \lambda} + \gamma J\]</span></p>
<p>This is what we minimize when searching for tree structure.</p>
</section>
<section id="computing-gain-from-splits" class="level3" data-number="1.4.7">
<h3 data-number="1.4.7" data-anchor-id="computing-gain-from-splits"><span class="header-section-number">1.4.7</span> Computing Gain from Splits</h3>
<p><strong>Before split:</strong> Node with examples <span class="math inline">\(I\)</span> is one leaf <span class="math display">\[\text{Loss}_{\text{before}} = -\frac{1}{2}\frac{G^2}{H + \lambda} + \gamma\]</span></p>
<p><strong>After split:</strong> Split into left <span class="math inline">\(I_L\)</span> and right <span class="math inline">\(I_R\)</span> (two leaves) <span class="math display">\[\text{Loss}_{\text{after}} = -\frac{1}{2}\frac{G_L^2}{H_L + \lambda} - \frac{1}{2}\frac{G_R^2}{H_R + \lambda} + 2\gamma\]</span></p>
<p><strong>Gain (loss reduction):</strong> <span class="math display">\[\text{Gain} = \frac{1}{2}\left[\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{G^2}{H + \lambda}\right] - \gamma\]</span></p>
<p><strong>Decision:</strong> Only split if Gain &gt; 0</p>
</section>
<section id="complete-xgboost-algorithm-one-iteration" class="level3" data-number="1.4.8">
<h3 data-number="1.4.8" data-anchor-id="complete-xgboost-algorithm-one-iteration"><span class="header-section-number">1.4.8</span> Complete XGBoost Algorithm (One Iteration)</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Input: Training data {(x_i, y_i)}, current model F<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}, loss L</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">1</span>: Compute gradients <span class="kw">and</span> hessians</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="op">=</span> <span class="dv">1</span> to n:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    g_i <span class="op">=</span> ∂L(y_i, F<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}(x_i))<span class="op">/</span>∂F<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}(x_i)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    h_i <span class="op">=</span> ∂²L(y_i, F<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}(x_i))<span class="op">/</span>∂F<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}(x_i)²</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">2</span>: Build tree (greedy, recursive splitting)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>Start <span class="cf">with</span> root: I <span class="op">=</span> {<span class="dv">1</span>,...,n}</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>Queue <span class="op">=</span> [I]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> Queue <span class="kw">not</span> empty:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    I <span class="op">=</span> Queue.pop()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> <span class="bu">sum</span>(g_i <span class="cf">for</span> i <span class="kw">in</span> I)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> <span class="bu">sum</span>(h_i <span class="cf">for</span> i <span class="kw">in</span> I)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find best split</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    best_gain <span class="op">=</span> <span class="op">-</span>∞</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> feature f:</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> threshold θ:</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            I_L <span class="op">=</span> {i <span class="kw">in</span> I: x_i<span class="op">^</span>f <span class="op">&lt;</span> θ}</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>            I_R <span class="op">=</span> {i <span class="kw">in</span> I: x_i<span class="op">^</span>f ≥ θ}</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            G_L <span class="op">=</span> <span class="bu">sum</span>(g_i <span class="cf">for</span> i <span class="kw">in</span> I_L)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            H_L <span class="op">=</span> <span class="bu">sum</span>(h_i <span class="cf">for</span> i <span class="kw">in</span> I_L)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>            G_R <span class="op">=</span> G <span class="op">-</span> G_L</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>            H_R <span class="op">=</span> H <span class="op">-</span> H_L</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>            gain <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>[G_L²<span class="op">/</span>(H_L<span class="op">+</span>λ) <span class="op">+</span> G_R²<span class="op">/</span>(H_R<span class="op">+</span>λ) <span class="op">-</span> G²<span class="op">/</span>(H<span class="op">+</span>λ)] <span class="op">-</span> γ</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> gain <span class="op">&gt;</span> best_gain:</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>                best_gain <span class="op">=</span> gain</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>                best_split <span class="op">=</span> (f, θ)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Execute split</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> best_gain <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> depth <span class="op">&lt;</span> max_depth:</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        Split I into I_L, I_R using best_split</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        Queue.append(I_L)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        Queue.append(I_R)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        Mark I <span class="im">as</span> leaf</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">3</span>: Assign leaf values</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> each leaf j:</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    G_j <span class="op">=</span> <span class="bu">sum</span>(g_i <span class="cf">for</span> i <span class="kw">in</span> I_j)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    H_j <span class="op">=</span> <span class="bu">sum</span>(h_i <span class="cf">for</span> i <span class="kw">in</span> I_j)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    w_j <span class="op">=</span> <span class="op">-</span>G_j<span class="op">/</span>(H_j <span class="op">+</span> λ)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>Step <span class="dv">4</span>: Update model</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>F<span class="op">^</span>t(x) <span class="op">=</span> F<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}(x) <span class="op">+</span> η·h_t(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="concrete-examples" class="level3" data-number="1.4.9">
<h3 data-number="1.4.9" data-anchor-id="concrete-examples"><span class="header-section-number">1.4.9</span> Concrete Examples</h3>
<p><strong>1. Squared loss:</strong> <span class="math inline">\(L = \frac{1}{2}(y - \hat{y})^2\)</span> - <span class="math inline">\(g_i = -(y_i - F(x_i)) = -r_i\)</span> (negative residual) - <span class="math inline">\(h_i = 1\)</span> - <span class="math inline">\(w_j = \frac{\sum_{i \in I_j} r_i}{|I_j| + \lambda}\)</span> (average residual)</p>
<p><strong>2. Logloss:</strong> <span class="math inline">\(L = -[y\log(p) + (1-y)\log(1-p)]\)</span> where <span class="math inline">\(p = \sigma(F)\)</span> - <span class="math inline">\(g_i = p_i - y_i\)</span> (probability error) - <span class="math inline">\(h_i = p_i(1-p_i)\)</span> (Bernoulli variance) - <span class="math inline">\(w_j = -\frac{\sum_{i \in I_j}(p_i - y_i)}{\sum_{i \in I_j}p_i(1-p_i) + \lambda}\)</span></p>
</section>
<section id="connection-to-weighted-least-squares" class="level3" data-number="1.4.10">
<h3 data-number="1.4.10" data-anchor-id="connection-to-weighted-least-squares"><span class="header-section-number">1.4.10</span> Connection to Weighted Least Squares</h3>
<p>Complete the square in objective: <span class="math display">\[\sum_{i=1}^n \left[g_i h_t(x_i) + \frac{1}{2}h_i h_t(x_i)^2\right] = \sum_{i=1}^n \frac{h_i}{2}\left(h_t(x_i) - \left(-\frac{g_i}{h_i}\right)\right)^2 + \text{const}\]</span></p>
<p>This is <strong>weighted least squares:</strong> - <strong>Target:</strong> <span class="math inline">\(-g_i/h_i\)</span> (Newton direction) - <strong>Weight:</strong> <span class="math inline">\(h_i\)</span> (hessian as importance weight)</p>
<p>Trees discretize this continuous problem into regions with constant values.</p>
<hr>
</section>
</section>
<section id="part-5-handling-missing-values" class="level2" data-number="1.5">
<h2 data-number="1.5" data-anchor-id="part-5-handling-missing-values"><span class="header-section-number">1.5</span> Part 5: Handling Missing Values</h2>
<section id="the-xgboost-approach" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" data-anchor-id="the-xgboost-approach"><span class="header-section-number">1.5.1</span> The XGBoost Approach</h3>
<p><strong>Problem:</strong> Real-world data often has missing values. Simple strategies (drop, impute mean) lose information.</p>
<p><strong>XGBoost’s learned sparsity-aware split:</strong></p>
<ol type="1">
<li><strong>During training</strong> at each node split:
<ul>
<li>Compute gain when missing values go LEFT</li>
<li>Compute gain when missing values go RIGHT</li>
<li>Choose direction that maximizes gain</li>
<li>Store default direction as part of split</li>
</ul></li>
<li><strong>During prediction:</strong>
<ul>
<li>Follow learned default direction for missing values</li>
<li>No need to impute</li>
</ul></li>
</ol>
<p><strong>Algorithm modification:</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>For each candidate split (feature f, threshold θ):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try missing → left</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    I_L <span class="op">=</span> {i: x_i<span class="op">^</span>f <span class="op">&lt;</span> θ OR x_i<span class="op">^</span>f <span class="kw">is</span> missing}</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    I_R <span class="op">=</span> {i: x_i<span class="op">^</span>f ≥ θ AND x_i<span class="op">^</span>f <span class="kw">not</span> missing}</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    gain_left <span class="op">=</span> compute_gain(I_L, I_R)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try missing → right</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    I_L <span class="op">=</span> {i: x_i<span class="op">^</span>f <span class="op">&lt;</span> θ AND x_i<span class="op">^</span>f <span class="kw">not</span> missing}</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    I_R <span class="op">=</span> {i: x_i<span class="op">^</span>f ≥ θ OR x_i<span class="op">^</span>f <span class="kw">is</span> missing}</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    gain_right <span class="op">=</span> compute_gain(I_L, I_R)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use better option</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">max</span>(gain_left, gain_right) <span class="op">&gt;</span> best_gain:</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        best_gain <span class="op">=</span> <span class="bu">max</span>(gain_left, gain_right)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        best_split <span class="op">=</span> (f, θ, default_direction)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Why this works:</strong> - Missing values may correlate with target (e.g., missing income → likely low income) - Algorithm learns this correlation automatically - More flexible than imputation</p>
<p><strong>Implementation detail:</strong> - Sparse features (mostly zeros) can be treated similarly - Only store non-zero values in memory - Massive speedup for high-dimensional sparse data (e.g., text, categorical one-hot)</p>
<hr>
</section>
</section>
<section id="part-6-histogram-based-split-finding" class="level2" data-number="1.6">
<h2 data-number="1.6" data-anchor-id="part-6-histogram-based-split-finding"><span class="header-section-number">1.6</span> Part 6: Histogram-Based Split Finding</h2>
<section id="exact-greedy-baseline" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" data-anchor-id="exact-greedy-baseline"><span class="header-section-number">1.6.1</span> Exact Greedy (Baseline)</h3>
<p>For each feature: 1. Sort all values 2. Try <span class="math inline">\(n-1\)</span> splits (between consecutive values) 3. Compute gain for each</p>
<p><strong>Cost:</strong> <span class="math inline">\(O(n \log n)\)</span> sorting + <span class="math inline">\(O(n \cdot \text{features})\)</span> evaluation</p>
</section>
<section id="histogram-based-xgboostlightgbm" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" data-anchor-id="histogram-based-xgboostlightgbm"><span class="header-section-number">1.6.2</span> Histogram-Based (XGBoost/LightGBM)</h3>
<p><strong>Pre-processing:</strong> 1. Bin continuous features into <span class="math inline">\(k\)</span> discrete bins (e.g., <span class="math inline">\(k=255\)</span>) 2. Use quantile-based binning (equal samples per bin)</p>
<p><strong>At each split:</strong> 1. <strong>Build histogram:</strong> For bin <span class="math inline">\(j\)</span>, accumulate <span class="math inline">\(G_j = \sum g_i\)</span>, <span class="math inline">\(H_j = \sum h_i\)</span> 2. <strong>Scan bins</strong> (only <span class="math inline">\(k\)</span> candidates instead of <span class="math inline">\(n\)</span>) 3. <strong>Cumulative sums:</strong> <span class="math inline">\(G_L = \sum_{s=0}^j G_s\)</span>, <span class="math inline">\(G_R = G_{\text{total}} - G_L\)</span> 4. Compute gain at each bin boundary</p>
<p><strong>Cost:</strong> <span class="math inline">\(O(n)\)</span> to build histogram + <span class="math inline">\(O(k \cdot \text{features})\)</span> evaluation</p>
<p><strong>Histogram subtraction trick:</strong> - For node with children <span class="math inline">\(L\)</span> and <span class="math inline">\(R\)</span>: <span class="math inline">\(H_R = H_{\text{parent}} - H_L\)</span> - Build histogram for smaller child only</p>
<p><strong>Memory savings:</strong> - Store <code>uint8</code> bins instead of <code>float32</code> values: 4× memory reduction</p>
<p><strong>Trade-off:</strong> - Small accuracy loss from discretization - Huge speed gain (especially for large <span class="math inline">\(n\)</span>) - Can act as regularization</p>
<hr>
</section>
</section>
<section id="part-7-tree-growth-strategies" class="level2" data-number="1.7">
<h2 data-number="1.7" data-anchor-id="part-7-tree-growth-strategies"><span class="header-section-number">1.7</span> Part 7: Tree Growth Strategies</h2>
<section id="level-wise-xgboost-default-traditional" class="level3" data-number="1.7.1">
<h3 data-number="1.7.1" data-anchor-id="level-wise-xgboost-default-traditional"><span class="header-section-number">1.7.1</span> Level-wise (XGBoost default, traditional)</h3>
<ul>
<li>Split ALL leaves at current level before moving to next</li>
<li>Depth 1: 2 leaves → Depth 2: 4 leaves → Depth 3: 8 leaves</li>
<li>Balanced trees, more conservative</li>
</ul>
</section>
<section id="leaf-wise-lightgbm-best-first" class="level3" data-number="1.7.2">
<h3 data-number="1.7.2" data-anchor-id="leaf-wise-lightgbm-best-first"><span class="header-section-number">1.7.2</span> Leaf-wise (LightGBM, best-first)</h3>
<ul>
<li>At each step, split the ONE leaf with maximum gain</li>
<li>Ignores depth/level structure</li>
<li>Can create unbalanced/deep trees</li>
</ul>
<p><strong>Trade-off:</strong> - <strong>Advantage:</strong> Faster convergence, lower loss with same #leaves - <strong>Disadvantage:</strong> More prone to overfitting (mitigate with <code>max_depth</code>)</p>
<p><strong>Visualization:</strong></p>
<pre><code>Level-wise (depth 2):
       O
      / \
     O   O
    / \ / \
   O  O O  O

Leaf-wise (4 splits):
       O
      / \
     O   O
    /     \
   O       O
  /         \
 O           O</code></pre>
<hr>
</section>
</section>
<section id="part-8-subsampling-and-regularization" class="level2" data-number="1.8">
<h2 data-number="1.8" data-anchor-id="part-8-subsampling-and-regularization"><span class="header-section-number">1.8</span> Part 8: Subsampling and Regularization</h2>
<section id="row-subsampling-stochastic-gradient-boosting" class="level3" data-number="1.8.1">
<h3 data-number="1.8.1" data-anchor-id="row-subsampling-stochastic-gradient-boosting"><span class="header-section-number">1.8.1</span> Row Subsampling (Stochastic Gradient Boosting)</h3>
<p>At each iteration, randomly sample a fraction of training data:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>subsample <span class="op">=</span> <span class="fl">0.8</span>  <span class="co"># Use 80% of data per tree</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="dv">1</span> to T:</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> random_sample(n, size<span class="op">=</span><span class="bu">int</span>(subsample <span class="op">*</span> n))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    X_sample <span class="op">=</span> X[indices]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    y_sample <span class="op">=</span> y[indices]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    g_sample <span class="op">=</span> g[indices]</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    h_sample <span class="op">=</span> h[indices]</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    tree <span class="op">=</span> fit_tree(X_sample, g_sample, h_sample)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    F <span class="op">=</span> F <span class="op">+</span> η <span class="op">*</span> tree</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Why this helps:</strong> 1. <strong>Reduces overfitting:</strong> Each tree sees different data 2. <strong>Speeds up training:</strong> Fewer examples per tree 3. <strong>Adds randomness:</strong> Similar to bagging in random forests 4. <strong>Typical values:</strong> 0.5 - 1.0 (50-100% of data)</p>
</section>
<section id="column-subsampling" class="level3" data-number="1.8.2">
<h3 data-number="1.8.2" data-anchor-id="column-subsampling"><span class="header-section-number">1.8.2</span> Column Subsampling</h3>
<p>Randomly select subset of features:</p>
<p><strong>Three variants:</strong></p>
<ol type="1">
<li><strong><code>colsample_bytree</code>:</strong> Sample features once per tree</li>
</ol>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="dv">1</span> to T:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> random_sample(all_features, size<span class="op">=</span>colsample_bytree <span class="op">*</span> n_features)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    tree <span class="op">=</span> fit_tree(X[:, features], g, h)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li><strong><code>colsample_bylevel</code>:</strong> Sample features at each level</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> each level <span class="kw">in</span> tree:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> random_sample(all_features, size<span class="op">=</span>colsample_bylevel <span class="op">*</span> n_features)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use only these features for all splits at this level</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="3" type="1">
<li><strong><code>colsample_bynode</code>:</strong> Sample features at each node</li>
</ol>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> each node <span class="kw">in</span> tree:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> random_sample(all_features, size<span class="op">=</span>colsample_bynode <span class="op">*</span> n_features)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use only these features for this node's split</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Why this helps:</strong> 1. <strong>Reduces overfitting:</strong> Prevents reliance on few dominant features 2. <strong>Increases diversity:</strong> Forces model to find alternative patterns 3. <strong>Feature decorrelation:</strong> Similar to random forests 4. <strong>Typical values:</strong> 0.5 - 1.0</p>
</section>
<section id="regularization-parameters-summary" class="level3" data-number="1.8.3">
<h3 data-number="1.8.3" data-anchor-id="regularization-parameters-summary"><span class="header-section-number">1.8.3</span> Regularization Parameters Summary</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
<th>Typical Range</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>eta</code> (learning_rate)</td>
<td>Shrinkage factor</td>
<td>0.01 - 0.3</td>
</tr>
<tr class="even">
<td><code>lambda</code> (reg_lambda)</td>
<td>L2 on leaf weights</td>
<td>0 - 10</td>
</tr>
<tr class="odd">
<td><code>alpha</code> (reg_alpha)</td>
<td>L1 on leaf weights</td>
<td>0 - 10</td>
</tr>
<tr class="even">
<td><code>gamma</code> (min_split_loss)</td>
<td>Min gain to split</td>
<td>0 - 10</td>
</tr>
<tr class="odd">
<td><code>max_depth</code></td>
<td>Tree depth limit</td>
<td>3 - 10</td>
</tr>
<tr class="even">
<td><code>min_child_weight</code></td>
<td>Min sum(hessian) in child</td>
<td>1 - 10</td>
</tr>
<tr class="odd">
<td><code>subsample</code></td>
<td>Row sampling rate</td>
<td>0.5 - 1.0</td>
</tr>
<tr class="even">
<td><code>colsample_bytree</code></td>
<td>Feature sampling per tree</td>
<td>0.5 - 1.0</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="part-9-lightgbm-contributions" class="level2" data-number="1.9">
<h2 data-number="1.9" data-anchor-id="part-9-lightgbm-contributions"><span class="header-section-number">1.9</span> Part 9: LightGBM Contributions</h2>
<section id="main-algorithmic-innovations" class="level3" data-number="1.9.1">
<h3 data-number="1.9.1" data-anchor-id="main-algorithmic-innovations"><span class="header-section-number">1.9.1</span> Main Algorithmic Innovations</h3>
<p><strong>1. GOSS (Gradient-based One-Side Sampling)</strong></p>
<p><strong>Idea:</strong> Not all examples contribute equally to learning: - Large gradient → model is uncertain → important to learn from - Small gradient → model is confident → less important</p>
<p><strong>Algorithm:</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> top_rate (e.g., <span class="fl">0.2</span>)      <span class="co"># Keep 20% large gradients</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> other_rate (e.g., <span class="fl">0.1</span>)    <span class="co"># Sample 10% small gradients</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by absolute gradient</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>sorted_indices <span class="op">=</span> argsort(<span class="op">|</span>g<span class="op">|</span>, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Keep all large gradients</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>large_grad <span class="op">=</span> sorted_indices[:<span class="bu">int</span>(a <span class="op">*</span> n)]</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Random sample from small gradients</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>small_grad <span class="op">=</span> sorted_indices[<span class="bu">int</span>(a <span class="op">*</span> n):]</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>sampled_small <span class="op">=</span> random_sample(small_grad, size<span class="op">=</span><span class="bu">int</span>(b <span class="op">*</span> n))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>used_indices <span class="op">=</span> large_grad <span class="op">+</span> sampled_small</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># IMPORTANT: Reweight small gradient examples</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>weight_small <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> a) <span class="op">/</span> b  <span class="co"># Compensate for sampling</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> sampled_small:</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    g[i] <span class="op">*=</span> weight_small</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    h[i] <span class="op">*=</span> weight_small</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Why reweighting?</strong> - We sample small gradients with probability <span class="math inline">\(p = \frac{b}{1-a}\)</span> - To maintain correct gradient distribution: multiply by <span class="math inline">\(1/p = \frac{1-a}{b}\)</span> - Without reweighting: biased toward large-gradient regions</p>
<p><strong>Trade-off:</strong> - Reduces data by <span class="math inline">\((1 - a - b) \times 100\%\)</span> (e.g., 70% reduction) - Minimal accuracy loss if tuned properly - Huge speedup</p>
<p><strong>2. EFB (Exclusive Feature Bundling)</strong></p>
<p><strong>Problem:</strong> High-dimensional sparse data (e.g., one-hot encoded categoricals)</p>
<p><strong>Observation:</strong> Many features are mutually exclusive (rarely both non-zero)</p>
<p><strong>Algorithm:</strong> 1. Build feature conflict graph: - Edge between features if they’re both non-zero for many examples - Conflict threshold: if overlap &gt; threshold, don’t bundle</p>
<ol start="2" type="1">
<li>Graph coloring problem:
<ul>
<li>Group features into bundles (colors)</li>
<li>Features in same bundle never conflict</li>
</ul></li>
<li>Merge features in each bundle:
<ul>
<li>Add offsets to distinguish: <code>bundled = f1 + (max_f1 + 1) * f2 + ...</code></li>
</ul></li>
</ol>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>Original features (one<span class="op">-</span>hot):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>city_NYC:  [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>city_LA:   [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>city_SF:   [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>Bundled:</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>city:      [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">3</span>]  <span class="co"># Same information, one feature</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Benefits:</strong> - Reduces feature count (e.g., 1000 → 100 features) - Faster split finding - Lower memory usage</p>
<p><strong>3. Leaf-wise tree growth</strong> (discussed in Part 7)</p>
<p><strong>4. Optimized histogram implementation</strong> - Cache-friendly memory access patterns - Parallel histogram building across features - Optimized binning algorithms</p>
<p><strong>Focus:</strong> Speed optimizations via sampling and bundling</p>
</section>
</section>
<section id="part-10-catboost-contributions" class="level2" data-number="1.10">
<h2 data-number="1.10" data-anchor-id="part-10-catboost-contributions"><span class="header-section-number">1.10</span> Part 10: CatBoost Contributions</h2>
<section id="problem-1-target-leakage-in-categorical-features" class="level3" data-number="1.10.1">
<h3 data-number="1.10.1" data-anchor-id="problem-1-target-leakage-in-categorical-features"><span class="header-section-number">1.10.1</span> Problem 1: Target Leakage in Categorical Features</h3>
<p><strong>Traditional approach (greedy target statistics):</strong> For category <span class="math inline">\(A\)</span> in feature <span class="math inline">\(x^i\)</span>:</p>
<p><span class="math display">\[\hat{x}^i_k = \frac{\sum_{j=1}^n \mathbb{1}[x^i_j = A] \cdot y_j + ap}{\sum_{j=1}^n \mathbb{1}[x^i_j = A] + a}\]</span></p>
<p>where: - <span class="math inline">\(p\)</span> = prior (e.g., global mean) - <span class="math inline">\(a\)</span> = smoothing parameter</p>
<p><strong>Problem:</strong> Uses <span class="math inline">\(y_k\)</span> to compute feature for <span class="math inline">\(x_k\)</span> → conditional shift - Distribution of <span class="math inline">\(\hat{x}^i_k | y_k\)</span> (train) ≠ distribution of <span class="math inline">\(\hat{x}^i | y\)</span> (test) - Model sees features that “know” the answer - Overfits to training set</p>
<p><strong>CatBoost solution: Ordered Target Statistics</strong></p>
<ol type="1">
<li>Take random permutation <span class="math inline">\(\sigma\)</span> of training data</li>
<li>For training example <span class="math inline">\(k\)</span>:</li>
</ol>
<p><span class="math display">\[\hat{x}^i_k = \frac{\sum_{j: \sigma(j) &lt; \sigma(k)} \mathbb{1}[x^i_j = x^i_k] \cdot y_j + ap}{\sum_{j: \sigma(j) &lt; \sigma(k)} \mathbb{1}[x^i_j = x^i_k] + a}\]</span></p>
<p><strong>Key:</strong> Only use examples BEFORE <span class="math inline">\(k\)</span> in permutation. Never use <span class="math inline">\(y_k\)</span> itself.</p>
<ol start="3" type="1">
<li>For test examples: use all training data</li>
</ol>
<p><strong>Works for both regression and classification (same formula):</strong> - <strong>Regression:</strong> estimates <span class="math inline">\(\mathbb{E}[y | x^i = A]\)</span> - <strong>Classification:</strong> estimates <span class="math inline">\(P(y=1 | x^i = A)\)</span> (click-through rate)</p>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>Permutation: [<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">5</span>]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>Category:    [A, A, B, A, B]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>Target:      [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>For example <span class="dv">3</span> (position <span class="dv">0</span>):</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  TS_3 <span class="op">=</span> prior (no predecessors)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>For example <span class="dv">1</span> (position <span class="dv">1</span>):</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  TS_1 <span class="op">=</span> (<span class="dv">1</span><span class="op">*</span><span class="dv">1</span> <span class="op">+</span> a<span class="op">*</span>prior) <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> a)  <span class="co"># Only uses example 3</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>For example <span class="dv">4</span> (position <span class="dv">2</span>):</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>  TS_4 <span class="op">=</span> (<span class="dv">1</span><span class="op">*</span><span class="dv">1</span> <span class="op">+</span> a<span class="op">*</span>prior) <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> a)  <span class="co"># Example 3 has category B</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>For example <span class="dv">2</span> (position <span class="dv">3</span>):</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>  TS_2 <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> <span class="dv">0</span> <span class="op">+</span> a<span class="op">*</span>prior) <span class="op">/</span> (<span class="dv">2</span> <span class="op">+</span> a)  <span class="co"># Examples 3,1 have category A</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>For example <span class="dv">5</span> (position <span class="dv">4</span>):</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>  TS_5 <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> a<span class="op">*</span>prior) <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> a)  <span class="co"># Only example 4 has category B</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="problem-2-prediction-shift-in-gradient-boosting" class="level3" data-number="1.10.2">
<h3 data-number="1.10.2" data-anchor-id="problem-2-prediction-shift-in-gradient-boosting"><span class="header-section-number">1.10.2</span> Problem 2: Prediction Shift in Gradient Boosting</h3>
<p><strong>The problem:</strong> Standard boosting at step <span class="math inline">\(t\)</span>: 1. Compute gradient: <span class="math inline">\(g_i = \nabla L(y_i, F^{t-1}(x_i))\)</span> 2. But <span class="math inline">\(F^{t-1}\)</span> was trained on <span class="math inline">\((x_i, y_i)\)</span> → <span class="math inline">\(F^{t-1}(x_i)\)</span> is overfitted 3. So <span class="math inline">\(g_i\)</span> is biased</p>
<p><strong>Formal statement:</strong> - Distribution of <span class="math inline">\(F^{t-1}(x_k) | x_k\)</span> (train) ≠ distribution of <span class="math inline">\(F^{t-1}(x) | x\)</span> (test) - Gradients computed on overfitted predictions → biased updates</p>
<p><strong>Theoretical result (Theorem 1 in CatBoost paper):</strong> With MSE loss, if you use same dataset at each step:</p>
<p><span class="math display">\[\mathbb{E}[F^2(x)] = f^*(x) - \frac{c_2}{n-1}(x_2 - \frac{1}{2}) + O(2^{-n})\]</span></p>
<p>There’s a bias inversely proportional to <span class="math inline">\(n\)</span>.</p>
<p><strong>CatBoost solution: Ordered Boosting</strong></p>
<p><strong>Naive version (infeasible):</strong> 1. Maintain <span class="math inline">\(n\)</span> models <span class="math inline">\(M_1, ..., M_n\)</span> 2. <span class="math inline">\(M_j\)</span> = trained on first <span class="math inline">\(j\)</span> examples in permutation <span class="math inline">\(\sigma\)</span> 3. For example at position <span class="math inline">\(j\)</span>: compute gradient using <span class="math inline">\(M_{j-1}\)</span> (never saw this example)</p>
<p><strong>Problem:</strong> Requires <span class="math inline">\(n\)</span> models - computationally infeasible</p>
<p><strong>Practical version (CatBoost’s innovation):</strong></p>
<ol type="1">
<li>Use <span class="math inline">\(s\)</span> permutations <span class="math inline">\(\{\sigma_1, ..., \sigma_s\}\)</span></li>
<li>Store <span class="math inline">\(s \times n\)</span> prediction values (not models): <span class="math inline">\(M_{r,j}(i)\)</span>
<ul>
<li><span class="math inline">\(r\)</span> = permutation index</li>
<li><span class="math inline">\(j\)</span> = trained on first <span class="math inline">\(j\)</span> examples in <span class="math inline">\(\sigma_r\)</span></li>
<li><span class="math inline">\(i\)</span> = example index</li>
</ul></li>
<li>At iteration <span class="math inline">\(t\)</span>:
<ul>
<li>Pick random permutation <span class="math inline">\(r\)</span></li>
<li>For example <span class="math inline">\(i\)</span> at position <span class="math inline">\(\sigma_r(i) = k\)</span>: use <span class="math inline">\(M_{r,k-1}(i)\)</span> for gradient</li>
<li>Build ONE tree structure (shared across all models)</li>
<li>Update all <span class="math inline">\(M_{r',j}\)</span> with same tree but different leaf values</li>
</ul></li>
</ol>
<p><strong>Memory:</strong> <span class="math inline">\(O(s \cdot n)\)</span> numbers + one tree sequence</p>
<p><strong>Why <span class="math inline">\(s &gt; 1\)</span> permutations?</strong> - Early examples in a permutation have few predecessors → high variance in statistics - Multiple permutations average out this variance - Typical: <span class="math inline">\(s = 4\)</span> or <span class="math inline">\(s = 8\)</span></p>
<p><strong>CRITICAL:</strong> Must use same permutation for both target statistics AND ordered boosting to avoid prediction shift.</p>
<p><strong>Pseudocode:</strong></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> r <span class="kw">in</span> <span class="dv">1</span> to s:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    permutations[r] <span class="op">=</span> random_permutation(n)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="dv">1</span> to n:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        M[r, <span class="dv">0</span>, i] <span class="op">=</span> <span class="dv">0</span>  <span class="co"># Initial prediction</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Build trees</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="dv">1</span> to T:</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pick random permutation</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> random_int(<span class="dv">1</span>, s)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> permutations[r]</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute gradients using ordered predictions</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> position k <span class="kw">in</span> <span class="dv">1</span> to n:</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> sigma[k]</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> M[r, k<span class="op">-</span><span class="dv">1</span>, i]  <span class="co"># Never trained on example i</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        g[i] <span class="op">=</span> gradient(y[i], pred)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        h[i] <span class="op">=</span> hessian(y[i], pred)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build tree (standard XGBoost way)</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    tree <span class="op">=</span> build_tree(X, g, h)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update ALL prediction tables</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="vs">r' in 1 to s:</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="er">        for k in 1 to n:</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i where sigma[<span class="vs">r']</span><span class="pp">[i]</span><span class="vs"> &lt;= k:</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="er">                M</span>[<span class="vs">r', k, i] </span><span class="op">+</span><span class="vs">= tree</span><span class="dv">.</span><span class="vs">predict</span><span class="kw">(</span><span class="vs">X</span><span class="pp">[i]</span><span class="kw">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="other-catboost-details" class="level3" data-number="1.10.3">
<h3 data-number="1.10.3" data-anchor-id="other-catboost-details"><span class="header-section-number">1.10.3</span> Other CatBoost Details</h3>
<p><strong>Oblivious trees (symmetric trees):</strong> - CatBoost uses symmetric trees: same split feature and threshold at each level across all nodes - Level 0: one split condition - Level 1: same second split applied to both children - Creates <span class="math inline">\(2^d\)</span> leaves for depth <span class="math inline">\(d\)</span></p>
<p><strong>Structure:</strong></p>
<pre><code>Oblivious tree (depth 2):
           [age &lt; 30]
          /          \
    [income &lt; 50k] [income &lt; 50k]
      /    \         /    \
   leaf0  leaf1   leaf2  leaf3

All "level 1" nodes use same split (income &lt; 50k)</code></pre>
<p><strong>Why oblivious?</strong> - <strong>Speed:</strong> Prediction is <span class="math inline">\(O(d)\)</span> via bitmask operations (not <span class="math inline">\(O(d)\)</span> tree traversal) - <strong>Regularization:</strong> Less expressive → less overfitting - <strong>Not fundamental:</strong> Can use regular trees with ordered boosting</p>
<p><strong>Focus:</strong> Fixes statistical bias problems (prediction shift, categorical encoding)</p>
</section>
</section>
<section id="part-11-multiclass-classification" class="level2" data-number="1.11">
<h2 data-number="1.11" data-anchor-id="part-11-multiclass-classification"><span class="header-section-number">1.11</span> Part 11: Multiclass Classification</h2>
<p>Gradient boosting naturally extends to multiclass problems. Two main approaches:</p>
<section id="approach-1-one-vs-all-ova" class="level3" data-number="1.11.1">
<h3 data-number="1.11.1" data-anchor-id="approach-1-one-vs-all-ova"><span class="header-section-number">1.11.1</span> Approach 1: One-vs-All (OvA)</h3>
<p>Build <span class="math inline">\(K\)</span> separate binary models for <span class="math inline">\(K\)</span> classes:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="kw">class</span> <span class="fu">k</span> <span class="fu">in</span> 1 <span class="fu">to</span> <span class="fu">K</span>:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Binary labels: y_binary[i] = 1 if y[i] == k else 0</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    F_k <span class="op">=</span> train_binary_model(X, y_binary)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prediction</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> softmax([F_1(x), F_2(x), ..., F_K(x)])</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>predicted_class <span class="op">=</span> argmax(probabilities)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Pros:</strong> - Simple to implement - Can use any binary loss function</p>
<p><strong>Cons:</strong> - Classes trained independently (no joint optimization) - Probabilities may not sum to 1 without calibration</p>
</section>
<section id="approach-2-softmax-xgboostlightgbm-default" class="level3" data-number="1.11.2">
<h3 data-number="1.11.2" data-anchor-id="approach-2-softmax-xgboostlightgbm-default"><span class="header-section-number">1.11.2</span> Approach 2: Softmax (XGBoost/LightGBM default)</h3>
<p>Joint optimization with multinomial logloss.</p>
<p><strong>Model:</strong> - Maintain <span class="math inline">\(K\)</span> functions: <span class="math inline">\(F_1(x), ..., F_K(x)\)</span> - Softmax probabilities: <span class="math inline">\(p_k(x) = \frac{e^{F_k(x)}}{\sum_{j=1}^K e^{F_j(x)}}\)</span></p>
<p><strong>Loss:</strong> Multinomial logloss (cross-entropy) <span class="math display">\[L = -\sum_{k=1}^K y_k \log(p_k)\]</span></p>
<p>where <span class="math inline">\(y_k \in \{0, 1\}\)</span> (one-hot encoding).</p>
<p><strong>Gradients and Hessians:</strong> For class <span class="math inline">\(k\)</span>: <span class="math display">\[g_k = \frac{\partial L}{\partial F_k} = p_k - y_k\]</span></p>
<p><span class="math display">\[h_k = \frac{\partial^2 L}{\partial F_k^2} = p_k(1 - p_k)\]</span></p>
<p><strong>Algorithm:</strong></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>Initialize: F_k<span class="op">^</span><span class="dv">0</span> <span class="op">=</span> log(n_k <span class="op">/</span> n) <span class="cf">for</span> each <span class="kw">class</span> <span class="fu">k</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>For t <span class="op">=</span> <span class="dv">1</span> to T:</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute probabilities</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="op">=</span> <span class="dv">1</span> to n:</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="op">=</span> <span class="dv">1</span> to K:</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>            p_k[i] <span class="op">=</span> softmax(F_1<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}(x_i), ..., F_K<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>}(x_i))[k]</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build K trees (one per class) simultaneously</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="op">=</span> <span class="dv">1</span> to K:</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute gradients for class k</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="op">=</span> <span class="dv">1</span> to n:</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>            g_k[i] <span class="op">=</span> p_k[i] <span class="op">-</span> (<span class="dv">1</span> <span class="cf">if</span> y[i] <span class="op">==</span> k <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>            h_k[i] <span class="op">=</span> p_k[i] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p_k[i])</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fit tree to gradients for class k</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        tree_k <span class="op">=</span> build_tree(X, g_k, h_k)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update all K functions</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="op">=</span> <span class="dv">1</span> to K:</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        F_k<span class="op">^</span>t <span class="op">=</span> F_k<span class="op">^</span>{t<span class="op">-</span><span class="dv">1</span>} <span class="op">+</span> η <span class="op">*</span> tree_k</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Key points:</strong> 1. Build <span class="math inline">\(K\)</span> trees per iteration (total of <span class="math inline">\(T \times K\)</span> trees) 2. Gradients depend on ALL class probabilities (coupled optimization) 3. Hessian is always <span class="math inline">\(p_k(1 - p_k)\)</span> (similar to binary logloss)</p>
<p><strong>Memory:</strong> <span class="math inline">\(K \times\)</span> more than binary classification</p>
<p><strong>Constraint handling:</strong> Some implementations add constraint <span class="math inline">\(\sum_{k=1}^K F_k(x) = 0\)</span> to remove ambiguity. If all <span class="math inline">\(F_k\)</span> shift by constant <span class="math inline">\(c\)</span>, probabilities unchanged. Common fix: set <span class="math inline">\(F_K = 0\)</span> or subtract mean.</p>
</section>
</section>
<section id="part-12-feature-importance" class="level2" data-number="1.12">
<h2 data-number="1.12" data-anchor-id="part-12-feature-importance"><span class="header-section-number">1.12</span> Part 12: Feature Importance</h2>
<p>How to measure which features are most important?</p>
<section id="gain-based-importance-default-in-xgboost" class="level3" data-number="1.12.1">
<h3 data-number="1.12.1" data-anchor-id="gain-based-importance-default-in-xgboost"><span class="header-section-number">1.12.1</span> 1. Gain-based Importance (Default in XGBoost)</h3>
<p><strong>Definition:</strong> Total gain contributed by a feature across all splits.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>importance[f] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> each tree:</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> each split node <span class="kw">in</span> tree:</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> split uses feature f:</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>            importance[f] <span class="op">+=</span> gain_from_this_split</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Interpretation:</strong> - High gain → feature provides more information - Most intuitive measure</p>
<p><strong>Issue:</strong> - Biased toward features with many possible splits (high cardinality) - Continuous features vs binary features</p>
</section>
<section id="cover-based-importance" class="level3" data-number="1.12.2">
<h3 data-number="1.12.2" data-anchor-id="cover-based-importance"><span class="header-section-number">1.12.2</span> 2. Cover-based Importance</h3>
<p><strong>Definition:</strong> Total hessian (cover) in nodes that split on feature.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>importance[f] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> each tree:</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> each split node <span class="kw">in</span> tree:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> split uses feature f:</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>            importance[f] <span class="op">+=</span> sum_of_hessian_in_this_node</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Interpretation:</strong> - Measures how many examples are affected by splits on this feature - Less sensitive to cardinality than gain</p>
</section>
<section id="frequency-based-importance-weight" class="level3" data-number="1.12.3">
<h3 data-number="1.12.3" data-anchor-id="frequency-based-importance-weight"><span class="header-section-number">1.12.3</span> 3. Frequency-based Importance (Weight)</h3>
<p><strong>Definition:</strong> Number of times feature appears in trees.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>importance[f] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> each tree:</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> each split node <span class="kw">in</span> tree:</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> split uses feature f:</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>            importance[f] <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Interpretation:</strong> - Simplest measure - Doesn’t account for quality of splits</p>
<p><strong>Issue:</strong> - Can be misleading: feature used many times with small gains vs rarely with huge gains</p>
</section>
<section id="permutation-importance-model-agnostic" class="level3" data-number="1.12.4">
<h3 data-number="1.12.4" data-anchor-id="permutation-importance-model-agnostic"><span class="header-section-number">1.12.4</span> 4. Permutation Importance (Model-agnostic)</h3>
<p><strong>Definition:</strong> Decrease in performance when feature values are randomly shuffled.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>baseline_score <span class="op">=</span> evaluate_model(X_val, y_val)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature f:</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    X_permuted <span class="op">=</span> copy(X_val)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    X_permuted[:, f] <span class="op">=</span> random_shuffle(X_permuted[:, f])</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    permuted_score <span class="op">=</span> evaluate_model(X_permuted, y_val)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    importance[f] <span class="op">=</span> baseline_score <span class="op">-</span> permuted_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Interpretation:</strong> - How much does model depend on this feature? - Breaking the relationship should hurt performance</p>
<p><strong>Pros:</strong> - Model-agnostic (works for any model) - Captures true predictive importance - Accounts for feature interactions</p>
<p><strong>Cons:</strong> - Computationally expensive (need to retrain/evaluate multiple times) - Can be unstable with correlated features</p>
</section>
<section id="shap-values-shapley-additive-explanations" class="level3" data-number="1.12.5">
<h3 data-number="1.12.5" data-anchor-id="shap-values-shapley-additive-explanations"><span class="header-section-number">1.12.5</span> 5. SHAP Values (Shapley Additive Explanations)</h3>
<p><strong>Theory:</strong> Based on cooperative game theory (Shapley values).</p>
<p><strong>For a single prediction:</strong> SHAP value for feature <span class="math inline">\(f\)</span> = how much does <span class="math inline">\(f\)</span> contribute to moving prediction from base value to final prediction?</p>
<p><strong>Properties:</strong> - Consistent: if feature helps more, gets higher value - Local accuracy: <span class="math inline">\(\sum \text{SHAP}_f = \text{prediction} - \text{base}\)</span> - Symmetric: features with same contribution get same value</p>
<p><strong>TreeSHAP:</strong> Efficient algorithm for tree-based models (<span class="math inline">\(O(\text{TL}D^2)\)</span> where <span class="math inline">\(T\)</span> = trees, <span class="math inline">\(L\)</span> = leaves, <span class="math inline">\(D\)</span> = depth)</p>
<p><strong>Aggregate importance:</strong></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>importance[f] <span class="op">=</span> mean(<span class="op">|</span>SHAP_f(x)<span class="op">|</span> <span class="cf">for</span> x <span class="kw">in</span> X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Pros:</strong> - Theoretically grounded - Handles feature interactions properly - Can explain individual predictions</p>
<p><strong>Cons:</strong> - More complex to compute - Requires additional library (shap)</p>
</section>
<section id="comparison-summary" class="level3" data-number="1.12.6">
<h3 data-number="1.12.6" data-anchor-id="comparison-summary"><span class="header-section-number">1.12.6</span> Comparison Summary</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Method</th>
<th>Bias</th>
<th>Cost</th>
<th>Interpretability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Gain</td>
<td>High cardinality</td>
<td>Free</td>
<td>High</td>
</tr>
<tr class="even">
<td>Cover</td>
<td>Medium</td>
<td>Free</td>
<td>Medium</td>
</tr>
<tr class="odd">
<td>Frequency</td>
<td>Misleading</td>
<td>Free</td>
<td>Low</td>
</tr>
<tr class="even">
<td>Permutation</td>
<td>Correlated features</td>
<td>High</td>
<td>High</td>
</tr>
<tr class="odd">
<td>SHAP</td>
<td>None</td>
<td>Medium</td>
<td>Very High</td>
</tr>
</tbody>
</table>
<p><strong>Recommendation:</strong> - Quick analysis: Gain - Production insights: Permutation or SHAP - Debugging: Multiple methods to cross-check</p>
</section>
</section>
<section id="part-13-early-stopping" class="level2" data-number="1.13">
<h2 data-number="1.13" data-anchor-id="part-13-early-stopping"><span class="header-section-number">1.13</span> Part 13: Early Stopping</h2>
<p><strong>Problem:</strong> More trees ≠ better performance. Eventually, model overfits.</p>
<p><strong>Solution:</strong> Monitor validation loss, stop when it stops improving.</p>
<section id="basic-early-stopping" class="level3" data-number="1.13.1">
<h3 data-number="1.13.1" data-anchor-id="basic-early-stopping"><span class="header-section-number">1.13.1</span> Basic Early Stopping</h3>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>best_loss <span class="op">=</span> infinity</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>rounds_without_improvement <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>stopping_rounds <span class="op">=</span> <span class="dv">50</span>  <span class="co"># Patience parameter</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="op">=</span> <span class="dv">1</span> to max_trees:</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build tree</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    tree_t <span class="op">=</span> build_tree(X_train, g, h)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    F_train <span class="op">=</span> F_train <span class="op">+</span> η <span class="op">*</span> tree_t(X_train)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate on validation set</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    F_val <span class="op">=</span> F_val <span class="op">+</span> η <span class="op">*</span> tree_t(X_val)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> compute_loss(y_val, F_val)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check for improvement</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val_loss <span class="op">&lt;</span> best_loss:</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        best_loss <span class="op">=</span> val_loss</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        rounds_without_improvement <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        save_model()</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>        rounds_without_improvement <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stop if no improvement</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> rounds_without_improvement <span class="op">&gt;=</span> stopping_rounds:</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Stopping at iteration </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Restore best model</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>load_model()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="key-parameters" class="level3" data-number="1.13.2">
<h3 data-number="1.13.2" data-anchor-id="key-parameters"><span class="header-section-number">1.13.2</span> Key Parameters</h3>
<ol type="1">
<li><strong><code>early_stopping_rounds</code></strong> (patience): How many rounds to wait for improvement
<ul>
<li>Too small → stop too early (underfitting)</li>
<li>Too large → waste computation</li>
<li>Typical: 10-100</li>
</ul></li>
<li><strong>Validation set:</strong> Must be separate from training
<ul>
<li>Simple split: 80-20 or 70-30</li>
<li>Better: Use cross-validation with early stopping per fold</li>
</ul></li>
<li><strong>Metric:</strong> What to monitor
<ul>
<li>Regression: RMSE, MAE</li>
<li>Classification: logloss, AUC, accuracy</li>
<li>Should match your actual goal</li>
</ul></li>
</ol>
</section>
<section id="advanced-cross-validation-with-early-stopping" class="level3" data-number="1.13.3">
<h3 data-number="1.13.3" data-anchor-id="advanced-cross-validation-with-early-stopping"><span class="header-section-number">1.13.3</span> Advanced: Cross-Validation with Early Stopping</h3>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>n_folds <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>stopping_rounds <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Run CV</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>cv_results <span class="op">=</span> []</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold <span class="kw">in</span> <span class="dv">1</span> to n_folds:</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    X_train, X_val <span class="op">=</span> split_fold(X, fold)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    y_train, y_val <span class="op">=</span> split_fold(y, fold)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train with early stopping on this fold</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    model, best_iter <span class="op">=</span> train_with_early_stopping(</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        X_train, y_train, X_val, y_val, stopping_rounds</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    cv_results.append(best_iter)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Use mean of best iterations</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>optimal_trees <span class="op">=</span> mean(cv_results)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Optimal number of trees: </span><span class="sc">{</span>optimal_trees<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrain on full data with optimal_trees iterations</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> train(X, y, n_trees<span class="op">=</span>optimal_trees)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Why this matters:</strong> - Often determines final model quality - Can reduce training time significantly (stop at 500 trees instead of 5000) - Prevents overfitting without manual tuning</p>
</section>
</section>
<section id="part-14-implementation-checklist" class="level2" data-number="1.14">
<h2 data-number="1.14" data-anchor-id="part-14-implementation-checklist"><span class="header-section-number">1.14</span> Part 14: Implementation Checklist</h2>
<section id="baseline-gradient-boosting" class="level3" data-number="1.14.1">
<h3 data-number="1.14.1" data-anchor-id="baseline-gradient-boosting"><span class="header-section-number">1.14.1</span> Baseline Gradient Boosting</h3>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> <span class="fu">BaseGradientBoosting</span>:</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_trees, learning_rate, max_depth):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_trees <span class="op">=</span> n_trees</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> learning_rate</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_depth <span class="op">=</span> max_depth</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.trees <span class="op">=</span> []</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">fit</span>(<span class="va">self</span>, X, y, loss_function):</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        F <span class="op">=</span> initial_prediction(y, loss_function)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_trees):</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute gradients</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>            g <span class="op">=</span> loss_function.gradient(y, F)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Fit tree to negative gradients</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>            tree <span class="op">=</span> DecisionTree(max_depth<span class="op">=</span><span class="va">self</span>.max_depth)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>            tree.fit(X, <span class="op">-</span>g)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update predictions</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>            F <span class="op">=</span> F <span class="op">+</span> <span class="va">self</span>.lr <span class="op">*</span> tree.predict(X)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.trees.append(tree)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">predict</span>(<span class="va">self</span>, X):</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>        F <span class="op">=</span> <span class="va">self</span>.initial_value</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> tree <span class="kw">in</span> <span class="va">self</span>.trees:</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>            F <span class="op">+=</span> <span class="va">self</span>.lr <span class="op">*</span> tree.predict(X)</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="xgboost-style-second-order" class="level3" data-number="1.14.2">
<h3 data-number="1.14.2" data-anchor-id="xgboost-style-second-order"><span class="header-section-number">1.14.2</span> XGBoost-style (Second-order)</h3>
<p><strong>Key additions:</strong> 1. Compute both gradients AND hessians 2. Use gain formula for splits: <span class="math inline">\(\frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{G^2}{H+\lambda}\right] - \gamma\)</span> 3. Optimal leaf weights: <span class="math inline">\(w_j = -G_j/(H_j + \lambda)\)</span> 4. Regularization parameters: <span class="math inline">\(\gamma\)</span> (min_split_loss), <span class="math inline">\(\lambda\)</span> (reg_lambda)</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> <span class="fu">XGBoostStyle</span>:</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">build_tree</span>(<span class="va">self</span>, X, g, h):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Start with root</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        root <span class="op">=</span> Node(examples<span class="op">=</span><span class="bu">range</span>(<span class="bu">len</span>(X)))</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        queue <span class="op">=</span> [root]</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> queue:</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>            node <span class="op">=</span> queue.pop(<span class="dv">0</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Don't split if too deep</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> node.depth <span class="op">&gt;=</span> <span class="va">self</span>.max_depth:</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>                node.is_leaf <span class="op">=</span> <span class="va">True</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Find best split</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>            best_gain <span class="op">=</span> <span class="op">-</span>inf</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> feature <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>]):</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> threshold <span class="kw">in</span> get_candidates(X[:, feature]):</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>                    left_idx <span class="op">=</span> X[node.examples, feature] <span class="op">&lt;</span> threshold</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>                    right_idx <span class="op">=</span> <span class="op">~</span>left_idx</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>                    G_L <span class="op">=</span> g[node.examples][left_idx].<span class="bu">sum</span>()</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>                    H_L <span class="op">=</span> h[node.examples][left_idx].<span class="bu">sum</span>()</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>                    G_R <span class="op">=</span> g[node.examples][right_idx].<span class="bu">sum</span>()</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>                    H_R <span class="op">=</span> h[node.examples][right_idx].<span class="bu">sum</span>()</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>                    G <span class="op">=</span> G_L <span class="op">+</span> G_R</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>                    H <span class="op">=</span> H_L <span class="op">+</span> H_R</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>                    gain <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>                        G_L<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (H_L <span class="op">+</span> <span class="va">self</span>.lambda_reg) <span class="op">+</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>                        G_R<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (H_R <span class="op">+</span> <span class="va">self</span>.lambda_reg) <span class="op">-</span></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>                        G<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (H <span class="op">+</span> <span class="va">self</span>.lambda_reg)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>                    ) <span class="op">-</span> <span class="va">self</span>.gamma</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> gain <span class="op">&gt;</span> best_gain:</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>                        best_gain <span class="op">=</span> gain</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>                        best_split <span class="op">=</span> (feature, threshold, left_idx, right_idx)</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Execute split if gain &gt; 0</span></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> best_gain <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>                node.split(<span class="op">*</span>best_split)</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>                queue.extend([node.left, node.right])</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>                node.is_leaf <span class="op">=</span> <span class="va">True</span></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assign leaf weights</span></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> leaf <span class="kw">in</span> tree.get_leaves():</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>            G_leaf <span class="op">=</span> g[leaf.examples].<span class="bu">sum</span>()</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>            H_leaf <span class="op">=</span> h[leaf.examples].<span class="bu">sum</span>()</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>            leaf.weight <span class="op">=</span> <span class="op">-</span>G_leaf <span class="op">/</span> (H_leaf <span class="op">+</span> <span class="va">self</span>.lambda_reg)</span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tree</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="histogram-based-optimization" class="level3" data-number="1.14.3">
<h3 data-number="1.14.3" data-anchor-id="histogram-based-optimization"><span class="header-section-number">1.14.3</span> Histogram-based Optimization</h3>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">bin_features</span>(X, n_bins<span class="op">=</span><span class="dv">255</span>):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Bin continuous features into discrete bins"""</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    bins <span class="op">=</span> {}</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    X_binned <span class="op">=</span> np.zeros_like(X, dtype<span class="op">=</span>np.uint8)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> f <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>]):</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Quantile-based binning</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        quantiles <span class="op">=</span> np.percentile(X[:, f], </span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>                                  np.linspace(<span class="dv">0</span>, <span class="dv">100</span>, n_bins))</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        bins[f] <span class="op">=</span> np.unique(quantiles)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        X_binned[:, f] <span class="op">=</span> np.digitize(X[:, f], bins[f])</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X_binned, bins</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">build_histogram</span>(node_examples, X_binned, g, h, n_bins):</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Build gradient histogram for node"""</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> X_binned.shape[<span class="dv">1</span>]</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    G_hist <span class="op">=</span> np.zeros((n_features, n_bins))</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    H_hist <span class="op">=</span> np.zeros((n_features, n_bins))</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> f <span class="kw">in</span> <span class="bu">range</span>(n_features):</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> bin_idx <span class="kw">in</span> <span class="bu">range</span>(n_bins):</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> X_binned[node_examples, f] <span class="op">==</span> bin_idx</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>            G_hist[f, bin_idx] <span class="op">=</span> g[node_examples][mask].<span class="bu">sum</span>()</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>            H_hist[f, bin_idx] <span class="op">=</span> h[node_examples][mask].<span class="bu">sum</span>()</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> G_hist, H_hist</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">find_best_split_histogram</span>(G_hist, H_hist, lambda_reg, gamma):</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Find best split using histogram"""</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>    best_gain <span class="op">=</span> <span class="op">-</span>inf</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    best_split <span class="op">=</span> <span class="va">None</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> f <span class="kw">in</span> <span class="bu">range</span>(G_hist.shape[<span class="dv">0</span>]):</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>        G_L <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>        H_L <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>        G_total <span class="op">=</span> G_hist[f].<span class="bu">sum</span>()</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>        H_total <span class="op">=</span> H_hist[f].<span class="bu">sum</span>()</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> bin_idx <span class="kw">in</span> <span class="bu">range</span>(G_hist.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>            G_L <span class="op">+=</span> G_hist[f, bin_idx]</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>            H_L <span class="op">+=</span> H_hist[f, bin_idx]</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>            G_R <span class="op">=</span> G_total <span class="op">-</span> G_L</span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>            H_R <span class="op">=</span> H_total <span class="op">-</span> H_L</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>            gain <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>                G_L<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (H_L <span class="op">+</span> lambda_reg) <span class="op">+</span></span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>                G_R<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (H_R <span class="op">+</span> lambda_reg) <span class="op">-</span></span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>                G_total<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (H_total <span class="op">+</span> lambda_reg)</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>            ) <span class="op">-</span> gamma</span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> gain <span class="op">&gt;</span> best_gain:</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>                best_gain <span class="op">=</span> gain</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>                best_split <span class="op">=</span> (f, bin_idx)</span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> best_split, best_gain</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="loss-functions-implementation" class="level3" data-number="1.14.4">
<h3 data-number="1.14.4" data-anchor-id="loss-functions-implementation"><span class="header-section-number">1.14.4</span> Loss Functions Implementation</h3>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> <span class="fu">SquaredLoss</span>:</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">loss</span>(<span class="va">self</span>, y, y_pred):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (y <span class="op">-</span> y_pred)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">gradient</span>(<span class="va">self</span>, y, y_pred):</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>(y <span class="op">-</span> y_pred)  <span class="co"># Note: negative residual</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">hessian</span>(<span class="va">self</span>, y, y_pred):</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.ones_like(y)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">init_estimate</span>(<span class="va">self</span>, y):</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean(y)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> <span class="fu">LogLoss</span>:</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">loss</span>(<span class="va">self</span>, y, y_pred):</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>._sigmoid(y_pred)</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>(y <span class="op">*</span> np.log(p) <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>y) <span class="op">*</span> np.log(<span class="dv">1</span><span class="op">-</span>p))</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">gradient</span>(<span class="va">self</span>, y, y_pred):</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>._sigmoid(y_pred)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> p <span class="op">-</span> y</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">hessian</span>(<span class="va">self</span>, y, y_pred):</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>._sigmoid(y_pred)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> p <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">init_estimate</span>(<span class="va">self</span>, y):</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> np.mean(y)</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.log(p <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> p))  <span class="co"># Log-odds</span></span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">_sigmoid</span>(<span class="va">self</span>, x):</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> <span class="fu">MulticlassLoss</span>:</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_classes):</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_classes <span class="op">=</span> n_classes</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">gradient</span>(<span class="va">self</span>, y, F):</span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># F is shape (n, K), y is shape (n,) with class indices</span></span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>._softmax(F)</span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> p.copy()</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>        g[np.arange(<span class="bu">len</span>(y)), y] <span class="op">-=</span> <span class="dv">1</span>  <span class="co"># p_k - y_k</span></span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> g</span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">hessian</span>(<span class="va">self</span>, y, F):</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="va">self</span>._softmax(F)</span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> p <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p)</span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">init_estimate</span>(<span class="va">self</span>, y):</span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return log of class frequencies</span></span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>        class_counts <span class="op">=</span> np.bincount(y, minlength<span class="op">=</span><span class="va">self</span>.n_classes)</span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.log(class_counts <span class="op">/</span> <span class="bu">len</span>(y))</span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">_softmax</span>(<span class="va">self</span>, F):</span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a>        exp_F <span class="op">=</span> np.exp(F <span class="op">-</span> np.<span class="bu">max</span>(F, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> exp_F <span class="op">/</span> np.<span class="bu">sum</span>(exp_F, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="catboost-ordered-target-statistics" class="level3" data-number="1.14.5">
<h3 data-number="1.14.5" data-anchor-id="catboost-ordered-target-statistics"><span class="header-section-number">1.14.5</span> CatBoost Ordered Target Statistics</h3>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">compute_ordered_ts</span>(X_cat, y, permutation, prior<span class="op">=</span><span class="fl">0.5</span>, a<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute ordered target statistics</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co">    X_cat: categorical feature values</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">    y: targets</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">    permutation: random ordering of examples</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">    prior: prior probability (default 0.5)</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">    a: smoothing parameter (default 1.0)</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(X_cat)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    ts <span class="op">=</span> np.zeros(n)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For each position in permutation</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, i <span class="kw">in</span> <span class="bu">enumerate</span>(permutation):</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Only use examples before this position</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>        prev_examples <span class="op">=</span> permutation[:idx]</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(prev_examples) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>            ts[i] <span class="op">=</span> prior</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Count matches with same category</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>            same_cat <span class="op">=</span> X_cat[prev_examples] <span class="op">==</span> X_cat[i]</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>            count <span class="op">=</span> same_cat.<span class="bu">sum</span>()</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>            sum_y <span class="op">=</span> y[prev_examples][same_cat].<span class="bu">sum</span>()</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Apply smoothing</span></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>            ts[i] <span class="op">=</span> (sum_y <span class="op">+</span> a <span class="op">*</span> prior) <span class="op">/</span> (count <span class="op">+</span> a)</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="catboost-ordered-boosting-conceptual" class="level3" data-number="1.14.6">
<h3 data-number="1.14.6" data-anchor-id="catboost-ordered-boosting-conceptual"><span class="header-section-number">1.14.6</span> CatBoost Ordered Boosting (Conceptual)</h3>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> <span class="fu">OrderedBoosting</span>:</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_permutations<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_permutations <span class="op">=</span> n_permutations</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.permutations <span class="op">=</span> []</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.predictions <span class="op">=</span> {}  <span class="co"># M[r,j,i] stored as dict</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">fit</span>(<span class="va">self</span>, X, y, n_trees):</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate permutations</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_permutations):</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.permutations.append(np.random.permutation(n))</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize predictions</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_permutations):</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.predictions[(r, j, i)] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Build trees</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(n_trees):</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Pick random permutation</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>            r <span class="op">=</span> np.random.randint(<span class="va">self</span>.n_permutations)</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>            sigma <span class="op">=</span> <span class="va">self</span>.permutations[r]</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute gradients using ordered predictions</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>            g <span class="op">=</span> np.zeros(n)</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx, i <span class="kw">in</span> <span class="bu">enumerate</span>(sigma):</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Use M[r, idx-1, i] - never saw example i</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>                pred <span class="op">=</span> <span class="va">self</span>.predictions[(r, <span class="bu">max</span>(<span class="dv">0</span>, idx<span class="op">-</span><span class="dv">1</span>), i)]</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>                g[i] <span class="op">=</span> <span class="va">self</span>.loss.gradient(y[i], pred)</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Build tree (standard way)</span></span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>            tree <span class="op">=</span> <span class="va">self</span>.build_tree(X, g, h)</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update all M[r',j,i] with new tree</span></span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> r_prime <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_permutations):</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>                        <span class="co"># Only update if i is in first j examples</span></span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> <span class="va">self</span>.permutations[r_prime].tolist().index(i) <span class="op">&lt;</span> j:</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>                            leaf_value <span class="op">=</span> tree.predict_single(X[i])</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>                            <span class="va">self</span>.predictions[(r_prime, j, i)] <span class="op">+=</span> leaf_value</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="part-15-summary-of-differences" class="level2" data-number="1.15">
<h2 data-number="1.15" data-anchor-id="part-15-summary-of-differences"><span class="header-section-number">1.15</span> Part 15: Summary of Differences</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 20%">
<col style="width: 18%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Baseline</th>
<th>XGBoost</th>
<th>LightGBM</th>
<th>CatBoost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Core method</strong></td>
<td>First-order gradient</td>
<td>Second-order (Newton)</td>
<td>Second-order</td>
<td>Second-order</td>
</tr>
<tr class="even">
<td><strong>Splits</strong></td>
<td>Exact greedy</td>
<td>Exact or histogram</td>
<td>Histogram</td>
<td>Histogram</td>
</tr>
<tr class="odd">
<td><strong>Tree growth</strong></td>
<td>Level-wise</td>
<td>Level-wise</td>
<td>Leaf-wise (best-first)</td>
<td>Level-wise</td>
</tr>
<tr class="even">
<td><strong>Regularization</strong></td>
<td>None</td>
<td><span class="math inline">\(\gamma J + \lambda \sum w_j^2\)</span></td>
<td><span class="math inline">\(\gamma J + \lambda \sum w_j^2\)</span></td>
<td><span class="math inline">\(\gamma J + \lambda \sum w_j^2\)</span></td>
</tr>
<tr class="odd">
<td><strong>Categorical</strong></td>
<td>One-hot/manual</td>
<td>One-hot/manual</td>
<td>Gradient statistics</td>
<td>Ordered target statistics</td>
</tr>
<tr class="even">
<td><strong>Sampling</strong></td>
<td>None</td>
<td>Row/column subsample</td>
<td>GOSS + EFB</td>
<td>None</td>
</tr>
<tr class="odd">
<td><strong>Bias handling</strong></td>
<td>None</td>
<td>None</td>
<td>None</td>
<td>Ordered boosting</td>
</tr>
<tr class="even">
<td><strong>Missing values</strong></td>
<td>Manual impute</td>
<td>Learned direction</td>
<td>Learned direction</td>
<td>Not explicitly handled</td>
</tr>
<tr class="odd">
<td><strong>Multiclass</strong></td>
<td>Manual OvA</td>
<td>Softmax (native)</td>
<td>Softmax (native)</td>
<td>Softmax (native)</td>
</tr>
<tr class="even">
<td><strong>Focus</strong></td>
<td>Foundation</td>
<td>General framework</td>
<td>Speed</td>
<td>Statistical correctness</td>
</tr>
</tbody>
</table>
</section>
<section id="key-takeaways-for-implementation" class="level2" data-number="1.16">
<h2 data-number="1.16" data-anchor-id="key-takeaways-for-implementation"><span class="header-section-number">1.16</span> Key Takeaways for Implementation</h2>
<ol type="1">
<li><strong>Start simple:</strong> Implement first-order gradient boosting with exact splits</li>
<li><strong>Add second-order:</strong> Compute hessians, use gain formula, optimal leaf weights</li>
<li><strong>Optimize:</strong> Add histogram-based splitting for speed</li>
<li><strong>Regularization:</strong> Implement <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\lambda\)</span> penalties</li>
<li><strong>Growth strategy:</strong> Choose level-wise (safer) or leaf-wise (faster)</li>
<li><strong>Missing values:</strong> XGBoost’s learned sparsity-aware splits are powerful</li>
<li><strong>Categoricals:</strong> Start with one-hot, optionally add target statistics</li>
<li><strong>Multiclass:</strong> Use softmax approach for joint optimization</li>
<li><strong>Early stopping:</strong> Always use it - critical for production models</li>
<li><strong>Feature importance:</strong> Use multiple methods to cross-check</li>
<li><strong>Subsampling:</strong> Row and column sampling are easy regularization wins</li>
<li><strong>Advanced:</strong> Ordered boosting/TS only if you need CatBoost’s guarantees</li>
</ol>
<p>The math is consistent across all implementations - differences are in optimizations and handling of specific problems (speed vs bias vs categoricals).</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{sarang2025,
  author = {Sarang, Nima},
  title = {GBT {Algorithms} and {Tips}},
  date = {2025-08-07},
  url = {https://www.nimasarang.com/blog/2025-10-17-gbt-algorithms/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-sarang2025" class="csl-entry quarto-appendix-citeas" role="listitem">
<div class="">Nima Sarang. GBT Algorithms and Tips, 2025.
Available: <a href="https://www.nimasarang.com/blog/2025-10-17-gbt-algorithms/">https://www.nimasarang.com/blog/2025-10-17-gbt-algorithms/</a>.</div>
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("^(?:http:|https:)\/\/(?:www\.)?(nimasarang\.com)\/.*$");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="nsarang/nsarang.github.io" data-repo-id="R_kgDOMZYgPg" data-category="Announcements" data-category-id="DIC_kwDOMZYgPs4ChVIN" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Nima Sarang</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>
async function loadBodyWhenReady() {
    while (!document.body.classList.contains('quarto-light') && !document.body.classList.contains('quarto-dark')) {
        await new Promise(resolve => setTimeout(resolve, 50));
    }
    setTimeout(() => {
        document.body.style.visibility = "visible";
    }, 50); // Adjust the delay to allow for the dark mode to be loaded
}
loadBodyWhenReady();
</script>




</body></html>